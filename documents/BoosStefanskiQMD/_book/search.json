[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Selected Solutions to Boos and Stefanski",
    "section": "",
    "text": "Preface\nThese are solutions of problems given from Essential Statistical Inference: Theory and Methods (Boos and Stefanski 2013). The solutions were written by me, with occasional help from fellow classmates during our coursework at the University of Utah. We covered chapters 1 through 8, and these are the solutions to the problems that came from the text.\nI am not affiliated with the authors of the book. The solutions are for self-study purposes only. Be aware that some of these solutions may not have been peer-reviewed or verified and as such may be incomplete or incorrect.\n\n\n\n\nBoos, D. D., and L. A. Stefanski. 2013. Essential Statistical Inference: Theory and Methods. Springer Texts in Statistics. Springer New York. https://link.springer.com/book/10.1007/978-1-4614-4818-1."
  },
  {
    "objectID": "chapters/ch1.html#section",
    "href": "chapters/ch1.html#section",
    "title": "Chapter 1: Roles of Modeling in Statistical Inference",
    "section": "1.12.",
    "text": "1.12.\nSuppose that \\(Y_1, \\cdots, Y_n\\) are iid \\(Poisson(\\lambda)\\) and that \\(\\hat{\\lambda} = \\bar{Y}\\). Define \\(\\sigma^2_{\\hat{\\lambda}} = var(\\hat{\\lambda})\\). Consider the following two estimators of \\(\\sigma^2_{\\hat{\\lambda}}\\) : \\[\\hat{\\sigma}^2_{\\hat{\\lambda}} = \\frac{\\hat{\\lambda}}{n}~~,~~\\tilde{\\sigma}^2_{\\hat{\\lambda}} = \\frac{s^2}{n}\\]\n\na.\nWhen the Poisson model holds, are both estimators consistent?\nSolution:\n\\[\n\\begin{aligned}\nn\\hat{\\sigma}^2_{\\hat{\\lambda}} &= n\\left(\\frac{\\hat{\\lambda}}{n} \\right) = \\bar{Y} \\overset{P}{\\to} \\lambda &\\text{ by weak law of large numbers} \\\\\nn{\\sigma}^2_{{\\lambda}} &= n \\, var(\\hat{\\lambda}) =n\\,var(\\bar{Y}) \\overset{P}{\\to} n\\frac{\\lambda}{n} = \\lambda & \\overset{\\text{ by Central Limit Theorem}}{\\text{and } Y \\sim POIS(\\lambda)} \\\\\n\\end{aligned}\n\\]\nThus \\(n\\hat{\\sigma}^2_{\\hat{\\lambda}}\\) is a consistent estimator of \\(n{\\sigma}^2_{\\hat{\\lambda}}\\).\nNow for the other estimator,\nLet \\(\\epsilon &gt; 0\\),\n\\[\n\\begin{aligned}\n\\lim_{n \\to \\infty}Pr(|n\\tilde{\\sigma}^2_{\\hat{\\lambda}} - n\\sigma^2_{\\hat{\\lambda}}| \\geq \\epsilon)\n&= \\lim_{n \\to \\infty}Pr(|n \\frac{s^2}n - n\\sigma^2_{\\hat{\\lambda}}| \\geq \\epsilon) \\\\\n&= \\lim_{n \\to \\infty}Pr(|s^2 - n \\, var(\\hat{\\lambda}) | \\geq \\epsilon) \\\\\n&= \\lim_{n \\to \\infty}Pr(|s^2 - n \\, var(\\bar{Y}) | \\geq \\epsilon) \\\\\n&=\\lim_{n \\to \\infty}Pr(|s^2 - n (\\lambda/n)| \\geq \\epsilon) \\\\\n&= \\lim_{n \\to \\infty}Pr(|s^2 - \\lambda | \\geq \\epsilon) \\\\\n&\\leq \\frac{E[s^2-\\lambda]^2}{\\epsilon^2} & \\overset{\\text{by Chebychev's Inequality}}{\\text{(see page 233 Cassella-Berger )}} \\\\ \\\\\n&= \\frac{Var(s^2)}{\\epsilon^2}\n\\end{aligned}\n\\]\nNote that \\(lim_{n \\to \\infty}Var(s^2) = 0\\) under the Poisson model since the kurtosis is not a function of n. Thus \\(\\lim_{n \\to \\infty}Pr(|n\\tilde{\\sigma}^2_{\\hat{\\lambda}} - n\\sigma^2_{\\hat{\\lambda}}| \\geq \\epsilon) = 0,\\) thus, by definition, \\(n\\tilde{\\sigma}^2_{\\hat{\\lambda}}\\) is a consistent estimator of \\(n\\sigma^2_{\\hat{\\lambda}}\\).\n\n\nb.\nWhen the Poisson model holds, give the asymptotic relative efficiency of the two estimators. (Actually here you can compare exact variances since the estimators are so simple.)\nSolution:\n\\[\n\\begin{aligned}\nRE(\\hat{\\sigma}^2_{\\hat{\\lambda}},\\tilde{\\sigma}^2_{\\hat{\\lambda}}) &= \\lim_{n \\to \\infty} VAR(\\frac{\\hat{\\lambda}}{n})/VAR(\\frac{s^2}{n}) \\\\\n&= \\lim_{n \\to \\infty} Var(\\hat{\\lambda})/Var({s^2}) \\\\\n&= \\lim_{n \\to \\infty}\\frac{Var(\\bar{Y})}{[Var(Y_1)]^2[\\frac{2}{n-1}+\\frac{Kurt -3}{n}]} \\\\\n&= \\lim_{n \\to \\infty}\\frac{\\lambda/n}{\\lambda^2\\left[\\frac{2}{n-1}+\\frac{1/\\lambda}{n}\\right]} \\\\\n&= \\lim_{n \\to \\infty}\\frac{1}{n\\lambda\\left[\\frac{2}{n-1}+\\frac{1/\\lambda}{n}\\right]} \\\\\n&= \\lim_{n \\to \\infty}\\frac{1}{\\left[\\frac{2n\\lambda}{n-1}+1\\right]} \\\\\n&= \\frac{1}{2\\lambda + 1}\n\\end{aligned}\n\\]\nAs the support for \\(\\lambda\\) is \\(\\mathbb N\\), this indicates \\(\\hat{\\sigma}^2_{\\hat{\\lambda}}\\) is more efficient.\n\n\nc.\nWhen the Poisson model does not hold but assuming second moments exist, are both estimators consistent?\nSolution:\n\\(n\\tilde{\\sigma}^2_{\\hat{\\lambda}}\\) will always be consistent as long as \\(Var(s^2) \\overset{P}{\\to} 0\\).\n\\(n\\hat{\\sigma}^2_{\\hat{\\lambda}}\\) would no longer necessarily be a consistent estimator of \\(n{\\sigma}^2_{\\hat{\\lambda}}\\). This is because\n\\[\\begin{aligned}\nn\\hat{\\sigma}^2_{\\hat{\\lambda}} &= n\\left(\\frac{\\hat{\\lambda}}{n} \\right) = \\bar{Y} \\overset{P}{\\to} \\mu &\\text{ by weak law of large numbers} \\\\\nn{\\sigma}^2_{{\\lambda}} &= n \\, var(\\hat{\\lambda}) =n\\,var(\\bar{Y}) \\overset{P}{\\to} n\\frac{\\sigma^2}{n} = \\sigma^2 & \\text{ by Central Limit Theorem}\n\\end{aligned}\\]\nTherefore, for any distribution where the mean does not equal the variance (such as a standard normal distribution), \\(n{\\hat{\\sigma}}^2_{\\hat{\\lambda}}\\) will not be a consistent estimator of \\(n{\\sigma}^2_{\\hat{\\lambda}}\\)."
  },
  {
    "objectID": "chapters/ch1.html#section-1",
    "href": "chapters/ch1.html#section-1",
    "title": "Chapter 1: Roles of Modeling in Statistical Inference",
    "section": "1.13",
    "text": "1.13\nDefine full model as \\(N(\\mu, c^2\\mu^2)\\), where \\(c\\) is known. Define \\(\\hat{\\mu}_{WLS}\\) as the value that minimizes \\[\\rho(\\mu) = \\sum_{i=1}^n \\frac{(Y_i- \\mu)^2}{c^2\\mu^2}\\].\n\na)\nShow that \\(\\mu_{WLS}\\) converges in probability, but that its limit is not \\(\\mu\\).\nSolution:\n\\[\n\\begin{aligned}\n0 &= \\frac{\\partial}{\\partial \\mu} \\left\\{\\rho(\\mu)\\right\\}\n= \\frac{\\partial }{\\partial \\mu} \\left\\{ \\sum_{i=1}^n \\frac{(Y_i- \\mu)^2}{c^2\\mu^2} \\right\\} = \\frac{\\partial }{\\partial \\mu} \\left\\{ \\sum_{i=1}^n \\frac{Y_i^2- 2Y_i\\mu +\\mu^2}{c^2\\mu^2} \\right\\} \\\\\n&= \\frac{\\partial }{\\partial \\mu} \\left\\{ \\frac{nm_2'- 2n\\bar{Y}\\mu +n\\mu^2}{c^2\\mu^2} \\right\\} \\\\\n&=\\frac{c^2\\mu^2(-2n\\bar{Y}+2n\\mu)-2c^2\\mu(nm_2'- 2n\\bar{Y}\\mu +n\\mu^2)}{c^4\\mu^4} \\\\\n&=\\frac{\\mu(-2n\\bar{Y}+2n\\mu)-2(nm_2'- 2n\\bar{Y}\\mu +n\\mu^2)}{c^2\\mu^3} \\\\\n&=\\frac{-2n\\bar{Y}\\mu+2n\\mu^2-2nm_2'+ 4n\\bar{Y}\\mu -2n\\mu^2}{c^2\\mu^3} \\\\\n&=\\frac{2n\\bar{Y}\\mu -2nm_2'}{c^2\\mu^3} = \\frac{2n(\\bar{Y}\\mu -m_2')}{c^2\\mu^3}\\\\\n\\implies \\mu &= \\frac{m_2'}{\\bar{Y}}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial^2 \\mu} \\left\\{\\rho(\\mu)\\right\\}_{\\mu=\\frac{m_2'}{\\bar{Y}}} &= \\frac{\\partial^2 }{\\partial^2 \\mu} \\left\\{ \\sum_{i=1}^n \\frac{(Y_i- \\mu)^2}{c^2\\mu^2} \\right\\}_{\\mu = \\frac{m_2'}{\\bar{Y}}}\n=\\frac{\\partial}{\\partial \\mu} \\left\\{ \\frac{2n(\\bar{Y}\\mu -m_2')}{c^2\\mu^3} \\right\\}_{\\mu = \\frac{m_2'}{\\bar{Y}}} \\\\\n&=\\left\\{ \\frac{c^2\\mu^3(2n\\bar{Y})-3c^2\\mu^2 2n(\\bar{Y}\\mu -m_2')}{c^4\\mu^6} \\right\\}_{\\mu = \\frac{m_2'}{\\bar{Y}}} \\\\\n&=\\left\\{ \\frac{2n\\bar{Y}\\mu-6n\\bar{Y}\\mu +6nm_2'}{c^2\\mu^4} \\right\\}_{\\mu = \\frac{m_2'}{\\bar{Y}}}\n=\\left\\{ \\frac{-4n\\bar{Y}\\mu+6nm_2'}{c^2\\mu^4} \\right\\}_{\\mu = \\frac{m_2'}{\\bar{Y}}} \\\\\n&=\\frac{-4n\\bar{Y}\\frac{m_2'}{\\bar{Y}}+6nm_2'}{c^2\\mu^4} =\\frac{2nm_2'}{c^2\\mu^4}\n&gt; 0.\n\\end{aligned}\n\\]\nThus \\(\\hat{\\mu}_{WLS} = \\frac{m_2'}{\\bar{Y}}\\). By the Strong Law of Large Numbers, \\(m_2' \\to E(m_2')\\) and \\(\\bar{Y} \\to \\mu\\). Note that,\n\\[\n\\begin{aligned}E(m_2') &= E\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i^2\\right)=\\frac{1}{n}\\sum_{i=1}^nE\\left( Y_i^2\\right) \\\\ &=\\frac{1}{n}\\sum_{i=1}^n \\left(Var(Y_i) +[E(Y_i)]^2 \\right) =\\frac{1}{n}\\sum_{i=1}^n \\left( c^2\\mu^2+\\mu^2 \\right)\\\\\n&=c^2\\mu^2+\\mu^2 = (c^2+1)\\mu^2\n\\end{aligned}\n\\]\nThus, by the continuous mapping theorem, \\(\\hat{\\mu}_{WLS}= \\frac{m_2'}{\\bar{Y}} \\overset{P}{\\to} = \\frac{(c^2+1)\\mu^2}{\\mu}=(c^2+1)\\mu\\)\n\n\nb)\nFind \\(E[\\rho ' (\\mu)]\\) and show that it does not equal zero.\nSolution:\n\\[\n\\begin{aligned}\nE[\\rho ' (\\mu)] &= E\\left(\\frac{2n(\\bar{Y}\\mu -m_2')}{c^2\\mu^3} \\right) \\\\\n&= \\frac{2n}{c^2\\mu^3}\\left[E(\\bar{Y})\\mu -E(m_2')\\right] \\\\\n&= \\frac{2n}{c^2\\mu^3}\\left[\\mu^2 -(c^2+1)\\mu^2\\right] \\\\\n&= \\frac{2n}{c^2\\mu^3}\\left[-c\\mu^2\\right] \\\\\n&=\\frac{-2n}{\\mu} \\neq 0.\n\\end{aligned}\n\\]\n\n\n\nc)\nFind the estimator obtained by minimizing \\(\\rho_*(\\mu) = \\sum_{i=1}^n \\frac{(Y_i- \\mu)^2}{c^2\\mu^2} + 2log(\\mu)\\).\nSolution:\n\\[\\begin{aligned}\n0 &= \\frac{\\partial}{\\partial \\mu} \\left\\{\\rho_*(\\mu)\\right\\} \\\\\n&= \\frac{\\partial}{\\partial \\mu} \\left\\{\\left[ \\sum_{i=1}^n \\frac{(Y_i- \\mu)^2}{c^2\\mu^2} \\right] + 2n\\log(\\mu)\\right\\}\\\\\n&= \\frac{2n(\\bar{Y}\\mu -m_2')}{c^2\\mu^3} +\\frac{2n}{\\mu} \\\\\n&= \\frac{2n(\\bar{Y}\\mu -m_2'+c^2\\mu^2)}{c^2\\mu^3}\\\\\n\\implies \\mu &= \\frac{-\\bar{Y} + \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2} & \\overset {\\text{other solution cannot exist since }}{\\mu &gt; 0}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial^2 \\mu} \\left\\{\\rho_*(\\mu)\\right\\}_{\\mu = \\hat{\\mu}}\n&=\\frac{\\partial}{\\partial \\mu} \\left\\{ \\frac{2n(\\bar{Y}\\mu -m_2'+c^2\\mu^2)}{c^2\\mu^3} \\right\\}_{\\mu = \\hat{\\mu}}\n=\\frac{\\partial}{\\partial \\mu} 2n\\left\\{ \\frac{\\bar{Y}\\mu -m_2'+c^2\\mu^2}{c^2\\mu^3} \\right\\}_{\\mu = \\hat{\\mu}} \\\\\n&=2n\\left\\{\\frac{c^2\\mu^3(\\bar{Y}+2c^2\\mu)-3c^2\\mu^2(\\bar{Y}\\mu -m_2'+c^2\\mu^2)}{c^4\\mu^6} \\right\\}_{\\mu = \\hat{\\mu}} \\\\\n&=2n\\left\\{\\frac{\\mu(\\bar{Y}+2c^2\\mu)-3(\\bar{Y}\\mu -m_2'+c^2\\mu^2)}{c^2\\mu^4} \\right\\}_{\\mu = \\hat{\\mu}} \\\\\n&=2n\\left\\{\\frac{-2\\bar{Y}\\mu - c^2\\mu^2+3m_2'}{c^2\\mu^4} \\right\\}_{\\mu = \\hat{\\mu}} \\\\\n\\end{aligned}\n\\]\nWe only need to show the numerator to be positive.\n\\[\n\\begin{aligned}\n&- c^2\\mu^2-2\\bar{Y}\\mu +3m_2' \\bigg|_{\\mu = \\left(\\frac{-\\bar{Y} + \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2}\\right)}\\\\\n&=-c^2\\left(\\frac{-\\bar{Y} + \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2}\\right)^2-2\\bar{Y}\\left(\\frac{-\\bar{Y} + \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2}\\right)+3m_2' \\\\\n&=-c^2\\left(\\frac{\\bar{Y}^2 -2\\bar{Y} \\sqrt{\\bar{Y}^2+4c^2m_2'}+\\bar{Y}^2+4c^2m_2'}{4c^4}\\right)-2\\bar{Y}\\left(\\frac{-\\bar{Y} + \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2}\\right)+3m_2' \\\\\n&=\\frac{-\\bar{Y}^2 +\\bar{Y} \\sqrt{\\bar{Y}^2+4c^2m_2'}-2c^2m_2'}{2c^2}+\\left(\\frac{2\\bar{Y}^2 -2\\bar{Y} \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2}\\right)+3m_2' \\\\\n&=\\frac{\\bar{Y}^2+4c^2m_2' -\\bar{Y} \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2} \\\\\n&=\\frac{\\left(\\sqrt{\\bar{Y}^2+4c^2m_2'} -\\bar{Y}\\right)\\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2} \\\\\n&&gt;0~\\text{because } \\sqrt{x^2+a} &gt; x~\\forall a&gt;0.\n\\end{aligned}\n\\]\nThus \\(\\hat{\\mu}_{^*{WLS}}=\\frac{-\\bar{Y} + \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2}\\)\n\n\n\nd)\nIs \\(\\hat{\\mu}_{^*{WLS}}\\) a consistent estimator of \\(\\mu\\)? If so, find its asymptotic variance and compare to \\(\\hat{\\mu}_{MLE}\\) and \\(\\hat{\\mu}_{MOM}\\).\nSolution:\nBy continuous mapping theorem,\n\\[\n\\begin{aligned}\n\\hat{\\mu}_{^*{WLS}} =\\frac{-\\bar{Y} + \\sqrt{\\bar{Y}^2+4c^2m_2'}}{2c^2} &\\overset{P}{\\to} \\frac{-\\mu + \\sqrt{\\mu^2+4c^2(c^2+1)\\mu^2}}{2c^2}\\\\\n&=\\frac{-\\mu +\\mu \\sqrt{1+4c^2(c^2+1)}}{2c^2} \\\\\n&=\\frac{-\\mu +\\mu \\sqrt{1+4c^2+ 4c^4}}{2c^2} \\\\\n&=\\frac{-\\mu +\\mu \\sqrt{(1+2c^2)^2}}{2c^2} \\\\\n&=\\frac{-\\mu +\\mu (1+2c^2)}{2c^2} \\\\\n&=\\mu.\n\\end{aligned}\n\\]\nThus \\(\\hat{\\mu}_{^*{WLS}}\\) is a consistent estimator of \\(\\mu\\).\nNote that \\(\\hat{\\mu}_{^*{WLS}} = \\hat{\\mu}_{MLE}\\). Thus, \\(Var(\\hat{\\mu}_{^*{WLS}}) = Var(\\hat{\\mu}_{MLE}) = \\frac{c^2\\mu^2}{n(1+2c^2)}\\), and the relative efficiency between the two is 1. That is,\n\\(ARE(\\hat{\\mu}_{^*{WLS}}, \\hat{\\mu}_{MLE}) =1\\) And, according to (1.17, p. 18) in the book, \\(ARE(\\hat{\\mu}_{MOM}, \\hat{\\mu}_{^*WLS}) =1+2c^2\\).\nThus \\(\\hat{\\mu}_{MLE}\\) and \\(\\hat{\\mu}_{^*{WLS}}\\) are equally more efficient than \\(\\hat{\\mu}_{MOM}\\)\n\n\ne)\nFind \\(E[\\rho_* ' (\\mu)]\\) and show that it does not equal zero.\nSolution:\n\\[\n\\begin{aligned}\nE[\\rho_* ' (\\mu)] &= E\\left(\\frac{2n(\\bar{Y}\\mu -m_2'+c^2\\mu^2)}{c^2\\mu^3}\\right) \\\\\n&= \\frac{2n}{c^2\\mu^3}\\left[E(\\bar{Y})\\mu -E(m_2') +E(c^2\\mu^2)\\right] \\\\\n&= \\frac{2n}{c^2\\mu^3}\\left[\\mu^2 -(c^2+1)\\mu^2 +c^2\\mu^2\\right]\\\\\n&= \\frac{2n}{c^2\\mu^3}(0) \\\\\n&=0.\n\\end{aligned}\n\\]\n\n\n\n1.17.\nTo check on the asymptotic variance expression in (1.20, p. 18), generate 1000 samples of size \\(n=20\\) from the exponential density \\((1/2)exp(-y/2)\\) that has mean \\(\\mu=2\\) and variance=4 so that in terms of the model \\(\\sigma^2=1\\). For each sample, calculate \\(\\hat{\\mu}_{MLE}\\) in (1.16, p. 17). Then compute (1.20, p. 18) for this exponential density and compare it to \\(n=20\\) times the sample variance of the 1000 values of \\(\\hat{\\mu}_{MLE}\\). Repeat for \\(n=50\\).\nSolution:\n\n\nCode\nlibrary(kableExtra)\nset.seed(123)\n\n#Set constants\nmu &lt;- 2\npop_var &lt;- 4\nc&lt;- 1\n\n\nskew &lt;- 2\nkurt &lt;- 9\n\n\n#For sample = 20\nsample_size&lt;- 20\n\n#Compute asymptotic variance as defined in book (1.21)\na_var_20&lt;- (c^2*mu^2/(sample_size*(1+2*c^2)^2))*(1+2*c^2+2*c*skew+c^2*(kurt-3))\n\n#Simulate mu_mle\nsample &lt;- list()\nmu_mle&lt;- rep(NA, 1000)\n\nfor(i in 1:1000){\n    sample[[i]]&lt;- rexp(n=sample_size,rate =0.5)\n    first_moment &lt;- mean(sample[[i]])\n    second_moment &lt;- sum(sample[[i]]^2)/sample_size\n    \n    mu_mle[i] &lt;-\n      (sqrt((first_moment^2)+\n              4*(c^2)*second_moment)- first_moment)/\n      (2*c^2)\n}\n\nsample_var_20 &lt;- var(mu_mle)\n\nsample_size&lt;- 50\n\n#Compute asymptotic variance as defined in book (1.21)\na_var_50&lt;- (c^2*mu^2/(sample_size*(1+2*c^2)^2))*(1+2*c^2+2*c*skew+c^2*(kurt-3))\n\n#Simulate mu_mle\nsample &lt;- list()\nmu_mle&lt;- rep(NA, 1000)\n\nfor(i in 1:1000){\n    sample[[i]]&lt;- rexp(n=sample_size,rate =0.5)\n    first_moment &lt;- mean(sample[[i]])\n    second_moment &lt;- sum(sample[[i]]^2)/sample_size\n    \n    mu_mle[i] &lt;-\n      (sqrt((first_moment^2)+\n              4*(c^2)*second_moment)- first_moment)/\n      (2*c^2)\n}\nsample_var_50 &lt;- var(mu_mle)\n\nsample_size_20 &lt;- c(sample_var_20, a_var_20)\nsample_size_50 &lt;- c(sample_var_50, a_var_50)\n\nkable(rbind(sample_size_20,sample_size_50),\n      col.names = c(expression(\"Sample Variance\"), expression(\"Asymptotic Variance\")))\n\n\n\n\n\n\nSample Variance\nAsymptotic Variance\n\n\n\n\nsample_size_20\n0.2465302\n0.2888889\n\n\nsample_size_50\n0.1007942\n0.1155556\n\n\n\n\n\nThe sample variance approaches the asymptotic variance as sample size increases."
  },
  {
    "objectID": "chapters/ch2.html#section",
    "href": "chapters/ch2.html#section",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.1",
    "text": "2.1\nLet \\(Y_1,\\dots, Y_n\\) be iid positive random variables such that \\(Y^{(\\lambda)}\\) is assumed to have a normal\\((\\mu,\\sigma^2)\\) distribution, where\n\\[ Y^{(\\lambda)} =\n\\begin{cases}\n\\frac{Y^\\lambda-1}{\\lambda} &when~ \\lambda \\neq 0 \\\\\n\\log(Y)& when~ \\lambda =0.\n\\end{cases}\n\\] Derive the log likelihood \\(\\ell_n(\\mu,\\sigma, \\lambda | \\mathbf{Y})\\) of the observed data \\(Y_1,\\dots, Y_n\\).\nSolution:\n\\[ \\begin{aligned}\nP\\left(\\frac{Y^{(\\lambda)}-1} \\lambda \\leq \\frac{y^{(\\lambda)}-1} \\lambda  \\right) &= P\\left(Y^{(\\lambda)}\\leq \\frac{y^{(\\lambda)}-1} \\lambda \\right)  & \\text{for } \\lambda \\neq0\\\\\n&=  \\Phi \\left(\\frac{\\frac{y^{(\\lambda)}-1} \\lambda  - \\mu}{\\sigma} \\right) \\\\\n\\implies f_{\\frac{Y^{(\\lambda)}-1} \\lambda} &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\frac{y^{(\\lambda)}-1} \\lambda  - \\mu}{\\sigma}\\right)^2\\right\\}\\left(\\frac{1}{\\lambda\\sigma}\\right)\n\\end{aligned}\n\\]\n\\[ \\begin{aligned}\nP\\left(\\log(Y) \\leq \\log(y\\right) &= P\\left( Y^{(\\lambda)} &lt;\\log(y) \\right) \\text{ for }\\lambda =0\\\\\n&= \\Phi \\left( \\frac{\\log (y)- \\mu}{\\sigma} \\right)\\\\\n\\implies f_{\\log(Y)} &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\log (y)- \\mu}{\\sigma}\\right)^2\\right\\}\\left(\\frac{1}{y\\sigma}\\right)\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\mathcal{L} (\\mu,\\sigma |\\mathbf{Y}) &= \\prod_{i=1}^n \\left[\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\frac{y_i^{(\\lambda)}-1} \\lambda  - \\mu}{\\sigma}\\right)^2\\right\\}\\left(\\frac{1}{\\lambda\\sigma}\\right)\\right]^{I(\\lambda \\neq0)} \\left[\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\log (y_i)- \\mu}{\\sigma}\\right)^2\\right\\}\\left(\\frac{1}{y_i\\sigma}\\right)\\right]^{I(\\lambda =0)} \\\\\n&= \\left[\\frac{1}{\\sigma^2\\sqrt{2\\pi }}\\right]^n\\prod_{i=1}^n \\left[\\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\frac{y_i^{(\\lambda)}-1} \\lambda  - \\mu}{\\sigma}\\right)^2\\right\\}\\left(\\frac{1}{\\lambda}\\right)\\right]^{I(\\lambda \\neq0)} \\left[\\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\log (y_i)- \\mu}{\\sigma}\\right)^2\\right\\}\\left(\\frac{1}{y_i}\\right)\\right]^{I(\\lambda =0)} \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\implies \\ell (\\mu,\\sigma |\\mathbf{Y}) &=\\sum_{i=1}^n\\left[ \\left( -\\frac{1}{2}\\left(\\frac{\\frac{y_i^{(\\lambda)-\\mu}} \\lambda- \\mu}{\\sigma}\\right)^2 - \\log(\\lambda)\\right){I(\\lambda \\neq0)} +\n\\left(-\\frac{1}{2}\\left(\\frac{\\log (y_i)- \\mu}{\\sigma}\\right)^2-\\log(y_i)\\right){I(\\lambda =0)} \\right] \\\\\n&~~~~~~~~~~~~~~~~~~~~~~~+n \\log\\left(\\frac{1}{\\sigma^2\\sqrt{2\\pi}}\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-1",
    "href": "chapters/ch2.html#section-1",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.3",
    "text": "2.3\nRecall the ZIP model\n\\[\n\\begin{aligned}\nP(Y = 0) &= p+(1-p)e^{-\\lambda} \\\\\nP(Y=y) &= (1-p)\\frac{\\lambda^y e^{-\\lambda}}{y!} & y=1,2,...\n\\end{aligned}\n\\]\n\na)\nReparametrize the model by defining \\(\\pi \\equiv P(Y =0) = p+(1-p)e^{-\\lambda}\\). Solve for \\(p\\) in terms of \\(\\pi\\) and \\(\\lambda\\), then substitute so that the density only depends on them.\nSolution:\n\\[\n\\begin{aligned}\n\\pi &= p+(1-p)e^{-\\lambda} \\\\\n&=p (1-e^{-\\lambda}) + e^{-\\lambda} \\\\\n\\implies p &= \\frac{\\pi-e^{-\\lambda}}{1-e^{-\\lambda}}\n\\end{aligned}\n\\]\n\\[ \\begin{aligned}\nP(Y = y) &= \\left(p+(1-p)e^{-\\lambda}\\right)^{I(y=0)}\\left((1-p)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right)^{I(y \\in \\mathbb{N}^+)} \\\\\n&= \\left[\\left( \\frac{\\pi-e^{-\\lambda}}{1-e^{-\\lambda}} \\right)+ \\left(1-\\left( \\frac{\\pi-e^{-\\lambda}}{1-e^{-\\lambda}} \\right)\\right)e^{-\\lambda}\\right]^{I(y=0)}\n\\left[\\left(1-\\left( \\frac{\\pi-e^{-\\lambda}}{1-e^{-\\lambda}} \\right)\\right)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right]^{I(y \\in \\mathbb{N}^+)} \\\\\n&= \\left[\\left( \\frac{\\pi-e^{-\\lambda}}{1-e^{-\\lambda}} \\right)+ \\left( \\frac{1-e^{-\\lambda}-\\pi+e^{-\\lambda}}{1-e^{-\\lambda}}\\right)e^{-\\lambda}\\right]^{I(y=0)}\n\\left[\\left( \\frac{1-e^{-\\lambda}-\\pi+e^{-\\lambda}}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right]^{I(y \\in \\mathbb{N}^+)}\\\\\n&= \\left[\\left( \\frac{\\pi-e^{-\\lambda}}{1-e^{-\\lambda}} \\right)+ \\left( \\frac{1-\\pi}{1-e^{-\\lambda}}\\right)e^{-\\lambda}\\right]^{I(y=0)}\n\\left[\\left( \\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right]^{I(y \\in \\mathbb{N}^+)} \\\\\n&= \\left[\\left( \\frac{\\pi-\\pi e^{-\\lambda}}{1-e^{-\\lambda}}\\right)\\right]^{I(y=0)}\n\\left[\\left( \\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right]^{I(y \\in \\mathbb{N}^+)} \\\\\n&= \\pi^{I(y=0)}\n\\left[\\left( \\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right]^{I(y \\in \\mathbb{N}^+)}\n\\end{aligned}\n\\] \n\n\nb)\nLet \\(n_0\\) represent the number of samples in an iid sample of size \\(n\\). Assuming the complete data is available, show that the likelihood factors into two pieces and that \\(\\hat{\\pi} = n_0/n\\). Also show that the MLE for \\(\\lambda\\) is the solution to a simple nonlinear equation involving \\(\\bar{Y}_+\\).\nSolution:\n\\[ \\begin{aligned}\n\\mathcal{L}(\\lambda, p | \\mathbf{Y}) &= \\prod_{i=1}^n\\pi^{I(y_i=0)}\n\\left[\\left( \\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right]^{I(y_i \\in \\mathbb{N}^+)} \\\\\n&= \\pi^{n_0}(1-\\pi)^{n-n_0}\\prod_{Y_i\\in \\mathbb{N}^+}\\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i! ( 1-e^{-\\lambda})}\n\\end{aligned}\n\\]\nTo find \\(\\hat{\\pi}\\), we only need to maximize \\(g(\\pi) = \\pi^{n_0}(1-\\pi)^{n-n_0}\\).\n\\[\\begin{aligned}\n\\log g(\\pi) &= n_0 \\log (\\pi) +(n-n_0)\\log(1-\\pi) \\\\\n\\frac{d \\log g}{d\\pi} &= \\frac{n_0}{\\pi } -\\frac{n-n_0}{1-\\pi } \\\\\n\\frac{d^2 \\log g}{d\\pi^2} &=-\\frac{n_0}{\\pi^2 } - \\frac{n-n_0}{(1-\\pi)^2}  &gt; 0\n\\end{aligned}\n\\]\n\\[\\begin{aligned}\n0 =\\frac{d \\log g}{d\\pi} &= \\frac{n_0}{\\pi } -\\frac{n-n_0}{1-\\pi }  \\\\\n\\implies \\frac{\\pi}{1-\\pi} &= \\frac{n_0}{n-n_0} \\\\\n\\implies \\pi\\left(1+ \\frac{n_0}{n-n_0}\\right) &= \\frac{n_0}{n-n_0} \\\\\n\\implies \\pi &= \\frac{n_0}{(n-n_0)\\left(1+ \\frac{n_0}{n-n_0}\\right)} \\\\\n&= \\frac{n_0}{n}\n\\end{aligned}\n\\]\nThus, \\(\\hat{\\pi} = n_0/n\\).\nTo find \\(\\hat{\\lambda}\\), we need to maximize \\(h(\\lambda) =\\prod_{Y_i\\in \\mathbb{N}^+}\\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i! ( 1-e^{-\\lambda})}\\)\n\\[\n\\begin{aligned}\n\\log h(\\lambda) &= \\sum_{Y_i \\in \\mathbb{N}^+} y_i\\log(\\lambda) -\\lambda - \\log(y_i!) -\\log(1-e^{-\\lambda}) \\\\\n\\implies \\frac{d \\log h}{d \\lambda} &=  (n-n_0) \\left(\\frac{\\bar{y}_+}{\\lambda}-1-\\frac{ e^{-\\lambda}}{1-e^{-\\lambda}} \\right) \\\\\n0= \\frac{d \\log h}{d \\lambda} &=  (n-n_0) \\left(\\frac{\\bar{y}_+}{\\lambda}-1-\\frac{ e^{-\\lambda}}{1-e^{-\\lambda}} \\right) \\\\\n&=  \\bar{y}_+-\\lambda -\\frac{ \\lambda e^{-\\lambda}}{1-e^{-\\lambda}}\n\\end{aligned}\n\\]\nThus, \\(\\hat{\\lambda}_{MLE}\\) will be a solution to the above simple non-linear equation, which involves \\(\\bar{y}_+\\).\n\n\nc)\nNow consider the truncated or conditional sample consisting of the \\(n- n_0\\) nonzero values. Write down the conditional likelihood for these values and obtain the same equation for \\(\\hat{\\lambda}_{MLE}\\) as in a)\nSolution:\n\\[\n\\begin{aligned}\nP(Y=y |Y&gt;0) &= \\frac{P(Y&gt;0 \\cap Y=y)}{P(Y&gt;0)} \\\\\n&= \\frac{\n\\left[\\left( \\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^y e^{-\\lambda}}{y!}\\right]^{I(y \\in \\mathbb{N}^+)}}{1-\\pi}\n\\\\\n&= \\left[\\frac{\\lambda^y e^{-\\lambda}}{y!(1-e^{-\\lambda})}\\right]^{I(y \\in \\mathbb N^+)}\n\\end{aligned}\n\\]\nThus, we need to maximize the same function we found in part b), which will be the solution to a non-simple linear equation involving \\(\\bar{y}\\)."
  },
  {
    "objectID": "chapters/ch2.html#section-2",
    "href": "chapters/ch2.html#section-2",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.4",
    "text": "2.4\nIn sampling land areas for counts of an animal species, we obtain an iid sample of counts \\(Y_1, \\dots, ,Y_n\\), where each \\(Y_i\\) has a Poisson distribution with parameter \\(\\lambda\\)\n\na) Derive the maximum likelihood estimator of \\(\\lambda\\). Call it \\(\\hat{\\lambda}_{MLE_1}\\)\nSolution:\n\\[ \\begin{aligned}\n\\mathcal{L} (\\lambda, \\mathbf{Y}) &= \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!} \\\\\n\\implies \\ell (\\lambda, \\mathbf{Y}) &= \\sum_{i=1}^n -\\lambda + y_i\\log(\\lambda)- \\log(y_i!) \\\\\n&=  -n\\lambda + n\\bar{y}\\log(\\lambda)- \\sum_{i=1}^n \\log(y_i!) \\\\\n\\implies \\frac{d \\ell}{d \\lambda} &= -n+ \\frac{n\\bar{y}}{\\lambda} \\\\\n\\implies \\frac{d^2 \\ell}{d \\lambda^2} &= -\\frac{n\\bar{y}}{\\lambda^2}\n&lt; 0 \\text{ because } \\bar{y}, \\lambda, n &gt;0.\n\\end{aligned}\n\\]\n\\[ \\begin{aligned}\n0=\\frac{d \\ell}{d \\lambda} &= -n+ \\frac{n\\bar{y}}{\\hat{\\lambda}_{MLE_1}}\\\\\n\\implies \\hat{\\lambda}_{MLE_1}= \\bar{y}\n\\end{aligned}\n\\]\n\n\nb)\nFor simplicity in quadrat sampling, sometimes only the presence or absence of a species is recorded. Let \\(n_0\\) be the number of \\(Y_i\\)’s that are zero. Write down the binomial likelihood based only on \\(n_0\\). Show that the maximum likelihood estimator of \\(\\lambda\\) based only on \\(n_0\\) is \\(\\hat{\\lambda}_{MLE_2}= -\\log(n_0/n)\\).\nSolution:\n\\[\n\\begin{aligned}\nP(Y=0)&= e^{-\\lambda} \\\\\n\\implies \\mathcal L (\\lambda | \\mathbf Y) &= {n \\choose n_0} (e^{-\\lambda})^{n_0}(1-e^{-\\lambda})^{n-n_o} \\\\\n\\implies \\ell (\\lambda | \\mathbf Y) &=\\log{n \\choose n_0}- n_0\\lambda+(n-n_0)\\log(1-e^{-\\lambda}) \\\\\n\\implies \\frac{ d\\ell (\\lambda | \\mathbf Y)}{d \\lambda} &= - n_0+\\frac{(n-n_0)e^{-\\lambda}}{1-e^{-\\lambda}} \\\\\n\\implies \\frac{ d^2 \\ell (\\lambda | \\mathbf Y)}{d \\lambda^2} &=  (n-n_0) \\frac{-(1-e^{-\\lambda})e^{-\\lambda} -e^{-\\lambda}(e^{-\\lambda})}{(1-e^{-\\lambda})^2} \\\\\n&=  \\frac{-e^{-\\lambda}(n-n_0)}{(1-e^{-\\lambda})^2} &lt; 0 ~\\text{since}~ n_0&lt;n\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n0= \\frac{ d\\ell (\\lambda | \\mathbf Y)}{d \\lambda} &= - n_0+\\frac{(n-n_0)e^{-\\lambda}}{1-e^{-\\lambda}} \\\\\n\\implies e^{-\\lambda} &= \\frac{n_0}{n-n_0}(1-e^{-\\lambda}) \\\\\n\\implies e^{-\\lambda} \\left(1+ \\frac{n_0}{n-n_0} \\right) &= \\frac{n_0}{n-n_0} \\\\\n\\implies e^{-\\lambda}  &= \\frac{n_0}{n-n_0}\\left(\\frac{n-n_0}{n} \\right) \\\\\n\\implies \\hat{\\lambda}_{MLE_2} &= -\\log(n_0/n)\n\\end{aligned}\n\\]\n\n\nc)\nUse the delta theorem from Ch. 1 to show that the asymptotic relative efficiency of \\(\\hat{\\lambda}_{MLE_1}\\) to \\(\\hat{\\lambda}_{MLE_2}\\) is \\(\\frac{e^{\\lambda}-1}{\\lambda}\\).\nSolution:\n\\[\n\\begin{aligned}\n\\sqrt n [\\bar{y} - \\lambda] &\\overset {d}{\\to} N(0,\\lambda) &\\text{CLT, poisson distribution}\\\\\n\\sqrt n [n_0/n - e^{-\\lambda}] &\\overset {d}{\\to} N(0,e^{-\\lambda}(1-e^{-\\lambda})) &\\text{CLT, bernoulli distribution} \\\\\n\\sqrt n [-\\log(n_0/n) - \\log(e^{-\\lambda}/n)] &\\overset {d}{\\to} N\\left(0,ne^{-\\lambda}(1-e^{-\\lambda})\\left[-\\frac{1}{e^{-\\lambda}/n}\\frac{1}{n}\\right]^2 \\right) & \\text{by Delta Method} \\\\\n&\\overset {d}{\\to} N\\left(0,\\frac{(1-e^{-\\lambda})}{e^{-\\lambda}} \\right) \\\\\n&\\overset {d}{\\to} N\\left(0,e^{\\lambda}-1 \\right) \\\\\n\\end{aligned}\n\\]\nThus,\n\\[ARE (\\hat{\\lambda}_{MLE_1},\\hat{\\lambda}_{MLE_2}) = \\frac{AVAR(\\hat{\\lambda}_{MLE_2})}{AVAR(\\hat{\\lambda}_{MLE_1})}= \\frac{e^\\lambda-1}{\\lambda}\\]\n\n\nd)\nThe overall goal of the sampling is to estimate the mean number of the species per unit land area. Comment on the use of \\(\\hat{\\lambda}_{MLE_2}\\) in place of \\(\\hat{\\lambda}_{MLE_1}\\). That is, explain to a researcher for what values and under what distributional assumptions is it reasonable?\nSolution:\nSince \\(e^{\\lambda}-1 &gt; \\lambda\\), \\(\\hat{\\lambda}_{MLE_2}\\) will always be more efficient. However, the distributional assumptions behind when to use which estimator are inherently different. \\(\\hat{\\lambda}_{MLE_2}\\) presumes that we have presence-absence data, or zero inflated poisson data at the worst, and is looking to predict the number of land units with the absence of a species. However, if we know the data absolutely comes from a poisson distribution, \\(\\hat{\\lambda}_{MLE_1}\\) will fit the distributional assumptions better."
  },
  {
    "objectID": "chapters/ch2.html#section-3",
    "href": "chapters/ch2.html#section-3",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.9",
    "text": "2.9\nThe sample \\(Y_1,\\dots, Y_n\\) is iid with distribution \\(F_Y(y;p_o,p_1,\\alpha, \\beta)= p_0I(0 \\leq y) + (1-p_0-p_1)F(y;\\alpha,\\beta) + p_1I(y\\geq 1)\\), where \\(F(y;\\alpha,\\beta)\\) is the beta distribution. Use the \\(2h\\) method to show that the likelihood is \\(p_0^{n_0}p_1^{n_1}(1-p_0-p_1)^{n-n_0-n_1} \\prod_{0&lt;Y_i&lt;1} f(Y_i;\\alpha,\\beta)\\).\nSolution:\n\\[\n\\begin{aligned}\n\\mathcal L(p_0,p_1,\\alpha, \\beta | \\mathbf{Y}) &= \\lim_{h \\to 0^+} (2h)^{-n} \\prod_{i=1}^n P_{p_0,p_1,p_2}\\left(Y_i^* \\in (Y_i-h,Y_i+h] |Y_i \\right) \\\\\n&= \\lim_{h \\to 0^+} (2h)^{-n} \\prod_{i=1}^n F_i(Y_i+h;p_o,p_1,\\alpha, \\beta)-F_i(Y_i-h;p_o,p_1,\\alpha, \\beta) \\\\\n&= \\lim_{h \\to 0^+} (2h)^{-n} \\prod_{i=1}^n \\big\\{ p_0I(0 \\leq y_i+h) + (1-p_0-p_1)F(y_i+h;\\alpha,\\beta) + p_1I(y_i+h\\geq 1) \\\\\n&~~~~~~~~~-[p_0I(0 \\leq y_i-h) + (1-p_0-p_1)F(y_i-h;\\alpha,\\beta) + p_1I(y_i-h\\geq 1)] \\big\\} \\\\\n&= \\lim_{h \\to 0^+} (2h)^{-n} \\prod_{i=1}^n \\big\\{ p_0[I(0 \\leq y_i+h)-I(0 \\leq y_i-h)] \\\\\n&~~~~~~~~~+ (1-p_0-p_1)[F(y_i+h;\\alpha,\\beta)-F(y_i-h;\\alpha,\\beta)] \\\\\n&~~~~~~~~~+ p_1[I(y_i+h\\geq 1)-I(y_i-h\\geq 1)] \\big\\} \\\\\n&=\\prod_{i=1}^n \\big\\{ p_0\\lim_{h \\to 0^+} (2h)^{-n}[I(0 \\leq y_i+h)-I(0 \\leq y_i-h)]\\\\\n&~~~~~~~~~+ p_1\\lim_{h \\to 0^+} (2h)^{-n}[I(y_i+h\\geq 1)-I(y_i-h\\geq 1)] \\big\\} \\\\\n&~~~~~~~~~+ (1-p_0-p_1)\\lim_{h \\to 0^+} (2h)^{-n}[F(y_i+h;\\alpha,\\beta)-F(y_i-h;\\alpha,\\beta)]\\big\\}  \\\\\n&=\\prod_{i=1}^n \\big\\{ p_0 I(y_i=0) + p_1I(y_i=1) + (1-p_0-p_1)f(y_i;\\alpha,\\beta)I(0&lt;y_i&lt;1) \\\\\n&=p_0^{n_0}p_1^{n_1}(1-p_0-p_1)\\prod_{0&lt;Y_i&lt;1} f(y_i;\\alpha,\\beta) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-4",
    "href": "chapters/ch2.html#section-4",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.12",
    "text": "2.12\nFor an iid sample \\(Y_1,\\dots, Y_n\\), Type II censoring occurs when we observe only the smallest \\(r\\) values. For example, in a study of light bulb lifetimes, we might stop the study after the first \\(r=10\\) bulbs have failed. Assuming a continuous distribution with density \\(f(y;\\mathbf{\\theta})\\), the likelihood is just the joint density of the smallest \\(r\\) order statistics evaluated at those order statistics: \\[\\mathcal L\\left(\\mathbf \\theta; Y_{(1)}, \\dots, Y_{(r)}\\right) = \\frac {n!}{(n-r)!}\\left[\\prod_{i=1}^r f\\left(Y_{(i)}; \\mathbf \\theta \\right) \\right]\\left[1-F\\left(Y_{(r)}; \\mathbf \\theta \\right)\\right]^{n-r}\\] For this situation, let \\(f(y;\\sigma) = e^{-y/\\sigma}/\\sigma\\) and find the MLE of \\(\\sigma\\).\nSolution:\n\\[ \\begin{aligned}\n\\mathcal L\\left(\\sigma; Y_{(1)}, \\dots, Y_{(r)}\\right) &= \\frac {n!}{(n-10)!}\\left[\\prod_{i=1}^{10} \\frac {e^{-y_{(i)}/\\sigma}} \\sigma \\right]\\left[1-\\left(1- e^{-y_{(10)}/\\sigma}\\right) \\right]^{n-10} \\\\\n&= \\frac {n!}{(n-10)!\\sigma^{10}}\\left[\\prod_{i=1}^{10} {e^{-y_{(i)}/\\sigma}} \\right]\\left[e^{-y_{(10)}/\\sigma} \\right]^{n-10} \\\\\n\\implies \\ell \\left(\\sigma; Y_{(1)}, \\dots, Y_{(r)}\\right) &= \\log \\left[\\frac {n!}{(n-10)!} \\right] - 10 \\log(\\sigma) -  \\frac{\\sum_{i=1}^{10}y_{(i)}} {\\sigma} - \\frac{(n-10)y_{(10)}}{\\sigma}\\\\\n\\implies \\frac {\\partial \\ell \\left(\\sigma; Y_{(1)}, \\dots, Y_{(r)}\\right)}{\\partial \\sigma} &= - \\frac{10}{\\sigma} +  \\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}} {\\sigma^2} \\\\\n\\implies \\frac {\\partial^2 \\ell \\left(\\sigma; Y_{(1)}, \\dots, Y_{(r)}\\right)}{\\partial \\sigma^2} &= \\frac{10}{\\sigma^2} -  2\\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}} {\\sigma^3}\n\\end{aligned}\\] \\[ \\begin{aligned}\n0= \\frac {\\partial \\ell \\left(\\sigma; Y_{(1)}, \\dots, Y_{(r)}\\right)}{\\partial {\\sigma}} &= - \\frac{10}{\\hat{\\sigma}} +  \\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}} {\\hat{\\sigma}^2} \\\\\n\\implies  \\hat{\\sigma} &= \\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}}{10}\n\\end{aligned}\n\\] Verifying this is a maximum, \\[\\begin{aligned}\n\\frac {\\partial^2 \\ell \\left(\\sigma; Y_{(1)}, \\dots, Y_{(r)}\\right)}{\\partial \\sigma^2} \\bigg|_{\\sigma = \\hat \\sigma}&= \\frac{10}{\\left(\\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}}{10}\\right)^2} - 2 \\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}} {\\left( \\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}}{10} \\right)^3} \\\\\n&=  \\frac{-10^3}{\\left({(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}}\\right)^2}\\\\\n&&lt; 0. ~~~~~~~~~~~\\text{(denominator is clearly positive)}\n\\end{aligned}\\] Conclusively,\n\\[\\hat{\\sigma}_{MLE} = \\frac{(n-10)y_{(10)}+ \\sum_{i=1}^{10}y_{(i)}}{10}\\]\nMore generally, I can follow the same steps outside this specific situation to find\n\\[\\hat{\\sigma}_{MLE} = \\frac{(n-r)y_{(r)}+ \\sum_{i=1}^{r}y_{(i)}}{r}\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-5",
    "href": "chapters/ch2.html#section-5",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.16",
    "text": "2.16\nThe standard Box-Cox regression model (Box and Cox 1964) assumes that after transformation of the observed \\(Y_i\\) to \\(Y_i^{(\\lambda)}\\). we have the linear model \\[ Y_i^{(\\lambda)} = \\mathbf{x_i^T \\beta} + e_i,~~~~~ i =1,\\dots,n \\] where \\(Y_i\\) is assumed positive and the \\(x_i\\) \\(i= 1, \\dots, n\\). In addition assume that \\(e_1,\\dots,e_n\\) are iid normal\\((0, \\sigma^2)\\) errors. Recall that the Box-Cox transformation is defined in Problem 2.1 (p. 107) and is strictly increasing for all \\(\\lambda\\) Show that the likelihood is \\[\\mathcal L \\left(\\beta,\\sigma,\\lambda| \\{Y_i, \\mathbf x_i\\}_{i=1}^n \\right) = \\left(\\sqrt{2\\pi}\\sigma\\right)^n \\exp\\left[-\\sum_{i=1}^n \\frac{ \\left(Y_i^{(\\lambda)} - \\mathbf{x}_i^T\\mathbf \\beta \\right)^2}{2 \\sigma^2}  \\right] \\prod_{i=1}^n \\left|\\frac{\\partial t^{(\\lambda)}}{\\partial t} \\bigg\\rvert_{t= Y_i} \\right|\\]\nSolution:\nWe know from the Jacobian method of transformations that \\[f_Y(y;\\theta) = f_{Y^{(\\lambda)}} (y^{(\\lambda)};\\theta)\\left|J\\right|=f_{Y^{(\\lambda)}} (y;\\theta)\\left|\\frac{\\partial t^{(\\lambda)}}{\\partial t} \\bigg\\rvert_{t= Y}\\right|\\].\nThus, since \\(Y_i^{(\\lambda)} \\overset {iid}\\sim N(\\mathbf{x^T \\beta}, \\sigma^2)\\),\n\\[f\\left(Y_i, \\mathbf x_i|\\beta,\\sigma,\\lambda \\right) = \\left(\\sqrt{2\\pi}\\sigma\\right) \\exp\\left[\\frac{- \\left(Y_i^{(\\lambda)} - \\mathbf{x}_i^T\\mathbf \\beta \\right)^2}{2 \\sigma^2}  \\right] \\left|\\frac{\\partial t^{(\\lambda)}}{\\partial t} \\bigg\\rvert_{t= Y_i} \\right|\\] which implies, \\[\\begin{aligned}\\mathcal L \\left(\\beta,\\sigma,\\lambda| \\{Y_i, \\mathbf x_i\\}_{i=1}^n \\right)\n&= \\prod_{i=1}^n \\left(\\sqrt{2\\pi}\\sigma\\right) \\exp\\left[\\frac{- \\left(Y_i^{(\\lambda)} - \\mathbf{x}_i^T\\mathbf \\beta \\right)^2}{2 \\sigma^2}  \\right] \\left|\\frac{\\partial t^{(\\lambda)}}{\\partial t} \\bigg\\rvert_{t= Y_i} \\right|\\\\\n&= \\left(\\sqrt{2\\pi}\\sigma\\right)^n \\exp\\left[-\\sum_{i=1}^n \\frac{ \\left(Y_i^{(\\lambda)} - \\mathbf{x}_i^T\\mathbf \\beta \\right)^2}{2 \\sigma^2}  \\right] \\prod_{i=1}^n \\left|\\frac{\\partial t^{(\\lambda)}}{\\partial t} \\bigg\\rvert_{t= Y_i} \\right|\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-6",
    "href": "chapters/ch2.html#section-6",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.20",
    "text": "2.20\nOne version of the negative binomial probability mass function is given by \\[f(y;\\mu,k) = \\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)} \\left( \\frac{k}{\\mu+k}\\right)^k \\left( 1- \\frac{k}{\\mu+k}\\right)^y I(y \\in \\mathbb N)\\] where \\(\\mu\\) and \\(k\\) are parameters. Assume that \\(k\\) is known and put \\(f(y; \\mu,k)\\) in the GLM form (2.14, p. 53), identifying \\(b(\\theta)\\), etc., and derive the mean and variance of \\(Y, E(Y) = \\mu, Var(Y)= \\mu + \\mu^2/k\\).\nSolution:\n\\[ \\begin{aligned}\nf(y;\\mu,k) &= \\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)} \\left( \\frac{k}{\\mu+k}\\right)^k \\left( 1- \\frac{k}{\\mu+k}\\right)^y  \\\\\n&= \\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)} \\left( \\frac{k}{\\mu+k}\\right)^k \\left( \\frac{\\mu}{\\mu+k}\\right)^y\\\\\n\\log f(y;\\mu,k) &= \\log \\left({\\Gamma(y+k)} \\right)- \\log \\left( \\Gamma(k)\\right)- \\log \\left({\\Gamma(y+1)} \\right) + k \\log \\left({k}\\right) -k \\log \\left({\\mu+k} \\right)+ y \\log \\left( \\frac{\\mu}{\\mu+k}\\right) \\\\\n&= \\{y \\log \\left( \\frac{\\mu}{\\mu+k}\\right)-k \\log \\left({\\mu+k}\\right)\\}+\\log \\left({\\Gamma(y+k)} \\right)- \\log \\left( \\Gamma(k)\\right)- \\log \\left({\\Gamma(y+1)} \\right) + k \\log \\left({k}\\right)\n\\end{aligned}\\]\nLet \\(\\theta = \\log \\left( \\frac{\\mu}{\\mu+k}\\right)\\). Then, setting \\(a(\\phi) = 1\\),\n\\[\\begin{aligned}\nc(y)&=\\log \\left({\\Gamma(y+k)} \\right)- \\log \\left( \\Gamma(k)\\right)- \\log \\left({\\Gamma(y+1)} \\right) + k \\log \\left({k}\\right) \\\\\nb(\\theta) &= k \\log \\left({\\mu+k}\\right) \\\\\n&= k \\log \\left((\\mu + k)\\frac{k}{k}\\right) \\\\\n&= k \\log \\left(\\frac{k}{\\frac{k}{(\\mu + k)}}\\right) \\\\\n&= k \\log \\left(\\frac{k}{\\frac{\\mu+k - \\mu}{(\\mu + k)}}\\right) \\\\\n&= k \\log \\left(\\frac{k}{1-\\frac{\\mu}{(\\mu + k)}}\\right) \\\\\n&= k \\log \\left(\\frac{k}{1-e^{\\theta}}\\right)\n\\end{aligned}\\]\nBecause \\(k\\) is known, I have shown this parametrization of the negative binomial distribution can be manipulated to be in Generalized Linear Model form.\nThus, \\(E(y) = b'(\\theta) = \\frac{ke^{\\theta}}{1-e^\\theta} = \\frac{\\frac{k\\mu}{\\mu+k}}{1- \\frac{\\mu}{\\mu+k}} = \\frac{\\frac{k\\mu}{\\mu+k}}{1- \\frac{\\mu}{\\mu+k}}= \\frac{\\frac{k\\mu}{\\mu+k}}{\\frac{k}{\\mu+k}}= \\mu\\).\nAnd, \\(Var(Y)= b''(\\theta)= \\frac{(1-e^\\theta)ke^\\theta - ke^\\theta(-e^\\theta)}{(1-e^\\theta)^2}= \\frac{\\left(\\frac{k}{\\mu+k}\\right)k\\left(\\frac{\\mu}{\\mu+k}\\right)+ k\\left(\\frac{\\mu}{\\mu+k}\\right)^2}{\\left(\\frac{k}{\\mu+k}\\right)^2}= \\mu+ \\frac{\\mu^2}{k}\\)."
  },
  {
    "objectID": "chapters/ch2.html#section-7",
    "href": "chapters/ch2.html#section-7",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.21",
    "text": "2.21\nThe usual gamma density is given by \\[f(y;\\alpha,\\beta) = \\frac{1}{\\Gamma (\\alpha)\\beta^\\alpha}y^{\\alpha-1}e^{-y/\\beta} I(0 \\leq y &lt; \\infty) ~~~ \\alpha,\\beta&gt;0\\] and has mean \\(\\alpha \\beta\\) and variance \\(\\alpha \\beta^2\\). First reparameterize by letting \\(\\mu= \\alpha \\beta\\) so that the parameter vector is now \\((\\mu, \\alpha)\\). Now put this gamma family in the form of a generalized linear model, identifying \\(\\theta, b(\\theta), a_i(\\phi)\\), and \\(c(y,\\phi)\\). Note that \\(\\alpha\\) is unknown here and should be related to \\(\\phi\\) Make sure that \\(b(\\theta)\\) is actually a function of \\(\\theta\\) and take the first derivative to verify that \\(b(\\theta)\\) is correct.\nSolution:\n\\[\\begin{aligned}\nf(y;\\alpha,\\beta) &= \\frac{1}{\\Gamma (\\alpha)\\left(\\frac{\\mu}{\\alpha}\\right)^\\alpha}y^{\\alpha-1}e^{-y (\\alpha/\\mu)} \\\\\n\\log f(y;\\alpha,\\beta) &= -\\log \\left(\\Gamma (\\alpha) \\right) -\\alpha \\log (\\mu) +\\alpha \\log(\\alpha)+ (\\alpha-1) \\log(y) -y (\\alpha/\\mu) \\\\\n&= \\left[y \\left(\\frac{-\\alpha}{\\mu} \\right) -\\alpha \\log (\\mu) \\right]  +(\\alpha-1)\\log(y)+\\alpha \\log(\\alpha)-\\log \\left(\\Gamma (\\alpha) \\right)  \\\\\n&= \\left[ \\frac{y(-1/\\mu)- \\log (\\mu)}{1/\\alpha} \\right]  +(\\alpha-1)\\log(y)+\\alpha \\log(\\alpha)-\\log \\left(\\Gamma (\\alpha) \\right)  \\\\\n\\end{aligned}\\]\nLet \\(\\theta = -1/\\mu\\), \\(\\phi = 1/\\alpha\\).Then,\n\\[\n\\log f(y;\\alpha,\\beta) = \\left[ \\frac{y\\theta- \\log \\left(-1/\\theta\\right)}{\\phi} \\right]  +\\left(\\frac{1}{\\phi}-1\\right)\\log(y)+\\left(\\frac{1}{\\phi}\\right) \\log\\left(\\frac{1}{\\phi}\\right)-\\log \\left(\\Gamma \\left(\\frac{1}{\\phi}\\right) \\right)\n\\]\nThus, \\[ \\begin{aligned}\nb(\\theta) &= \\log(-1/\\theta) \\\\\na(\\phi) &= \\phi \\\\\nc(y, \\phi) &= \\left(\\frac{1}{\\phi}-1\\right)\\log(y)+\\left(\\frac{1}{\\phi}\\right) \\log\\left(\\frac{1}{\\phi}\\right)-\\log \\left(\\Gamma \\left(\\frac{1}{\\phi}\\right) \\right)\n\\end{aligned}\n\\]\nAnd, \\(b'(\\theta) = \\left(\\frac{1}{-1/\\theta}\\right)\\left(\\frac{-1}{\\theta^2}\\right) = 1/\\theta = \\mu\\)."
  },
  {
    "objectID": "chapters/ch2.html#section-8",
    "href": "chapters/ch2.html#section-8",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.22",
    "text": "2.22\nConsider the standard one-way ANOVA situation with \\(Y_{ij}\\) distributed as \\(N(\\mu_i,\\sigma^2)\\), \\(i=1,\\dots,k\\), \\(j=1,\\dots,n_i\\) and all the random variables are independent.\n\na.\nForm the log likelihood, take derivatives, and show that the MLEs are \\(\\hat \\mu_i = \\bar{Y_i}\\), \\(i=1,\\dots,k\\), \\(\\hat \\sigma^2 = SSE/N\\), where \\(SSE= \\sum_{i=1}^k \\sum_{j=1}^{n_i} (Y_{ij}-\\bar{Y_i})^2\\) and \\(N=\\sum_{i=1}^k n_i\\).\nSolution:\n\\[\n\\begin{aligned}\n\\mathcal L (\\mu_i, \\sigma^2) &= \\prod_{i=1}^k \\prod_{j=1}^{n_i} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} {\\exp\\left(\\frac{-(y_{ij}-\\mu_i)^2}{2\\sigma^2}\\right)} \\\\\n\\ell(\\mu_i, \\sigma^2) &= \\sum_{i=1}^k \\sum_{j=1}^{n_i} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} {\\exp\\left(\\frac{-(y_{ij}-\\mu_i)^2}{2\\sigma^2}\\right)} \\\\\n&= -\\frac{N}{2}\\log(2\\pi)-\\frac{N}{2}\\log(\\sigma^2)-\\sum_{i=1}^k \\sum_{j=1}^{n_i}  \\frac{(y_{ij}-\\mu_i)^2}{2\\sigma^2} \\\\\n\\end{aligned}\n\\] Finding \\(\\hat \\mu_i\\), \\[\n\\begin{aligned}\n0 =\\frac{\\partial \\ell}{\\partial \\mu_i}&=\\sum_{i=1}^k \\sum_{j=1}^{n_i}  \\frac{2(y_{ij}-\\mu_i)}{2\\sigma^2} \\\\\n&=\\sum_{i=1}^k \\sum_{j=1}^{n_i}  y_{ij}-\\mu_i \\\\\n&= \\sum_{i=1}^N \\bar y_i- n_i\\mu_i \\\\\n\\implies \\mu_i &= \\bar y_i \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\mu_i^2} \\bigg|_{\\mu_i= \\bar{Y_i}}&=\\sum_{i=1}^k \\sum_{j=1}^{n_i}  -2 &lt;0 \\\\\n\\implies \\hat \\mu_i &= \\bar y_i\n\\end{aligned}\n\\]\nFinding \\(\\hat \\sigma^2\\), \\[\n\\begin{aligned}\n0 =\\frac{\\partial \\ell}{\\partial \\sigma^2} \\bigg|_{\\mu_i = \\bar y_i}&=-\\frac{N}{2\\sigma^2}+\\sum_{i=1}^k \\sum_{j=1}^{n_i}  \\frac{(y_{ij}-\\bar y_i)^2}{2(\\sigma^2)^2} \\\\\n&=-N\\sigma^2+\\sum_{i=1}^k \\sum_{j=1}^{n_i}(y_{ij}-\\bar y_i)^2 \\\\\n\\implies \\sigma^2&=\\frac{\\sum_{i=1}^k \\sum_{j=1}^{n_i}(y_{ij}-\\bar y_i)^2}{N} \\\\\n&= SSE/N \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\left(\\sigma^2 \\right)^2} \\bigg|_{\\mu_i = \\bar y_i, \\sigma^2 = SSE/N}&=\\frac{N}{2(\\sigma^2)^2}-2\\sum_{i=1}^k \\sum_{j=1}^{n_i}  \\frac{(y_{ij}-\\bar y_i)^2}{2(\\sigma^2)^3}  \\bigg|_{\\sigma^2 = SSE/N} \\\\\n&=\\frac{N}{2(SSE/N)^2}-\\frac{SSE}{(SSE/N)^3} \\\\\n&=\\frac{N}{2(SSE/N)^2}-\\frac{N}{(SSE/N)^2} &lt;0. \\\\\n\\implies \\hat \\sigma^2 &= SSE/N\n\\end{aligned}\n\\] \n\n\nb.\nNow define \\(\\mathbf V_i^T = (Y_{i1} -\\bar {Y_i}, \\dots, Y_{i,n_i-1}- \\bar{Y_i})\\). Using standard matrix manipulations with the multivariate normal distribution, the density of \\(\\mathbf V_i\\) is given by \\[(2 \\pi)^{-(n_i-1)/2}n_i^{1/2}\\sigma^{-(n_i-1)} \\exp \\left(- \\frac{1}{2 \\sigma ^2} \\mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \\mathbf v_i \\right) \\] where \\(I_{n_i-1}\\) is the \\(n_i-1\\) by \\(n_i-1\\) identity matrix and \\(J_{n_i-1}\\) is an \\(n_i-1\\) by \\(n_i-1\\) matrix of 1’s. Now form the (marginal) likelihood based on \\(\\mathbf V_1, \\dots, \\mathbf V_k\\) and show that the MLE for \\(\\sigma^2\\) is now \\(\\sigma^2 = SSE/(N-k)\\).\nSolution:\n\\[\n\\begin{aligned}\n\\mathcal L (\\sigma^2; \\mathbf V_1,\\dots \\mathbf V_k) &= \\prod_{i=1}^k(2 \\pi)^{-(n_i-1)/2}n_i^{1/2}(\\sigma^2)^{-(n_i-1)/2} \\exp \\left(- \\frac{1}{2 \\sigma ^2} \\mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \\mathbf v_i \\right) \\\\\n\\ell (\\sigma^2; \\mathbf V_1,\\dots \\mathbf V_k) &= \\sum_{i=1}^k\\frac{-(n_i-1)\\log(2 \\pi)+n_i}{2}-\\frac{(n_i-1)}{2}\\log(\\sigma^2) - \\frac{1}{2 \\sigma ^2} \\mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \\mathbf v_i \\\\\n0=\\frac{d \\ell}{d \\sigma^2}&=\\sum_{i=1}^k \\frac{-(n_i-1)}{\\sigma^2}+\\frac{1}{2 (\\sigma^2)^2}  \\mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \\mathbf v_i \\\\\n&=\\sum_{i=1}^k -2\\sigma^2(n_i-1)+ \\mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \\mathbf v_i \\\\\n&=-2\\sigma^2(N-k)+\\sum_{i=1}^k\\sum_{j=1}^{n_i} 2(y_{ij} - \\bar y_i)^2\\\\\n\\implies  \\sigma^2 &= SSE/(N-k)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\bigg|_{\\sigma^2 = SSE/(N-k)}&=\\sum_{i=1}^k \\frac{(n_i-1)}{(\\sigma^2)^2}-\\frac{1}{(\\sigma^2)^3}  \\mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \\mathbf v_i \\\\\n&=\\sum_{i=1}^k \\frac{(n_i-1)}{(\\sigma^2)^2}-\\frac{1}{(\\sigma^2)^3} \\sum_{i=1}^k\\sum_{j=1}^{n_i} 2(y_{ij} - \\bar y_i)^2 \\\\\n&=\\frac{N-k}{(SSE/(N-k))^2}-\\frac{1}{(SSE/(N-k))^3}\\sum_{i=1}^k\\sum_{j=1}^{n_i} 2(y_{ij} - \\bar y_i)^2\\\\\n&=\\frac{N-k}{(SSE/(N-k))^2}-2\\frac{(N-k)}{(SSE/(N-k))^2}\\\\\n&=-1 \\\\\n\\implies  \\hat \\sigma^2 &= SSE/(N-k)\n\\end{aligned}\n\\]\n\n\nc.\nFinally, let us take a more general approach and assume that \\(Y\\) has an \\(N\\) dimensional multivariate normal distribution with mean \\(X \\mathbf{\\beta}\\) and covariance matrix \\(\\Sigma = \\sigma^2 Q(\\boldsymbol{\\theta})\\), where X is an \\(N\\times p\\) full rank matrix of known constants, \\(\\boldsymbol \\beta\\) is a \\(p\\)-vector of regression parameters, and \\(Q(\\boldsymbol{\\theta})\\) is an \\(N\\times N\\) standardized covariance matrix depending on the unknown parameter \\(\\boldsymbol{\\theta}\\). Typically \\(\\boldsymbol{\\theta}\\) would consist of variance component and/or spatial correlation parameters. We can concentrate the likelihood by noting that if \\(Q(\\boldsymbol{\\theta})\\) were known, then the generalized least squares estimator would be \\(\\boldsymbol{\\hat \\beta(\\theta)} =(X^T Q(\\boldsymbol{\\theta})^{-1}X)^{-1}X^TQ(\\boldsymbol{\\theta})^{-1} \\mathbf Y\\). Substituting for \\(\\boldsymbol \\beta\\) yields the profile log likelihood \\[-\\frac{N}{2} \\log(2\\pi) - N \\log \\sigma- \\frac{1}{2} \\log |Q(\\boldsymbol{\\theta})| - \\frac{GSSE(\\boldsymbol{\\theta})}{2\\sigma^2},\\] where \\(GSSE(\\boldsymbol{\\theta}) =(\\mathbf Y - X \\boldsymbol{\\hat{\\beta}}(\\boldsymbol{\\theta}))^T Q(\\boldsymbol{\\theta})^{-1}(\\mathbf Y -X\\boldsymbol{\\hat \\beta}(\\boldsymbol{\\theta}))\\). To connect with part a), let \\(Q(\\boldsymbol{\\theta})\\) be the identity matrix (so that \\(GSSE(\\boldsymbol{\\theta})\\) is just \\(SSE\\)) and find the maximum likelihood estimator of \\(\\sigma^2\\).\nSolution:\n\\[\n\\begin{aligned}\n\\ell(\\sigma^2 | Y) &= -\\frac{N}{2} \\log(2\\pi) - \\frac{N}{2} \\log \\sigma^2- \\frac{1}{2} \\log |Q(\\boldsymbol{\\theta})| - \\frac{GSSE(\\boldsymbol{\\theta})}{2\\sigma^2} \\\\\n0=\\frac{\\partial \\ell}{\\partial \\sigma^2} &= - \\frac{N}{2 \\sigma^2}+ \\frac{GSSE(\\boldsymbol{\\theta})}{2(\\sigma^2)^2}\\\\\n&=-N\\sigma^2 +GSSE(\\boldsymbol \\theta) \\\\\n\\implies \\sigma^2 &= GSSE(\\boldsymbol \\theta)/N \\\\\n\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\bigg|_{\\sigma^2=GSSE(\\boldsymbol \\theta)/N}&= \\frac{N}{2 (\\sigma^2)^2}-\\frac{GSSE(\\boldsymbol{\\theta})}{(\\sigma^2)^3}\\bigg|_{\\sigma^2=GSSE(\\boldsymbol \\theta)/N}\\\\\n&= \\frac{N}{2 (GSSE(\\boldsymbol \\theta)/N)^2}-\\frac{GSSE(\\boldsymbol{\\theta})}{(GSSE(\\boldsymbol \\theta)/N)^3} \\\\\n&= -1/2 \\\\\n\\implies \\hat \\sigma^2 &= GSSE(\\boldsymbol \\theta)/N \\\\\n\\end{aligned}\n\\] \n\n\nd.\nContinuing part c), the REML approach is to transform to \\(\\mathbf V = A^T \\mathbf Y\\), where the \\(N \\times p\\) columns of \\(A\\) are linearly independent and \\(A^T X =0\\) so that \\(\\mathbf V\\) is \\(MN(0;A^T \\Sigma A)\\) A special choice of \\(A\\) leads to the REML log likelihood \\[-\\frac{N-p}{2}\\log(2\\pi) -(N-p)\\log \\sigma - \\frac{1}{2} \\log |X^T Q(\\boldsymbol{\\theta} )^{-1}X|-\\frac{1}{2} \\log |Q(\\boldsymbol{\\theta})| - \\frac{GSSE(\\boldsymbol{\\theta})}{2 \\sigma^2}.\\] To connect with part b), let \\(Q(\\boldsymbol{\\theta})\\) be the identity matrix and find the maximum likelihood estimator of \\(\\sigma^2\\).\nSolution:\n\\[\\ell(\\sigma^2 | Y) = -\\frac{N-p}{2}\\log(2\\pi) -\\frac{(N-p)}{2}\\log (\\sigma^2) - \\frac{1}{2} \\log |X^T Q(\\boldsymbol{\\theta} )^{-1}X|-\\frac{1}{2} \\log |Q(\\boldsymbol{\\theta})| - \\frac{GSSE(\\boldsymbol{\\theta})}{2 \\sigma^2}\\] \\[\n\\begin{aligned}\n0=\\frac{\\partial \\ell}{\\partial \\sigma^2} &= - \\frac{N-p}{2 \\sigma^2}+ \\frac{GSSE(\\boldsymbol{\\theta})}{2(\\sigma^2)^2}\\\\\n&=-(N-p)\\sigma^2 +GSSE(\\boldsymbol \\theta) \\\\\n\\implies \\sigma^2 &= GSSE(\\boldsymbol \\theta)/(N-p) \\\\\n\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\bigg|_{\\sigma^2=GSSE(\\boldsymbol \\theta)/(N-p)}&= \\frac{N}{2 (\\sigma^2)^2}-\\frac{GSSE(\\boldsymbol{\\theta})}{(\\sigma^2)^3}\\bigg|_{\\sigma^2=GSSE(\\boldsymbol \\theta)/(N-p)}\\\\\n&= \\frac{N-p}{2 (GSSE(\\boldsymbol \\theta)/(N-p))^2}-\\frac{GSSE(\\boldsymbol{\\theta})}{(GSSE(\\boldsymbol \\theta)/(N-p))^3} \\\\\n&= -1/2 \\\\\n\\implies \\hat \\sigma^2 &= GSSE(\\boldsymbol \\theta)/(N-p) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-9",
    "href": "chapters/ch2.html#section-9",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.25",
    "text": "2.25\nIf \\(\\mathbf Y\\) is from an exponential family where \\((\\mathbf W, \\mathbf V)\\) are jointly sufficient for \\((\\boldsymbol \\theta_1, \\boldsymbol \\theta_2)\\), then the conditional density of \\(\\mathbf W | \\mathbf V\\) is free of the nuisance parameter \\(\\boldsymbol \\theta_2\\) and can be used as a conditional likelihood for estimating \\(\\boldsymbol \\theta_1\\). In some cases it may be difficult to find the conditional density. However, from (2.19, p. 57) we have \\[\\frac{f_{\\mathbf Y}(\\mathbf y; \\boldsymbol \\theta_1, \\boldsymbol \\theta_2)}{f_{\\mathbf V}(\\mathbf y; \\boldsymbol \\theta_1, \\boldsymbol \\theta_2)}= f_{\\mathbf{W|V}}(\\mathbf{w|v}; \\boldsymbol \\theta_1).\\] Thus, if you know the density of \\(\\mathbf Y\\) and of \\(\\mathbf V\\), then you can get a conditional likelihood equation.\n\na.\nNow let \\(\\mathbf Y_1, \\dots, \\mathbf Y_n\\) be iid \\(N(\\mu,\\sigma^2)\\), \\(V=\\bar Y\\), and \\(\\boldsymbol \\theta = (\\sigma, \\mu)^T\\). Form the ratio above and note that it is free of \\(\\mu\\). (It helps to remember that \\(\\sum (Y_i-\\mu)^2= \\sum (Y_i- \\bar Y)^2 + n(\\bar Y - \\mu)^2\\).)\nSolution:\nBy Central Limit Theorem, \\(\\mathbf V = \\bar Y \\sim N(\\mu, \\sigma^2/n)\\). Thus, \\[ \\begin{aligned}\n\\frac{f_{\\mathbf Y}(\\mathbf y; \\boldsymbol \\sigma, \\boldsymbol \\mu)}{f_{\\mathbf V}(\\mathbf y; \\boldsymbol \\sigma, \\boldsymbol \\mu)} &= \\frac{\\prod_{i=1}^n(2\\pi)^{-1/2}\\sigma^{-1}\\exp(-(y_i- \\mu)^2/2\\sigma^2)}{(2\\pi)^{-1/2}\\sigma^{-1}n^{1/2}\\exp(-n(\\bar y- \\mu)^2/2\\sigma^2)} \\\\\n&= \\frac{(2\\pi)^{-n/2}\\sigma^{-n}\\exp(-\\sum_{i=1}^n(y_i- \\mu)^2/2\\sigma^2)}{(2\\pi)^{-1/2}\\sigma^{-1}n^{1/2}\\exp(-n(\\bar y- \\mu)^2/2\\sigma^2)} \\\\\n&= \\frac{(2\\pi)^{-n/2}\\sigma^{-n}\\exp(-\\sum_{i=1}^n[(y_i- \\bar y)^2 + n(\\bar y - \\mu)^2]/2\\sigma^2)}{(2\\pi)^{-1/2}\\sigma^{-1}n^{1/2}\\exp(-n(\\bar y- \\mu)^2/2\\sigma^2)} \\\\\n&=(2\\pi)^{-(n-1)/2}\\sigma^{-(n-1)}n^{-1/2}\\exp\\left({-\\sum_{i=1}^n (Y_i- \\bar Y)^2}/{2\\sigma^2}\\right) &\\text{This is free of}~\\mu.\n\\end{aligned}\\]\n\n\nb.\nFind the conditional maximum likelihood estimator of \\(\\sigma^2\\).\nSolution:\n\\[\\begin{aligned}\n\\mathcal L (\\sigma^2| \\mathbf Y)&=(2\\pi)^{-(n-1)/2}(\\sigma^2)^{-(n-1)/2}n^{-1/2}\\exp\\left({-\\sum_{i=1}^n (Y_i- \\bar Y)^2}/{2\\sigma^2}\\right) \\\\\n\\ell (\\sigma^2| \\mathbf Y)&=\\frac{-(n-1)}{2}\\log(2\\pi)-\\frac{n-1}{2}\\log(\\sigma^2) -\\frac{\\log n}{2} -\\frac{\\sum_{i=1}^n (Y_i- \\bar Y)^2}{2\\sigma^2} \\\\\n0 = \\frac{\\partial \\ell}{\\partial \\sigma^2}&=-\\frac{n-1}{2(\\sigma^2)} +\\frac{\\sum_{i=1}^n (Y_i- \\bar Y)^2}{2(\\sigma^2)^2}\\\\\n\\implies \\sigma^2 &= \\frac{\\sum_{i=1}^n (Y_i- \\bar Y)^2}{n-1}= SSE/(n-1) \\\\\n\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\bigg|_{\\sigma^2 = SSE/(n-1)}&=\\frac{n-1}{2(\\sigma^2)^2} -\\frac{\\sum_{i=1}^n (Y_i- \\bar Y)^2}{(\\sigma^2)^3} \\bigg|_{\\sigma^2 = SSE/(n-1)} \\\\\n&=\\frac{n-1}{2(SSE/(n-1))^2} -\\frac{SSE}{(SSE/(n-1))^3}\\\\\n&= -1/2 &lt;0. \\\\\n\\implies \\hat \\sigma^2 &= \\frac{\\sum_{i=1}^n (Y_i- \\bar Y)^2}{n-1}= SSE/(n-1)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-10",
    "href": "chapters/ch2.html#section-10",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.26",
    "text": "2.26\nConsider the normal theory linear measurement error model \\[\\begin{aligned} Y_i = \\alpha + \\beta U_i +\\sigma_e e_i, & X_i =U_i+ \\sigma Z_i, & i = 1,\\dots,n \\end{aligned}\\] where \\(e_1, \\dots, e_n\\), \\(Z_1,\\dots, Z_n\\) are iid \\(N(0,1)\\) random variables, \\(\\sigma^2\\) is known, and \\(\\alpha,\\beta, \\sigma_e\\), and \\(U_1, \\dots, U_n\\) are unknown parameters.\n\na.\nLet \\(s_X^2\\) denote the sample variance of \\(X_1,\\dots, X_n\\). Show that \\(E(s_X^2)=s_U^2+\\sigma^2\\) where \\(s_U^2\\) is the sample variance of \\(\\{U_i\\}_1^n\\).\nSolution:\n\\[\n\\bar X = \\sum_{i=1}^n (U_i+\\sigma Z_i)/n = \\bar U + \\sigma \\bar Z, \\bar Z \\sim N(0,1/n)\n\\] \\[\\begin{aligned}\nE(s_X^2) &= E \\left(\\frac{\\sum_{i=1}^n (X_i -\\bar X)^2}{n-1} \\right).\n= E \\left(\\frac{\\sum_{i=1}^n X_i^2 -2X_i\\bar X + \\bar X^2}{n-1} \\right)\\\\\n&= E \\left(\\frac{\\sum_{i=1}^n (U_i + \\sigma Z_i)^2 -2(U_i + \\sigma Z_i)(\\bar U + \\sigma \\bar Z) +(\\bar U + \\sigma \\bar Z)^2}{n-1} \\right)\\\\\n&= E \\left(\\frac{\\sum_{i=1}^n U_i^2 + 2\\sigma  U_i Z_i+ \\sigma^2 Z_i^2 -2(U_i \\bar U + \\sigma U_i \\bar Z +\\sigma Z_i \\bar U + \\sigma^2Z_i \\bar Z) +\\bar U^2 + 2\\sigma \\bar U \\bar Z + \\sigma^2\\bar Z^2}{n-1} \\right)\\\\\n&= E \\left(\\frac{\\sum_{i=1}^n U_i^2-2U_i \\bar U+\\bar U^2 + \\sigma^2 Z_i^2  -2 \\sigma^2Z_i \\bar Z  + \\sigma^2\\bar Z^2}{n-1} \\right)\\\\\n&= E \\left(\\frac{\\sum_{i=1}^n(U_i- \\bar U)^2}{n-1} \\right) + \\sigma^2 E \\left(\\frac{\\sum_{i=1}^n(Z_i- \\bar Z)^2}{n-1} \\right)\\\\\n&= s_U^2 + \\sigma^2 E \\left(\\frac{\\sum_{i=1}^n(Z_i- \\bar Z)^2}{n-1} \\right)\\\\\n&= s_U^2 + \\sigma^2 E \\left(\\frac{\\sum_{i=1}^n(Z_i- \\bar Z)^2}{n-1} \\right)\\\\\n\\end{aligned}\n\\] Note that, \\[ \\begin{aligned}\n(n-1) \\left(\\frac{\\sum_{i=1}^n(Z_i- \\bar Z)^2}{n-1} \\right)\\sim \\chi^2(n-1) \\\\\n\\implies E \\left[(n-1) \\left(\\frac{\\sum_{i=1}^n(Z_i- \\bar Z)^2}{n-1} \\right)\\right] &= n-1 \\\\\n\\implies E \\left(\\frac{\\sum_{i=1}^n(Z_i- \\bar Z)^2}{n-1} \\right) &= 1\n\\end{aligned}\n\\] Therefore, \\(E(s_X^2)= s_U^2 + \\sigma^2 E \\left(\\frac{\\sum_{i=1}^n(Z_i- \\bar Z)^2}{n-1} \\right) =s_U^2 + \\sigma^2\\).\nFor the remainder of this problem assume that \\(s_U^2 \\to \\sigma_U^2\\) as \\(n \\to \\infty\\) and that \\(s_X^2\\) converges in probability to \\(\\sigma_U^2\\). (Note that even though \\(\\{U_i\\}_1^n\\) are parameters it still makes sense to talk about their sample variance, and denoting the limit of \\(s_U^2\\) as \\(n \\to \\infty\\) by \\(\\sigma_U^2\\) is simply a matter of convenience).\n\n\nb.\nShow that the estimate of slope from the least squares regression of \\(\\{Y_i\\}_1^n\\) on \\(\\{X_i\\}_1^n\\) (call it \\(\\hat{\\beta}_{Y|X}\\) is not consistent for \\(\\beta\\) as \\(n \\to \\infty\\)). This shows that it is not OK to simply ignore the measurement error in the predictor variable.\nSolution:\nIt is a well-known result that in SLR \\(\\hat{\\beta}_{Y|X}= \\frac{\\widehat{Cov(X,Y)}}{s^2_X}\\). Thus, \\[ \\begin{aligned}\n\\lim_{n \\to \\infty}\\hat{\\beta}_{Y|X} &= \\lim_{n \\to \\infty} \\frac{Cov(X,Y)}{s^2_X} \\\\\n&= \\frac{1}{\\sigma_U^2+ \\sigma^2} \\lim_{n \\to \\infty} \\frac{\\sum_{i=1}^n(X_i-\\bar X)(Y_i-\\bar Y)}{n-1} \\\\\n&= \\frac{1}{\\sigma_U^2+ \\sigma^2} \\lim_{n \\to \\infty} \\frac{\\sum_{i=1}^nX_iY_i-X_i\\bar Y-\\bar XY_i+\\bar X\\bar Y}{n-1} \\\\\nX_iY_i &= (U_i + \\sigma Z_i)(\\alpha + \\beta U_i +\\sigma_e e_i) \\\\\n&= \\alpha U_i + \\beta U_i^2 +\\sigma_e U_ie_i +\\alpha \\sigma Z_i+ \\beta \\sigma U_i Z_i +\\sigma_e\\sigma e_i Z_i \\\\\nX_i\\bar Y &= (U_i + \\sigma Z_i)(\\alpha + \\beta \\bar U +\\sigma_e \\bar e) \\\\\n&= \\alpha U_i + \\beta U_i \\bar U +\\sigma_e U_i \\bar e +\\alpha \\sigma Z_i+ \\beta \\sigma \\bar U Z_i +\\sigma_e\\sigma \\bar e Z_i \\\\\n\\bar XY_i &= (\\bar U + \\sigma \\bar Z)(\\alpha + \\beta U_i +\\sigma_e e_i) \\\\\n&= \\alpha \\bar U + \\beta U_i \\bar U +\\sigma_e \\bar Ue_i +\\alpha \\sigma \\bar Z+ \\beta \\sigma U_i \\bar Z +\\sigma_e\\sigma e_i \\bar Z \\\\\n\\bar X \\bar Y &= (\\bar U + \\sigma \\bar Z)(\\alpha + \\beta \\bar U +\\sigma_e \\bar e) \\\\\n&= \\alpha \\bar U + \\beta \\bar U^2 +\\sigma_e \\bar U \\bar e +\\alpha \\sigma \\bar Z+ \\beta \\sigma \\bar U \\bar Z +\\sigma_e\\sigma \\bar e \\bar Z \\\\\n\\implies \\lim_{n \\to \\infty}\\hat{\\beta}_{Y|X} &= \\frac{1}{\\sigma_U^2+ \\sigma^2} \\lim_{n \\to \\infty} \\frac{\\sum_{i=1}^n \\beta (U_i^2-2 U_i\\bar U +\\bar U^2)+\\sigma_e(U_i e_i -U_i \\bar e -\\bar U e_i + \\bar U \\bar e)}{n-1} \\\\\n&~~~~~+\\frac{\\sum_{i=1}^n \\beta \\sigma (U_iZ_i- \\bar U Z_i - U_i \\bar Z +\\bar U \\bar Z)+\\sigma_e \\sigma(e_i Z_i -\\bar e Z_i - e_i\\bar Z + \\bar e\\bar Z )}{n-1}  \\\\\n&= \\frac{1}{\\sigma_U^2+ \\sigma^2} \\lim_{n \\to \\infty} \\left[ \\beta s_U^2+\\frac{\\sum_{i=1}^n \\sigma_e \\sigma(e_i Z_i -\\bar e Z_i - e_i\\bar Z + \\bar e\\bar Z )}{n-1} \\right] ~~~~~~ \\left( \\overset{\\text{Central Limit Theorem,}}{e_i,Z_i, \\sqrt n \\bar e, \\sqrt n \\bar Z \\sim N(0,1)} \\right) \\\\\n&= \\frac{1}{\\sigma_U^2+ \\sigma^2} \\lim_{n \\to \\infty} \\left[ \\beta s_U^2+\\frac{\\sigma_e \\sigma \\left[\\sum_{i=1}^n (e_i - \\bar e) (Z_i- \\bar Z)\\right]}{n-1} \\right]\\\\\n&= \\frac{1}{\\sigma_U^2+ \\sigma^2} \\lim_{n \\to \\infty} \\left[ \\beta s_U^2+\\sigma_e \\sigma Cov(e_i,Z_i) \\right] \\\\\n&= \\frac{1}{\\sigma_U^2+ \\sigma^2}(\\beta s_U^2)~~~~~~~~~~~~~~~~~\\bigg(e_i,Z_i \\overset{iid}{\\sim} N(0,1) \\implies Cov(e_i,Z_i) =0 \\bigg) \\\\\n&= \\frac{\\beta s_U^2}{\\sigma_U^2+ \\sigma^2}\n\\end{aligned}\n\\] Therefore \\(\\lim_{n \\to \\infty}\\hat{\\beta}_{Y|X}\\) is not a consistent estimator of \\(\\beta\\).\n\n\nc. \nNow construct the full likelihood for \\(\\alpha,\\beta,\\sigma_{\\epsilon}^2, U_1,\\dots,U_n\\) and show that it has no sensible maximum. Do this by showing that the full likelihood diverges to \\(\\infty\\) when \\(U_i =(Y_i-\\alpha)/\\beta\\) for all \\(i\\) and \\(\\sigma_{\\epsilon}^2 \\to 0\\). This is another well-known example of the failure of maximum likelihood to produce meaningful estimates.\nSolution:\n\\[ \\begin{aligned}\nY_i &= \\alpha + \\beta U_i +\\sigma_e e_i \\sim N(\\alpha + \\beta U_i, \\sigma_{e}^2) \\\\\n\\implies \\mathcal L(\\alpha, \\beta, \\mathbf U, \\sigma_e | \\mathbf Y) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^2}}\\exp \\left(\\frac{-(y_i -(\\alpha +\\beta U_i))^2}{2\\sigma_{e}^2}\\right) \\\\\n\\lim_{\\sigma^2_{e} \\to 0}L(\\alpha, \\beta, \\mathbf U, \\sigma_e | \\mathbf Y)|_{U_i =(Y_i-\\alpha)/\\beta} &=\\lim_{\\sigma^2_{e} \\to 0} \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^2}}\\exp \\left(\\frac{-(y_i -(\\alpha +\\beta ((Y_i-\\alpha)/\\beta)))^2}{2\\sigma_{e}^2}\\right) \\\\\n&=\\lim_{\\sigma^2_{e} \\to 0}\\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^2}} = \\lim_{\\sigma^2_{e} \\to 0} (2 \\pi \\sigma_{e}^2)^{-n/2} \\\\\n&= \\infty\n\\end{aligned} \\]\n\n\nd.\nConsider the simple estimator (of \\(\\beta\\)) \\[\\hat \\beta_{MOM} = \\frac{s_X^2}{s_X^2-\\sigma^2} \\hat \\beta _{Y|X},\\] and show that it is a consistent estimator of \\(\\beta\\). This shows that consistent estimators exist, and thus the problem with maximum likelihood is not intrinsic to the model.\nSolution:\n\\[\n\\begin{aligned}\n\\lim_{n \\to \\infty}\\hat \\beta_{MOM} &= \\lim_{n \\to \\infty}\\frac{s_X^2}{s_X^2-\\sigma^2} \\hat \\beta _{Y|X} \\\\\n&=\\frac{\\sigma_U^2 + \\sigma^2}{\\sigma_U^2} \\left(\\frac{\\beta \\sigma_U^2}{\\sigma_U^2+ \\sigma^2}\\right) \\\\\n&=\\beta\n\\end{aligned}\n\\] \n\n\ne.\nAssuming that all other parameters are known, show that \\(T_i = Y_i \\beta/\\sigma_{e}^2+X_i/\\sigma^2\\) is a sufficient statistic for \\(U_i\\), \\(i=1,\\dots,n\\).\nSolution:\n\\[\n\\begin{aligned}\nX_i  &\\sim N(U_i, \\sigma^2)\\\\\nY_i &  \\sim N(\\alpha + \\beta U_i, \\sigma^2_e) \\\\\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\nf_{X_i,Y_i|U_i}(x,y)&=f_{Y_i|U_i}(y_i|u_i) f_{X_i|U_i}(x_i) \\\\\n&=\\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^2}}\\exp \\left(\\frac{-(y_i -(\\alpha +\\beta u_i))^2}{2 \\sigma_{e}^2}\\right) \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp \\left(\\frac{-(x_i- u_i)^2}{2 \\sigma^2}\\right)   \\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left(\\frac{-(y_i^2 -2y_i\\alpha - 2y_i \\beta u_i +\\alpha^2 +2\\alpha \\beta u_i + \\beta^2u_i^2)}{2 \\sigma_{e}^2}-\\frac{x_i^2-2x_iu_i+ u_i^2}{2 \\sigma^2}\\right)   \\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left(\\frac{-\\sigma^2((y_i-\\alpha)^2 - 2y_i \\beta u_i +2\\alpha \\beta u_i + \\beta^2u_i^2)-\\sigma_e^2(x_i^2-2x_iu_i+ u_i^2)}{2 \\sigma^2\\sigma_e^2}\\right)   \\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left(\\frac{(2\\sigma^2 y_i\\beta +2\\sigma_e^2x_i)u_i}{2\\sigma^2\\sigma^2_e}\\right)\\exp \\left(\\frac{-\\sigma^2((y_i-\\alpha)^2 +2\\alpha \\beta u_i + \\beta^2u_i^2)-\\sigma_e^2(x_i^2+ u_i^2)}{2 \\sigma^2\\sigma_e^2}\\right)   \\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left[ u_i\\left(\\frac{y_i\\beta}{\\sigma_e^2}+\\frac{ x_i}{\\sigma^2}\\right)\\right]\\exp \\left(\\frac{-(\\sigma^2(y_i-\\alpha)^2 +2\\sigma^2\\alpha \\beta u_i + \\sigma^2\\beta^2u_i^2+ \\sigma_e^2x_i^2+\\sigma_e^2u_i^2)}{2 \\sigma^2\\sigma_e^2}\\right)   \\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left[ u_i\\left(\\frac{y_i\\beta}{\\sigma_e^2}+\\frac{ x_i}{\\sigma^2}\\right)\\right]\\exp \\left[\\frac{-(\\sigma^2(y_i-\\alpha)^2+ \\sigma_e^2x_i^2)}{2 \\sigma^2\\sigma_e^2} -\\left(\\frac{\\alpha \\beta}{\\sigma_e^2}u_i + \\frac{\\beta^2}{2\\sigma_e^2}u_i^2+\\frac{1}{2\\sigma^2}u_i^2\\right)\\right]   \\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left[ u_i\\left(\\frac{y_i\\beta}{\\sigma_e^2}+\\frac{ x_i}{\\sigma^2}-\\frac{\\alpha \\beta}{\\sigma_e^2}\\right) -\\left(\\frac{\\beta^2}{2\\sigma_e^2}+\\frac{1}{2\\sigma^2}\\right)u_i^2\\right]\\exp \\left[\\frac{-(\\sigma^2(y_i-\\alpha)^2+ \\sigma_e^2x_i^2)}{2 \\sigma^2\\sigma_e^2}\\right]\\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left[ u_i\\left(t_i-\\frac{\\alpha \\beta}{\\sigma_e^2}\\right) -\\left(\\frac{\\beta^2}{2\\sigma_e^2}+\\frac{1}{2\\sigma^2}\\right)u_i^2\\right]\\exp \\left[\\frac{-(\\sigma^2(y_i-\\alpha)^2+ \\sigma_e^2x_i^2)}{2 \\sigma^2\\sigma_e^2}\\right]\\\\\n\\end{aligned}\n\\]\nThus, \\(\\left(\\frac{y_i\\beta}{\\sigma_e^2}+\\frac{ x_i}{\\sigma^2}\\right)\\) is sufficient for \\(U_i\\).\n\n\n\nf.\nFind the conditional distribution of \\(Y_i|T_i\\) and use it to construct a conditional likelihood for \\(\\alpha, \\beta\\), and \\(\\sigma_{\\epsilon}^2\\) in a manner similar to that for the logistic regression measurement error model.\nSolution:\n\\[ \\begin{aligned}\nT_i &= Y_i \\beta/\\sigma_{e}^2+X_i/\\sigma^2 \\implies X_i= (T_i-Y_i\\beta)\\left(\\frac{\\sigma^2}{\\sigma_e^2} \\right)\n\\end{aligned}\n\\] By the Jacobian method of transformations: \\[ \\begin{aligned}\nf_{Y_i,T_i} (y_i,t_i) &= f_{X_i,Y_i}\\left(y_i,(t_i-y_i\\beta)\\left(\\frac{\\sigma^2}{\\sigma_e^2}\\right)\\right)\\left| \\begin{matrix} 0&1/\\sigma^2 \\\\ 1 &  \\beta/\\sigma_e^2\\end{matrix} \\right| \\\\\n&=\\frac{1}{2 \\pi \\sigma_e \\sigma}\\exp \\left[ u_i\\left(\\frac{y_i\\beta}{\\sigma_e^2}+\\frac{(t_i-y_i\\beta)\\left(\\frac{\\sigma^2}{\\sigma_e^2}\\right)}{\\sigma^2}-\\frac{\\alpha \\beta}{\\sigma_e^2}\\right) -\\left(\\frac{\\beta^2}{2\\sigma_e^2}+\\frac{1}{2\\sigma^2}\\right)u_i^2\\right] \\\\\n&~~~~~~\\times\\exp \\left[\\frac{-(\\sigma^2(y_i-\\alpha)^2+ \\sigma_e^2\\left((t_i-y_i\\beta)\\left(\\frac{\\sigma^2}{\\sigma_e^2}\\right)\\right)^2)}{2 \\sigma^2\\sigma_e^2}\\right] \\left|\\frac{-1}{\\sigma^2}\\right|\\\\\n\\implies f_{Y_i|T_i=t_i}(y_i,t_i) &= \\frac{f_{Y_i, T_i}(y_i,t_i)}{f_{T_i}(t_i)}\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n&=\\frac{\\frac{1}{2 \\pi \\sigma_e \\sigma^3}\\exp \\left[ u_i\\left(\\frac{y_i\\beta}{\\sigma_e^2}+\\frac{(t_i-y_i\\beta)\\left(\\frac{\\sigma^2}{\\sigma_e^2}\\right)}{\\sigma^2}-\\frac{\\alpha \\beta}{\\sigma_e^2}\\right) -\\left(\\frac{\\beta^2}{2\\sigma_e^2}+\\frac{1}{2\\sigma^2}\\right)u_i^2\\right]\\exp \\left[\\frac{-(\\sigma^2(y_i-\\alpha)^2+ \\sigma_e^2\\left((t_i-y_i\\beta)\\left(\\frac{\\sigma^2}{\\sigma_e^2}\\right)\\right)^2)}{2 \\sigma^2\\sigma_e^2}\\right] }\n{\\frac{1}{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}} \\exp \\left[\\frac{-(t_i-(\\sigma_e^{-2}\\beta \\alpha+ \\sigma_e^{-2}\\beta^2 u_i+\\sigma^{-2}u_i))^2}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right]} \\\\\n&=\\frac{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\exp \\left[ u_i\\left(\\frac{y_i\\beta}{\\sigma_e^2}+\\frac{t_i-y_i\\beta}{\\sigma_e^2}-\\frac{\\alpha \\beta}{\\sigma_e^2}\\right) -\\left(\\frac{\\beta^2}{2\\sigma_e^2}+\\frac{1}{2\\sigma^2}\\right)u_i^2-\\frac{(y_i-\\alpha)^2+ (t_i-y_i\\beta)^2\\left(\\frac{\\sigma^2}{\\sigma_e^2}\\right)}{2 \\sigma_e^2}\\right] }\n{2 \\pi \\sigma_e \\sigma^3 \\exp \\left[\\frac{-(t_i-(\\sigma_e^{-2}\\beta \\alpha+ \\sigma_e^{-2}\\beta^2 u_i+\\sigma^{-2}u_i))^2}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right]} \\\\\n&=\\frac{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\exp \\left[ u_i\\left(\\frac{t_i-\\alpha \\beta}{\\sigma_e^2}\\right) -\\left(\\frac{\\beta^2}{2\\sigma_e^2}+\\frac{1}{2\\sigma^2}\\right)u_i^2-\\frac{(y_i-\\alpha)^2+ (t_i-y_i\\beta)^2\\left(\\frac{\\sigma^2}{\\sigma_e^2}\\right)}{2 \\sigma_e^2}\\right] }\n{2 \\pi \\sigma_e \\sigma^3 \\exp \\left[\\frac{-(t_i^2-2t_i\\sigma_e^{-2}\\beta \\alpha-2t_i \\sigma_e^{-2}\\beta^2 u_i-2t_i\\sigma^{-2}u_i+\\sigma_e^{-4}\\beta^2\\alpha^2 +2\\sigma_e^{-4}\\beta^3\\alpha u_i +2 \\sigma_e^{-2}\\sigma^{-2}\\alpha\\beta u_i+\\sigma_e^{-4}\\beta^4u_i^2 + 2\\sigma_e^{-2}\\sigma^{-2}\\beta^2u_i^2 + \\sigma^{-4}u_i^2)}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right]} \\\\\n&=\\frac{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\exp \\left[ \\frac{2u_i (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})(t_i-\\alpha\\beta)\\sigma_e^{-2} -(\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})\\left(\\beta^2\\sigma_e^{-2}+\\sigma^{-2}\\right)u_i^2- (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})\\left((y_i-\\alpha)^2+ (t_i-y_i\\beta)^2\\sigma^2\\sigma_e^{-2}\\right)\\sigma_e^{-2} }{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right] }\n{2 \\pi \\sigma_e \\sigma^3 \\exp \\left[\\frac{-\\left(t_i^2-2t_i\\sigma_e^{-2}\\beta \\alpha-2t_i \\sigma_e^{-2}\\beta^2 u_i-2t_i\\sigma^{-2}u_i+\\sigma_e^{-4}\\beta^2\\alpha^2 +2\\sigma_e^{-4}\\beta^3\\alpha u_i +2 \\sigma_e^{-2}\\sigma^{-2}\\alpha\\beta u_i+\\sigma_e^{-4}\\beta^4u_i^2 + 2\\sigma_e^{-2}\\sigma^{-2}\\beta^2u_i^2 + \\sigma^{-4}u_i^2\\right)}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right]} \\\\\n&=\\frac{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}}{2 \\pi \\sigma_e \\sigma^3}\\\\\n&~~~~~~~\\times\\exp \\left[ \\frac{2(\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})(t_i-\\alpha\\beta)\\sigma_e^{-2} u_i-2t_i \\sigma_e^{-2}\\beta^2 u_i-2t_i\\sigma^{-2}u_i+2\\sigma_e^{-4}\\beta^3\\alpha u_i +2 \\sigma_e^{-2}\\sigma^{-2}\\alpha\\beta u_i}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right] \\\\\n&~~~~~~~\\times \\exp \\left[ \\frac{-(\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})\\left(\\beta^2\\sigma_e^{-2}+\\sigma^{-2}\\right)u_i^2+\\sigma_e^{-4}\\beta^4u_i^2 + 2\\sigma_e^{-2}\\sigma^{-2}\\beta^2u_i^2 + \\sigma^{-4}u_i^2}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right] \\\\\n&~~~~~~\\times\\exp \\left[\\frac{t_i^2-2t_i\\sigma_e^{-2}\\beta \\alpha+\\sigma_e^{-4}\\beta^2\\alpha^2- (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})\\left((y_i-\\alpha)^2+ (t_i-y_i\\beta)^2\\sigma^2\\sigma_e^{-2}\\right)\\sigma_e^{-2}}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right] \\\\\n&=\\frac{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}}{2 \\pi \\sigma_e \\sigma^3}\\exp \\left[\\frac{t_i^2-2t_i\\sigma_e^{-2}\\beta \\alpha+\\sigma_e^{-4}\\beta^2\\alpha^2- (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})\\left((y_i-\\alpha)^2+ (t_i-y_i\\beta)^2\\sigma^2\\sigma_e^{-2}\\right)\\sigma_e^{-2}}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right] \\\\\n\\end{aligned}\n\\] Therefore, the conditional likelihood is: \\[\\mathcal L =\\frac{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}}{2 \\pi \\sigma_e \\sigma^3}\\exp \\left[\\frac{t_i^2-2t_i\\sigma_e^{-2}\\beta \\alpha+\\sigma_e^{-4}\\beta^2\\alpha^2- (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})\\left((y_i-\\alpha)^2+ (t_i-y_i\\beta)^2\\sigma^2\\sigma_e^{-2}\\right)\\sigma_e^{-2}}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\right]\\]\nAnd the MLEs based on this are:\n{% raw %}\n\\[ \\begin{aligned}\n\\ell (\\alpha, \\beta, \\sigma_e^2, \\sigma^2 ; Y_i, T_i) &= \\log \\left(\\frac{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}}{2 \\pi \\sigma_e \\sigma^3}\\right)+\\frac{t_i^2-2t_i\\sigma_e^{-2}\\beta \\alpha+\\sigma_e^{-4}\\beta^2\\alpha^2- (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})\\left((y_i-\\alpha)^2+ (t_i-y_i\\beta)^2\\sigma^2\\sigma_e^{-2}\\right)\\sigma_e^{-2}}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})} \\\\\n0 = \\frac{\\partial \\ell}{\\partial \\alpha} &= \\frac{2t_i\\sigma_e^{-2}\\beta +2\\sigma_e^{-4}\\beta^2\\alpha-2(\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})(y_i-\\alpha)}{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}\\\\\n&= t_i\\sigma_e^{-2}\\beta -\\beta^2 \\sigma_e^{-2}y_i- \\sigma^{-2}y_i +\\beta^2 \\sigma_e^{-2}\\alpha+\\sigma^{-2}\\alpha\\\\\n\\implies \\alpha &= \\frac{t_i\\sigma_e^{-2}\\beta-\\beta^2 \\sigma_e^{-2}y_i- \\sigma^{-2}y_i}{\\sigma_e^{-4}\\beta^2-\\beta^2 \\sigma_e^{-2}-\\sigma^{-2}}\\\\\n0 = \\frac{\\partial \\ell}{\\partial \\beta} &=  \\left(\\frac{1}{\\sqrt{2\\pi (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}}\\right)\\left(\\frac{{2 (\\beta^2 \\sigma_e^{-2}+ \\sigma^{-2})}[-2t_i\\sigma_2^{-2}\\alpha+2\\sigma_e^{-4}\\alpha^2\\beta-2\\beta\\sigma_e^{-4}[(y_i-\\alpha)^2-(t_i-y_i\\beta)^2\\sigma^2\\sigma_e^2]+2y_i(t_i-y_i\\beta)\\sigma^2\\sigma_e^{-2}(\\beta^2\\sigma_e^{-2}+\\sigma^{-2})]}{4(\\beta^2\\sigma_e^{-2}+\\sigma^{-2})^2}\\right)\\\\\n&=  -2t_i\\sigma_2^{-2}\\alpha+2\\sigma_e^{-4}\\alpha^2\\beta-\\beta\\sigma_e^{-4}[(y_i-\\alpha)^2-(t_i-y_i\\beta)^2\\sigma^2\\sigma_e^2]+y_i(t_i-y_i\\beta)\\sigma^2\\sigma_e^{-2}(\\beta^2\\sigma_e^{-2}+\\sigma^{-2})\\\\\n\\end{aligned}\\] {% endraw %}\nRan out of time to finish problem. but just taking derivatives to find MLEs of conditional likelihood from here."
  },
  {
    "objectID": "chapters/ch2.html#section-11",
    "href": "chapters/ch2.html#section-11",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.35.",
    "text": "2.35.\nDerive the Fisher information matrix \\(\\mathbf I(\\boldsymbol \\theta)\\) for the reparameterized ZIP model:\\[\\begin{aligned} P(Y =0) = \\pi\\\\ P(Y=y) = \\left(\\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^ye^{-y}}{y!} \\end{aligned}\\]\nSolution:\n\\[\\begin{aligned}\nP(Y =0) &= \\pi\\\\\nP(Y=y) &= \\left(\\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^ye^{-y}}{y!} \\\\\n\\implies \\mathcal L(\\pi, \\lambda |y) &= \\pi^{I(y=0)} \\left[\\left(\\frac{1-\\pi}{1-e^{-\\lambda}}\\right)\\frac{\\lambda^ye^{-y}}{y!}\\right]^{I(y \\in \\mathbb N^+)} \\\\\n\\implies \\ell(\\pi, \\lambda |y) &= \\log (\\pi) I(y=0) + \\left[\\log (1-\\pi) - \\log(1-e^{-\\lambda})+y \\log (\\lambda) -y -\\log (y!)\\right]I(y \\in \\mathbb N^+) \\\\\nS(\\pi,\\lambda) = \\frac{\\partial \\ell}{\\partial (\\pi,\\lambda)^T} &= \\begin{pmatrix}\\frac{1}{\\pi}I(y= 0)-\\frac{1}{1-\\pi} I(y \\in \\mathbb N^+) \\\\ \\left[-\\frac{e^{-\\lambda}}{1-e^{-\\lambda}} + \\frac{y}{\\lambda} \\right]I(y \\in \\mathbb N^+)\\end{pmatrix} \\\\\nI(\\theta) = - E \\left[\\frac{\\partial S}{\\partial (\\pi,\\lambda)} \\right] &= -E\\begin{pmatrix}\\frac{-1}{\\pi^2}I(y= 0)+\\frac{1}{(1-\\pi)^2} I(y \\in \\mathbb N^+) & 0 \\\\ 0& \\left[-\\frac{-e^{-\\lambda}(1-e^{-\\lambda})-e^{-\\lambda}(e^{-\\lambda})}{1-e^{-\\lambda}} - \\frac{y}{\\lambda^2} \\right]I(y \\in \\mathbb N^+)\\end{pmatrix} \\\\\n&= -\\begin{pmatrix}\\frac{-1}{\\pi^2}\\pi-\\frac{1}{(1-\\pi)^2} (1-\\pi) & 0 \\\\ 0& \\left[-\\frac{-e^{-\\lambda}(1-e^{-\\lambda})-e^{-\\lambda}(e^{-\\lambda})}{1-e^{-\\lambda}} - \\frac{\\left(\\frac{1}{1-e^{-\\lambda}}\\right)\\lambda}{\\lambda^2} \\right](1-\\pi))\\end{pmatrix} \\\\\n&=\\begin{pmatrix}\\frac{1}{\\pi}+\\frac{1}{1-\\pi} & 0 \\\\ 0& \\left[\\frac{-e^{-\\lambda}}{1-e^{-\\lambda}} + \\frac{1}{(1-e^{-\\lambda})\\lambda} \\right](1-\\pi)\\end{pmatrix}\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-12",
    "href": "chapters/ch2.html#section-12",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.37",
    "text": "2.37\nSuppose that \\(Y_1, \\dots, Y_n\\) are iid from the one parameter exponential family \\(f(y;\\theta)= \\exp[yg(\\theta)-b(\\theta)+c(y)]\\).\n\na.\nFor \\(g(\\theta) = \\theta\\), find \\(\\bar{\\mathbf{I}}(Y,\\theta)\\) (sample version) and explain why it is the same as \\(I(\\theta)\\) (Fisher information).\nSolution:\n\\[\n\\begin{aligned}\n\\mathcal L(\\theta|\\mathbf Y) &= \\prod_{i=1}^n \\exp[y_ig(\\theta)-b(\\theta)+c(y_i)] \\\\\n\\implies \\ell (\\theta | \\mathbf Y) &= \\sum_{i=1}^n y_i \\theta-b(\\theta)+c(y_i) \\\\\n\\implies S (\\theta | \\mathbf Y) &= \\sum_{i=1}^n y_i -b'(\\theta)\\\\\n\\implies \\frac{d S (\\theta | \\mathbf Y)}{d \\theta} &= -b''(\\theta) = -nb''(\\theta) \\\\\n\\implies \\bar{\\mathbf{I}}(Y,\\theta) &= \\frac{1}{n} \\sum_{i=1}^n \\left[-\\frac{d S (\\theta | Y_i)}{d \\theta}\\right] \\\\\n&= b''(\\theta)\n\\end{aligned}\n\\]\nThe fisher information is the same as the sample fisher information as the derivative of the score function is constant with respect to the sample data. The fisher information and sample information should be equal when variables are iid.\n\n\nb.\nNow for general differentiable \\(g(\\theta)\\), find \\(I(\\theta)\\)\nSolution:\n\\[\n\\begin{aligned}\n\\mathcal L(\\theta|\\mathbf Y) &= \\prod_{i=1}^n \\exp[y_ig(\\theta)-b(\\theta)+c(y_i)] \\\\\n\\implies \\ell (\\theta | \\mathbf Y) &= \\sum_{i=1}^n y_i g(\\theta)-b(\\theta)+c(y_i) \\\\\n\\implies S (\\theta | \\mathbf Y) &= \\sum_{i=1}^n y_ig'(\\theta) -b'(\\theta)\\\\\n\\implies \\frac{d S (\\theta | \\mathbf Y)}{d \\theta} &= \\sum_{i=1}^n y_ig''(\\theta) -b''(\\theta) \\\\\n&= n \\bar y g''(\\theta) -nb''(\\theta) \\\\\n\\implies I(Y,\\theta) &= - E \\left[\\frac{d S (\\theta | Y_i)}{d \\theta}\\right] \\\\\n&= -E\\left[y_ig''(\\theta) -b''(\\theta)\\right] \\\\\n&= - \\left[b'(\\theta) g''(\\theta) -b''(\\theta)\\right] & E(y_i) = b'(\\theta) \\\\\n&= b''(\\theta) - b'(\\theta) g'' (\\theta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-13",
    "href": "chapters/ch2.html#section-13",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.41",
    "text": "2.41\nUse simulation to verify that the information matrix for Example 2.21 (p. 78) is correct when \\(\\mu =1\\) and \\(\\sigma =1\\). One approach is to generate samples of size \\(n=100\\) (or larger) from a normal(1,1) distribution and exponentiate to get lognormal data. Then form the log likelihood and use a numerical derivative routine to find the second derivative matrix for each sample. Then average over 1000 replications and compare to the given information matrix.\nSolution:\n\n\nCode\nlibrary(numDeriv)\nset.seed(11062023)\nn = 100\nn_sim = 1000\nparams = c(mu = 1, sigma = 1, lambda = 0)\nfisher_info = list()\n\n##Using Sima's code as a base\n\n# Define log-likelihood function of box-cox transformation\n# Source: Scott Hyde master's thesis, Montana State University\n#https://math.montana.edu/grad_students/writing-projects/Pre-2000/99hyde.pdf\n\nlog_likelihood &lt;- function(params, y) {\n  mu = params[1]\n  sigma = params[2]\n  lambda = params[3]\n  \n  if(lambda == 0){\n    z= log(y)\n  } else{\n    z = (y^(lambda)-1)/lambda\n  }\n  \n  log_lik &lt;- sum(-1/(2*sigma^2)*(z-mu)^2 - 0.5*log(2*pi*sigma^2)+(lambda-1)*log(y))\n  return(log_lik)  # Return log-likelihood\n}\nfor(i in 1:n_sim){\n  y = exp(rnorm(n, mean = params[\"mu\"], sd = params[\"sigma\"]))\n  fisher_info[[i]] = -hessian(log_likelihood,params,y= y)/n\n}\n\nmean_fisher_info = Reduce(\"+\", fisher_info) / length(fisher_info)\n\n\n## Get matrix as defined in book\n\ntau_1 = (params[\"sigma\"]^2+params[\"mu\"]^2)/2\ntau_2 = (7*params[\"sigma\"]^4+10*params[\"sigma\"]^2*params[\"mu\"]^2+params[\"mu\"]^4)/4\n\nbook = matrix(c(1,0,-tau_1,\n                0,2,-2*params[\"sigma\"]*params[\"mu\"],\n                -tau_1,-2*params[\"sigma\"]*params[\"mu\"], tau_2),nrow =3,ncol= 3)\n\nmean_fisher_info\n\n\n              [,1]          [,2]      [,3]\n[1,]  1.0000000000 -0.0007235042 -0.998414\n[2,] -0.0007235042  1.9926544888 -2.001938\n[3,] -0.9984139964 -2.0019375153  4.503790\n\n\nCode\nbook\n\n\n     [,1] [,2] [,3]\n[1,]    1    0 -1.0\n[2,]    0    2 -2.0\n[3,]   -1   -2  4.5\n\n\nThe mean of the fisher information matrices that I simulated is printed above, with the fisher info the book describes right below it. These two matrices are very close together, which means the simulation supports the book’s example."
  },
  {
    "objectID": "chapters/ch2.html#section-14",
    "href": "chapters/ch2.html#section-14",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.43",
    "text": "2.43\nFor the generalized linear model with link function \\(g\\), not necessarily the canonical link, write down the likelihood function and show how to obtain the likelihood score equation, \\[S(\\boldsymbol \\beta, \\phi) = \\sum_{i=1}^n \\mathbf D_i \\frac{(Y_i-\\mu_i)}{Var(Y_i)}=0, \\text{ where } \\mathbf D_i =\\mathbf D_i(\\boldsymbol \\beta) = \\frac{\\partial \\mu_i(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta^T}\\] (In the above expression we have suppressed the dependence of \\(D_i\\) and \\(\\mu_i\\) on \\(\\boldsymbol \\beta\\).) The key idea used is the chain rule and the fact that the derivative of \\(\\theta_i= b'^{-1}(\\mu_i)\\) with respect to \\(\\mu_i\\) is \\(1/b''(\\theta_i)\\).\nSolution:\n\\[\n\\begin{aligned}\n\\ell (y_i;\\theta_i,\\phi) &= \\frac{y_i\\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i,\\phi) \\\\\n&= \\frac{y_ib'^{-1}(\\mu_i) - b(b'^{-1}(\\mu_i))}{a_i(\\phi)} + c(y_i,\\phi) \\\\\nS(\\beta, \\phi) &= \\frac{y_ib''^{-1}(\\mu_i)\\mathbf D_i - b'(b'^{-1}(\\mu_i))b''^{-1}(\\mu_i) \\mathbf D_i}{a_i(\\phi)} \\\\\n&= \\frac{y_i\\mathbf D_i - b'(b'^{-1}(\\mu_i)) \\mathbf D_i}{b''(\\theta_i)a_i(\\phi)} & \\left[b''^{-1}(\\mu_i) = \\frac{1}{b''(\\theta_i)}\\right] \\\\\n&= \\frac{y_i\\mathbf D_i - \\mu_i \\mathbf D_i}{b''(\\theta_i)a_i(\\phi)} \\\\\n&= \\mathbf D_i\\frac{Y_i - \\mu_i}{Var(Y_i)} & \\left[Var(Y_i) = b''(\\theta_i)a_i(\\phi)\\right]\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-15",
    "href": "chapters/ch2.html#section-15",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.44",
    "text": "2.44\nContinuing the last problem, show that the Fisher information matrix for the \\(\\boldsymbol \\beta\\) part of the generalized linear model is given by \\[\\bar{\\mathbf I} ( \\boldsymbol  \\beta ) = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\mathbf{D_i D_i}^T}{Var(Y_i)}\\] Here you can use either of two methods: a) take the expectation of the negative of the derivative of \\(S(\\boldsymbol \\beta, \\phi)\\), and noting that all the ugly derivatives drop out because \\(E(Y_i-\\mu_i)=0\\); or b) the individual summed components of \\(\\mathbf{\\bar I}(\\boldsymbol \\beta)\\) can also be found using the cross-product definition of information in (2.39, p. 67).\nSolution:\n\\[\n\\begin{aligned}\n\\bar{\\mathbf I}(\\boldsymbol \\beta)\n&= \\frac{1}{n}\\sum_{i=1}^n E\\left[\\mathbf s_i(\\boldsymbol \\beta) \\mathbf s_i(\\boldsymbol \\beta)^T\\right] \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n E\\left[\\mathbf D_i\\frac{Y_i - \\mu_i}{Var(Y_i)}\\mathbf D_i^T\\frac{Y_i - \\mu_i}{Var(Y_i)}\\right] \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n \\mathbf D_i \\mathbf D_i^T\\frac{E[(Y_i - \\mu_i)^2]}{[Var(Y_i)]^2} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n \\mathbf D_i \\mathbf D_i^T\\frac{Var(Y_i)}{[Var(Y_i)]^2} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n \\frac{\\mathbf D_i \\mathbf D_i^T}{Var(Y_i)} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-16",
    "href": "chapters/ch2.html#section-16",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.45",
    "text": "2.45\nSuppose that \\(X_1\\) and \\(X_2\\) are independent and continuous random variables with densities \\(f_1\\) and \\(f_2\\), respectively. \\(Z\\) is a Bernoulli(\\(p\\)) random variable and independent of \\(X_1\\) and \\(X_2\\). Define \\(Y= ZX_1 +(1-ZX_2)\\).\n\na)\nUse the \\(2h\\) method to show that the joint density of \\((Y,Z)\\) is given by \\[f_{Y,Z}(y,z) = [pf_1(y)]^z[(1-p)f_2(y)]^{1-z}\\]\nSolution:\n\\[\\begin{aligned}\nf_{Y,Z} (y,z) &= lim_{h \\to 0^+} (2h)^{-2} P\\left(Y \\in(y-h, y+h], Z \\in (z-h,z+h]\\right) \\\\\n&= lim_{h \\to 0^+} (2h)^{-1} P\\left(Y \\in(y-h, y+h], Z = z\\right) ~~~~~~~~~\\text{(for small }h) \\\\\n&= lim_{h \\to 0^+} (2h)^{-1} \\big[P(z\\,x_1+ (1-z)\\,x_2 \\leq y+h], z= 0) -P(z\\,x_1+ (1-z)\\,x_2 &lt; y-h], z= 0) \\\\ &~~~~~~~~~~~~~~~~~~+\nP(z\\,x_1+ (1-z)\\,x_2 \\leq y+h], z= 1) -P(z\\,x_1+ (1-z)\\,x_2 &lt; y-h], z= 1)\\big] \\\\\n&= lim_{h \\to 0^+} (2h)^{-1} \\big[P(z=0)[P(x_2 \\leq y+h]) -P(\\,x_2 &lt; y-h])] \\\\&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+P(z=1)[P(x_1\\leq y+h]) -P(x_1 &lt; y-h])\\big] \\\\\n&= lim_{h \\to 0^+} (2h)^{-1} \\big\\{p[P((x_2 \\leq y+h]) -P(\\,x_2 &lt; y-h])]^{1-z} \\\\\n&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\times\\left[(1-p)[P(z\\,x_1 \\leq y+h]) -P(x_1 &lt; y-h]) \\right]^{z} \\big\\} \\\\\n&= [pf_1(y)]^z[(1-p)f_2(y)]^{1-z}\n\\end{aligned}\n\\]\n\n\nb)\nUse the \\(2h\\) method to show that \\[P(Z=1|Y=y) = \\frac{pf_1(y)}{pf_1(y) + (1-p)f_2(y)}\\]\nSolution:\nI have already used the \\(2h\\) method in the problem above to find the pdf \\(f_{Y,Z}(y,z)\\), now we can use simple probability rules for the rest of the work:\n\\[\\begin{aligned}\nP(Z=1|Y=y) &= \\frac{P(Z=1,Y=y)}{P(Y=y)}\\\\\n&= \\frac{P(Z=1,Y=y)}{\\sum_{Z}P(Y=y,Z=z)}\\\\\n&= \\frac{P(Z=1,Y=y)}{P(Y=y,Z=0) + P(Y=y,z=1)}\\\\\n&= \\frac{f_{Y,Z} (y,z=1)}{f_{Y,Z} (y,z=0) + f_{Y,Z} (y,z=1)} \\\\\n&= \\frac{pf_1(y)}{[(1-p)f_2(y)] + [pf_1(y)]}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-17",
    "href": "chapters/ch2.html#section-17",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.47",
    "text": "2.47\nA mixture of three component densities has the form \\[f(y;\\boldsymbol \\theta, \\mathbf p) = p_1f_1(y;\\boldsymbol \\theta) + p_2f_2(y;\\boldsymbol \\theta) + p_3f_3(y;\\boldsymbol \\theta),\\] where \\(p_1= p_2 = p_3 = 1\\). We observe an iid sample \\(Y_1,\\dots, Y_n\\) from \\(f(y;\\boldsymbol \\theta, \\mathbf p )\\).\n\na.\nShow how to define multinomial(\\(1;p_1,p_2,p_3\\)) vectors \\((Z_{i1},Z_{i2},Z_{i3})\\) to get a representation for the \\(Y_i\\) from \\(f(y;\\boldsymbol \\theta, \\mathbf p )\\) based on independent random variables \\((X_{i1},X_{i2},X_{i3})\\) from the individual components.\nSolution:\nLet \\(Z_{i1} \\sim Bernoulli(p_1),Z_{i2} \\sim Bernoulli(p_2),Z_{i3} \\sim Bernoulli(p_3)\\) such that \\((Z_{i1},Z_{i2},Z_{i3}) \\sim multinomial(1;p_1,p_2,p_3)\\) Also, let the pdfs of \\(X_{i1},X_{i2},X_{i3}\\) be \\(f_1(y;\\boldsymbol \\theta), f_1(y;\\boldsymbol \\theta), f_1(y;\\boldsymbol \\theta)\\) respectively.\nDefine \\(Y_i = Z_{i1}X_{i1}+ Z_{i2}X_{i2} + Z_{i3}X_{i3}\\).\nThus, \\(f_{Y_i}(y) = p_1f_1(y;\\boldsymbol \\theta) + p_2f_2(y;\\boldsymbol \\theta) + p_3f_3(y;\\boldsymbol \\theta)\\)\n\n\nb.\nGive the complete data log likelihood and the function \\(Q\\) to be maximized at the M step.\nSolution:\nThe joint distribution \\(f(Y_i,Z_i)\\) can be found as follows:\n\\[\n\\begin{aligned}\nf(Y_i,Z_i) &= f(Y_i|Z_i)f(Z_i) \\\\\n&= f(Z_{i1}X_{i1}+ Z_{i2}X_{i2} + Z_{i3}X_{i3}|Z_i)f(Z_i) \\\\\n&= f_{X_{i1}}(y_i|Z_{i1})f_{X_{i2}}(y_i|Z_{i2})f_{X_{i3}}(y_i|Z_{i3})f(Z_i)\\\\\n&= [f_{X_{i1}}(y_i;\\boldsymbol \\theta)]^{z_{i1}}[f_{X_{i2}}(y_i;\\boldsymbol \\theta)]^{Z_{i2}}[f_{X_{i3}}(y_i;\\boldsymbol \\theta)]^{Z_{i3}}p_1^{z_{i1}}p_2^{z_{i2}}p_3^{z_i3}\n\\end{aligned}\n\\] Thus, the complete log likelihood is \\[\\begin{aligned}\\ell_C(\\boldsymbol \\theta,p_1,p_2,p_3|\\mathbf Y,\\mathbf Z) &= \\sum_{i=1}^n z_{i1}\\log(f_{X_{i1}}(\\boldsymbol \\theta;y_i))+z_{i2}\\log (f_{X_{i2}}(\\boldsymbol \\theta;y_i))+z_{i3}\\log(f_{X_{i3}}(\\boldsymbol \\theta;y_i))\\\\\n&~~~~~~~~~~~~~~~~~~~~~+z_{i1}\\log p_1+z_{i2}\\log p_2+z_{i3} \\log p_3 \\end{aligned}\\]\nand, \\[\\begin{aligned}Q(\\boldsymbol \\theta,p_1,p_2,p_3,\\boldsymbol \\theta^v,p_1^v,p_2^v,p_3^v,\\mathbf Y) &= E_{(\\boldsymbol \\theta^v,p_1^v,p_2^v,p_3^v)} [\\ell_C(\\boldsymbol \\theta,p_1,p_2,p_3|\\mathbf Y,\\mathbf Z)]\\\\\n&=\\sum_{i=1}^n \\bigg\\{w_{i1}^v\\log(f_{X_{i1}}(y_i;\\boldsymbol \\theta))+w_{i2}^v\\log (f_{X_{i2}}(y_i;\\boldsymbol \\theta))+w^v_{i3}\\log(f_{X_{i3}}(y_i;\\boldsymbol \\theta))\\\\\n&~~~~~~~~~~~~~~~~~~~~~+w^v_{i1}\\log p_1+w^v_{i2}\\log p_2+w^v_{i3} \\log p_3\\bigg\\} \\end{aligned}\\]\nWhere \\(\\begin{aligned} w_{ij}^v= E_{(\\boldsymbol \\theta^v,p_1^v,p_2^v,p_3^v)}[Z_{ij}|Y_i]&= \\frac{p_j^{v}f_j(Y_i,\\boldsymbol \\theta^v)}{\\sum_{j=1}^n p_j^vf_j(Y_i,\\boldsymbol \\theta^v)}\\end{aligned}\\)"
  },
  {
    "objectID": "chapters/ch2.html#section-18",
    "href": "chapters/ch2.html#section-18",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.48",
    "text": "2.48\nSuppose that the data \\(Y_1, \\dots, Y_n\\) are assumed to come from a mixture of two binomial distributions. Thus \\[f(y;p,\\theta_1,\\theta_2) = p {n \\choose y} \\theta_1^y(1-\\theta_1)^{n-y} +(1-p) {n \\choose y} \\theta_2^y(1-\\theta_2)^{n-y}.\\] Find \\(Q(p,\\theta_1,\\theta_2,p^v,\\theta_1^v,\\theta_2^v)\\) and the updating formulas.\nSolution:\nDefine \\(Z_i \\sim Bernoulli(p)\\), \\(Y_i = ZX_{1i}+ (1-Z)X_{2i}\\), where \\(X_{1i} \\sim Binom(n,\\theta_1)\\), \\(X_{2i} \\sim Binom(n,\\theta_2)\\) and \\(X_{1i},X_{2i}\\) are independent of \\(Z\\). Thus, \\(Y_i\\) will have a mixture density equal to \\(f(y;p,\\theta_1,\\theta_2)\\).\nTherefore, \\[\\mathcal L_C(p,\\theta_1,\\theta_2;y_i,Z) = \\left[p f_{X_1}(y_i;n,\\theta_1)\\right]^{z_i} \\left[(1-p) f_{X_2}(y_i;n,\\theta_2)\\right]^{1-{z_i}}\\] And, following steps outlined on page 85 of Boos, \\[\\begin{aligned}\nQ(p,\\theta_1,\\theta_2,p^v,\\theta_1^v,\\theta_2^v) &= \\sum_{i=1}^n  w_i^v \\log f_{X_1}(y_i;n,\\theta_1)+(1-w^v_i) \\log f_{X_2}(y_i;n,\\theta_2) +w^v_i \\log p + (1-w^v_i)\\log(1-p)\n\\end{aligned}\n\\]\nWhere \\(w^v_i = \\frac{p^vf_{X_1}(y_i;n,\\theta_1)}{p^vf_{X_1}(y_i;n,\\theta_1)+(1-p^v)f_{X_2}(y_i;n,\\theta_2)}\\).\nSubstituting in the distributions,\n\\[\n\\begin{aligned}\nQ(p,\\theta_1,\\theta_2,p^v,\\theta_1^v,\\theta_2^v) &= \\sum_{i=1}^n \\bigg\\{ w_i^v \\log \\left[{n \\choose y_i} \\theta_1^{y_i}(1-\\theta_1)^{n-y_i}\\right] +(1-w_i^v) \\log \\left[{n \\choose y_i} \\theta_2^{y_i}(1-\\theta_2)^{n-y_i}\\right]\\\\\n&~~~~~~~~~ +w_i^v \\log p + (1-w_i^v)\\log(1-p)\\bigg\\} \\\\\n&= \\sum_{i=1}^n \\bigg\\{ w_i^v \\left[ \\log {n \\choose y_i} + y_i \\log \\theta_1 +\\log y_i +(n-y_i)\\log(1-\\theta_1)\\right]\\\\\n&~~~~~~~~~  +(1-w_i^v) \\left[ \\log {n \\choose y_i} + y_i \\log \\theta_2 +\\log y_i +(n-y_i)\\log(1-\\theta_2)\\right]\\\\\n&~~~~~~~~~ +w_i^v \\log p + (1-w_i^v)\\log(1-p)\\bigg\\}\n\\end{aligned}\n\\]\nNow to maximize \\(Q\\), \\[ \\begin{aligned}\n0= \\frac{\\partial Q}{\\partial \\theta_1} &=\\sum_{i=1}^n \\frac{w_i^vy_i}{\\theta_1}-\\frac{w_i^v(n-y_i)}{1-\\theta_1} \\\\\n\\implies 0&= \\sum_{i=1}^n w_i^v(y_i-y_i\\theta_1-(n-y_i)\\theta_1)\\\\\n\\implies 0&= \\sum_{i=1}^n w_i^v(y_i-n\\theta_1)\\\\\n\\implies \\theta_1^{v+1} &= \\frac{\\sum_{i=1}^n w_i^vy_i}{\\sum_{i=1}^n w_i^vn}\n\\end{aligned}\n\\] Similarly, \\[ \\begin{aligned}\n0= \\frac{\\partial Q}{\\partial \\theta_1} &=\\sum_{i=1}^n \\frac{(1-w_i^v)y_i}{\\theta_2}-\\frac{(1-w_i^v)(n-y_i)}{1-\\theta_2} \\\\\n\\implies 0&= \\sum_{i=1}^n (1-w_i^v)(y_i-y_i\\theta_2-(n-y_i)\\theta_2)\\\\\n\\implies \\theta_2^{v+1} &= \\frac{\\sum_{i=1}^n (1-w_i^v)y_i}{\\sum_{i=1}^n (1-w_i^v)n}\n\\end{aligned}\n\\]\nand, \\[\\begin{aligned}\n0=\\frac{\\partial Q}{\\partial p} &=\\frac{w_i^v}{p}-\\frac{1-w_i^v}{1-p} \\\\\n\\implies 0&= w_i^v-pw_i^v-p+pw_i^v \\\\\n\\implies p^{v+1} &= w_i^v\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch2.html#section-19",
    "href": "chapters/ch2.html#section-19",
    "title": "Chapter 2: Likelihood Construction and Estimation",
    "section": "2.49",
    "text": "2.49\nRecall that the ZIP model is just a mixture of densities \\(f(y;\\lambda, p) =pf_1(y)+(1-p)f_2(y;\\lambda)\\) where \\[\\begin{aligned}f_1(y)=I(y=0)~&~~~~~~~~~f_2(y;\\lambda)= \\frac{\\lambda^ye^{-\\lambda}}{y!}~&~~y=0,1,2,\\dots\\end{aligned}\\] Lambert (1992) used it to model product defects as a function of covariates. In the “perfect” state, no defects occur (\\(P(Y_i= 0)= 0\\)), whereas in the “imperfect” state, the number of defects \\(Y_i\\) follows a Poisson\\((\\lambda)\\) distribution. The author used the EM Algorithm as follows (except we won’t do the more complicated modeling with covariates.) Let \\(Z_i = 1\\) if the product is in the perfect state and \\(Z_i = 0\\) for the imperfect state. Recall that the contribution to the complete data likelihood for a pair \\((Y_i,Z_i)\\) is \\([pf_1(Y_i)]^{Z_i}[(1-p)f_2(Y_i;\\lambda)]^{1-Z_i}\\) (and here note that the first part reduces to \\(p^{Z_i}\\) because \\(f_1\\) is a point mass at 0).\n\na. E step.\nWrite down the complete data log likelihood and find \\(Q(\\lambda,p,\\lambda^v,p^v)\\) in terms of \\(w_i^v = E(Z_i|Y_i,\\lambda^v,p^v)\\). (You do not need to give an expression for \\(w_i^v\\).)\nSolution:\n\\[\n\\begin{aligned}\n\\mathcal L_C (\\lambda, p, \\lambda^v, p^v; Y_i,Z_i) &= \\prod_{i=1}^n[pf_1(Y_i)]^{Z_i}[(1-p)f_2(Y_i;\\lambda)]^{1-Z_i} \\\\\n\\implies  \\ell_C(\\lambda, p, \\lambda^v, p^v; Y_i,Z_i) &= \\sum_{i=1}^n z_i \\log(f_1(y_i))+(1-z_i)\\log(f_2(y)) +z_i \\log p +(1-z_i)\\log(1-p) \\\\\n\\implies  Q(\\lambda, p, \\lambda^v, p^v; Y_i) &=\\sum_{i=1}^nE_{(\\lambda^v,p^v)}\n\\left[z_i \\log(f_1(y_i))+(1-z_i)\\log(f_2(y,\\lambda)) +z_i \\log p +(1-z_i)\\log(1-p)\\right] \\\\\n&=\\sum_{i=1}^nw_i^v \\log(f_1(y_i))+(1-w^v_i)\\log(f_2(y,\\lambda)) +w^v_i \\log p +(1-w^v_i)\\log(1-p) \\\\\n&=\\sum_{i=1}^nw_i^v I(y_i=0)+(1-w^v_i)\\log\\left(\\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}\\right)I(y_i \\in \\mathbb N^+) \\\\&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+w^v_i \\log p +(1-w^v_i)\\log(1-p) \\\\\n\\end{aligned}\n\\]\n\n\nb. M step.\nFind expressions for \\(\\lambda^{v+1}\\) and \\(p^{v+1}\\) by maximizing \\(Q\\) from the E step.\nSolution:\n\\[\n\\begin{aligned}\n0 =\\frac{\\partial Q}{\\partial p} &= \\sum_{i=1}^n\\frac{w_i^v}{p}-\\frac{1-w_i^v}{1-p}\\\\\n\\implies 0 &= \\sum_{i=1}^n w_i^v-pw_i^v-p+pw_i^v \\\\\n\\implies p^{v+1} &= \\frac{\\sum_{i=1}^n w_i^v}{n}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n0 =\\frac{\\partial Q}{\\partial \\lambda} &= (1-w_i^v)\\left[\\frac{y_i}{\\lambda}-1\\right]I(y_i \\in \\mathbb N^+)\\\\\n\\implies 0 &= (1-w_i^v)(y_i-\\lambda)I(y_i \\in \\mathbb N^+) \\\\\n\\implies \\lambda^{v+1} &=\\frac{(1-w_i^v)y_i}{1-w_i^v}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch3.html#section",
    "href": "chapters/ch3.html#section",
    "title": "Chapter 3: Likelihood-Based Tests and Confidence Regions",
    "section": "3.6",
    "text": "3.6\nAssume that \\(Y_1\\) and \\(Y_2\\) are independent with respective geometric probability mass functions, \\[f(y;p_i) = p_i(1-p_i)^{y-1}~~~~y=0,1,\\dots~~~~ 0 \\leq p_i \\leq 1, ~~i=1,2.\\] Recall that the mean and variance of a geometric random variable with parameter \\(p\\) are \\(1/p\\) and \\((1-p)/p^2\\), respectively. For \\(H_0: p_1=p_2\\) versus \\(H_a: p_1 \\neq p_2\\) show that the score statistic is \\[T_S=\\frac{\\overset \\sim p ^2}{2(1- \\overset \\sim p)}(Y_1-Y_2)^2, ~~~where~\\overset \\sim p = \\frac 2 {Y_1+Y_2}\\]\nSolution:\n\\[\\begin{aligned}\n\\boldsymbol \\theta &= \\begin{pmatrix} p_1 \\\\p_2 \\end{pmatrix}, ~~~ h(\\boldsymbol \\theta) =p_1-p_2 = 0 \\\\\n\\mathcal L(\\boldsymbol \\theta) &= p_1(1-p_1)^{y_1-1}p_2(1-p_2)^{y_2-1},~~~~ H(\\boldsymbol \\theta) = \\begin{bmatrix} 1 & -1 \\end{bmatrix}\n\\\\\n\\mathcal \\ell(\\boldsymbol \\theta) &= \\log p_1 + (y_1-1)\\log(1-p_1) +\\log p_2+ (y_2-1)\\log(1-p_2)\n\\end{aligned}\n\\] \\[\\begin{aligned}\nS(\\boldsymbol \\theta) &= \\begin{pmatrix} \\frac 1 {p_1}- \\frac{y_1-1}{1-p_1} \\\\ \\frac 1 {p_2}- \\frac{y_2-1}{1-p_2} \\end{pmatrix} \\\\\nI_T(\\boldsymbol \\theta) &= -E\\begin{pmatrix} \\frac {-1} {p_1^2}- \\frac{y_1-1}{(1-p_1)^2} & 0 \\\\ 0& \\frac {-1} {p_2^2}- \\frac{y_2-1}{(1-p_2)^2} \\end{pmatrix} =-\\begin{pmatrix} \\frac {-1} {p_1^2}- \\frac{\\frac{1}{p_1}-1}{(1-p_1)^2} & 0 \\\\ 0& \\frac {-1} {p_2^2}- \\frac{\\frac{1}{p_2}-1}{(1-p_2)^2} \\end{pmatrix} \\\\\n&=-\\begin{pmatrix} \\frac {-(1-p_1)^2-p_1+p_1^2}{p_1^2(1-p_1)^2}& 0 \\\\ 0& \\frac {-(1-p_2)^2-p_2+p_2^2}{p_2^2(1-p_2)^2} \\end{pmatrix}=-\\begin{pmatrix} \\frac {-1+2p_1-p_1^2-p_1+p_1^2}{p_1^2(1-p_1)^2}& 0 \\\\ 0& \\frac {-1+2p_2 - p_2^2-p_2+p_2^2}{p_2^2(1-p_2)^2} \\end{pmatrix} \\\\\n&=-\\begin{pmatrix} \\frac {p_1-1}{p_1^2(1-p_1)^2}& 0 \\\\ 0& \\frac {p_2-1}{p_2^2(1-p_2)^2} \\end{pmatrix}\n=\\begin{pmatrix} \\frac {1}{p_1^2(1-p_1)}& 0 \\\\ 0& \\frac {1}{p_2^2(1-p_2)} \\end{pmatrix} \\\\\nI_T(\\boldsymbol \\theta)^{-1} &=  \\begin{pmatrix} p_1^2(1-p_1)& 0 \\\\ 0& p_2^2(1-p_2) \\end{pmatrix}\n\\end{aligned}\\]\nUsing the Lagrange multiplier method, \\[\\begin{aligned}\nmaximize \\{\\ell(p)  -\\lambda h(\\theta)\\}\n& = \\log p_1 + (y_1-1)\\log(1-p_1) +\\log p_2+ (y_2-1)\\log(1-p_2) + \\lambda p_1 - \\lambda p_2\\\\\np_1 &= p_2  = p\\\\\n0 &= \\frac{1} {p} -\\frac{y_1-1}{1-p} + \\lambda \\implies \\lambda = -\\frac{1} {p} +\\frac{y_1-1}{1-p}\\\\\n0 &= \\frac{1} {p} -\\frac{y_2-1}{1-p} -\\lambda \\implies \\lambda = \\frac{1} {p} -\\frac{y_2-1}{1-p} \\\\\n\\implies 0&= \\frac{2} {p} -\\frac{y_2-1+y_1-1}{1-p}\\\\\n&= 2-2p +2p -p(y_1+y_2) \\\\\n\\implies \\overset \\sim p &= \\frac 2 {y_1+y_2},~~~ \\hat \\lambda_{RMLE} = \\frac{1} {\\overset \\sim p} -\\frac{y_2-1}{1-\\overset \\sim p}\\\\\n& \\hat \\lambda_{RMLE} =  \\frac{1- \\overset \\sim p - \\overset \\sim p y_2+ \\overset \\sim p}{\\overset \\sim p (1-\\overset \\sim p)}=  \\frac{1 - \\frac {2y_2} {y_1+y_2}}{\\overset \\sim p (1-\\overset \\sim p)} \\\\\n&\\hat \\lambda_{RMLE}=  \\frac{\\frac {y_1-y_2} {y_1+y_2}}{\\overset \\sim p (1-\\overset \\sim p)}=\\frac {(y_1-y_2) \\overset \\sim p /2}{\\overset \\sim p (1-\\overset \\sim p)}\\\\\n&\\hat \\lambda_{RMLE}=  \\frac{y_1-y_2}{2 (1-\\overset \\sim p)}\n\\end{aligned}\\]\n\\[\\begin{aligned}\nT_S &= \\hat \\lambda_{RMLE}^T H(\\hat \\theta_{RMLE})I_T^{-1}(\\hat \\theta_{RMLE})  H(\\hat \\theta_{RMLE})^T\\hat \\lambda_{RMLE}^T \\\\\n&= \\left(\\frac{y_1-y_2}{2 (1-\\overset \\sim p)}\\right)^2  \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} \\overset \\sim p^2(1-\\overset \\sim p)& 0 \\\\ 0&\\overset \\sim p^2(1-\\overset \\sim p) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\\\\n&= \\left(\\frac{y_1-y_2}{2 (1-\\overset \\sim p)}\\right)^2  2 (\\overset \\sim p^2(1-\\overset \\sim p)) \\\\\n&= \\frac{\\overset \\sim p^2}{2 (1-\\overset \\sim p)}(y_1-y_2)^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch3.html#section-1",
    "href": "chapters/ch3.html#section-1",
    "title": "Chapter 3: Likelihood-Based Tests and Confidence Regions",
    "section": "3.8",
    "text": "3.8\nSuppose that \\(Y_1,\\dots, Y_{n_1}\\) are iid from a \\(N(\\mu_1,\\sigma^2)\\) distribution, \\(X_1,\\dots,X_{n_2}\\) are iid from a \\(N(\\mu_2,\\sigma^2)\\) distribution, the samples are independent of each other, and we desire to test \\(H_0:\\mu_1=\\mu_2\\).\n\na.\nDerive the Wald and score tests and express them as a function of the square of the usual two-sample pooled t . Also, show that the likelihood ratio statistic is \\(T_{LR} = (n_1+n_2)\\log[1+t^2/(n_1+n_2-2)]\\).\nSolution:\n\\[\\begin{aligned}\n\\ell (\\mu_1,\\mu_2,\\sigma) &= c-n_1\\log \\sigma - \\frac 1 {2\\sigma^2} \\sum_{i=1}^{n_1}(Y_i-\\mu_1)^2 +c-n_2\\log \\sigma - \\frac 1 {2\\sigma^2} \\sum_{i=1}^{n_2}(X_i-\\mu_2)^2\\\\\n&= 2c-(n_1+n_2)\\log \\sigma - \\frac 1 {2\\sigma^2} \\sum_{i=1}^{n_1}(Y_i-\\mu_1)^2 - \\frac 1 {2\\sigma^2} \\sum_{i=1}^{n_2}(X_i-\\mu_2)^2 \\\\\nS\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\\\sigma \\end{pmatrix}\n&=\\begin{pmatrix} \\frac 1 {\\sigma^2} \\sum_{i=1}^{n_1} (Y_i-\\mu_1)\\\\\n\\frac 1 {\\sigma^2} \\sum_{i=1}^{n_2} (X_i-\\mu_2)\\\\\n\\frac{-n_1-n_2}{\\sigma} +\\frac 1 {\\sigma^3} \\sum_{i=1}^{n_1} (Y_i-\\mu_1)^2 +\\frac 1 {\\sigma^3} \\sum_{i=1}^{n_2} (X_i-\\mu_2)^2 \\end{pmatrix}\\\\\n&=\\begin{pmatrix} \\frac 1 {\\sigma^2} \\sum_{i=1}^{n_1} (Y_i-\\mu_1)\\\\\n\\frac 1 {\\sigma^2} \\sum_{i=1}^{n_2} (X_i-\\mu_2)\\\\\n\\frac 1 {\\sigma^3} \\sum_{i=1}^{n_1} [(Y_i-\\mu_1)^2-\\sigma^2] +\\frac 1 {\\sigma^3} \\sum_{i=1}^{n_2} [(X_i-\\mu_2)^2 - \\sigma^2] \\end{pmatrix}\\\\\nI_T \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\\\sigma \\end{pmatrix}\n&= \\begin{pmatrix} \\frac {n_1} {\\sigma^2} &0&0\\\\\n0&\\frac {n_2} {\\sigma^2} & 0\\\\\n0&0& \\frac{n_1+n_2}{\\sigma^2}\\end{pmatrix},\nI_T^{-1} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\\\sigma \\end{pmatrix}\n= \\sigma^2 \\begin{pmatrix} \\frac {1} {n_1} &0&0\\\\\n0&\\frac {1} {n_2} & 0\\\\\n0&0& \\frac 1 {n_1+n_2}\\end{pmatrix} \\\\\n\\mathbf h( \\boldsymbol {\\theta}) &= \\mu_1-\\mu_2 ,\n\\mathbf H(\\boldsymbol \\theta) = \\begin{pmatrix} 1 & -1 &0\\end{pmatrix}, \\mathbf H(\\boldsymbol \\theta) \\mathbf {I_T^{-1}}(\\boldsymbol {\\theta}) \\mathbf H(\\boldsymbol \\theta)^T= \\sigma^2 \\left(\\frac 1 {n_1} +\\frac 1 {n_2}\\right)\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\begin{pmatrix} \\frac 1 {\\sigma^2} \\sum_{i=1}^{n_1} (Y_i-\\mu_1)\\\\\n\\frac 1 {\\sigma^2} \\sum_{i=1}^{n_2} (X_i-\\mu_2)\\\\\n\\frac 1 {\\sigma^3} \\sum_{i=1}^{n_1} [(Y_i-\\mu_1)^2-\\sigma^2] +\\frac 1 {\\sigma^3} \\sum_{i=1}^{n_2} [(X_i-\\mu_2)^2 - \\sigma^2] \\end{pmatrix} &= \\mathbf 0 \\\\\n\\implies \\hat \\mu_1 = \\frac 1 {n_1} \\sum_{i=1}^{n_1} Y_i = \\bar Y ~~,~~\\hat \\mu_2 = \\frac 1 {n_2} \\sum_{i=1}^{n_2} X_i = \\bar X \\\\\n\\sum_{i=1}^{n_1} [(Y_i-\\hat \\mu_1)^2-\\sigma^2] + \\sum_{i=1}^{n_2} [(X_i-\\hat \\mu_2)^2 - \\hat \\sigma^2] = 0 \\\\\n\\sum_{i=1}^{n_1} (Y_i-\\bar Y)^2 + \\sum_{i=1}^{n_2} (X_i-\\bar X)^2  =  (n_1+ n_2) \\hat \\sigma^2 \\\\\n\\implies \\hat \\sigma_{MLE} ^2 = \\frac{\\sum_{i=1}^{n_1} (Y_i-\\bar Y)^2 + \\sum_{i=1}^{n_2} (X_i-\\bar X)^2 }{n_1+n_2}\n\\end{aligned}\n\\]\nNote that \\[\\begin{aligned}\nS_p^2 &= \\frac{\\sum_{i=1}^{n_1} (Y_i-\\bar Y)^2 + \\sum_{i=1}^{n_2} (X_i-\\bar X)^2 }{n_1+n_2-2} \\\\\n\\implies \\frac{(n_1+ n_2 -2)}{n_1+n_2}S_p^2 &= \\hat \\sigma^2_{MLE}\n\\end{aligned}\\] \\[\n\\begin{aligned}\n\\implies  \\sigma_{RMLE} ^2\n&= \\frac{\\sum_{i=1}^{n_1} (Y_i- \\mu)^2 + \\sum_{i=1}^{n_2} (X_i- \\mu)^2 }{n_1+n_2}\\\\\n&= \\frac{\\sum_{i=1}^{n_1} (Y_i-\\bar Y + \\bar Y- \\mu)^2 + \\sum_{i=1}^{n_2} (X_i- \\bar X + \\bar X- \\mu)^2 }{n_1+n_2}\\\\\n&= \\frac{\\sum_{i=1}^{n_1} (Y_i-\\bar Y)^2 + 2(Y_i-\\bar Y)(\\bar Y- \\mu) +(\\bar Y- \\mu)^2 + \\sum_{i=1}^{n_2} (X_i-\\bar X)^2 + 2(X_i-\\bar X)(\\bar X- \\mu) +(\\bar X- \\mu)^2  }{n_1+n_2}\\\\\n&= \\frac{\\sum_{i=1}^{n_1} (Y_i-\\bar Y)^2  +(\\bar Y- \\mu)^2 + \\sum_{i=1}^{n_2} (X_i-\\bar X)^2 +(\\bar X- \\mu)^2  }{n_1+n_2}\\\\\n&= \\frac{\\sum_{i=1}^{n_1} (Y_i-\\bar Y)^2 + \\sum_{i=1}^{n_2} (X_i-\\bar X)^2 +n_1(\\bar Y- \\mu)^2+n_2(\\bar X- \\mu)^2  }{n_1+n_2}\\\\\n&= \\sigma^2_{MLE}+\\frac{n_1(\\bar Y- \\mu)^2+n_2(\\bar X- \\mu)^2  }{n_1+n_2}\n\\end{aligned}\\]\nThus, the Wald Statistic is:\n\\[\\begin{aligned}\nT_W &= \\mathbf h( \\boldsymbol {\\hat \\theta})^T [\\mathbf H(\\boldsymbol {\\hat \\theta}) \\mathbf {I_T^{-1}}(\\boldsymbol {\\hat \\theta}) \\mathbf H(\\boldsymbol {\\hat \\theta})^T]^{-1}\\mathbf h( \\boldsymbol {\\hat \\theta})\\\\\n&= \\frac{(\\bar Y- \\bar X)^2}{\\hat \\sigma^2_{MLE} \\left(\\frac 1 {n_1} +\\frac 1 {n_2}\\right)}\n\\end{aligned}\\]\nNow, for the score function, I need to maximize \\(\\ell (\\mu_1,\\mu_2, \\sigma) -\\lambda(\\mu_1-\\mu_2)\\).\n\\[\\begin{aligned}\n\\mu_1- \\mu_2 = 0 &\\implies \\mu_1 = \\mu_2 = \\mu\\\\\n\\frac 1 {\\sigma^2} \\sum_{i=1}^{n_1} (Y_i-\\mu) -\\lambda =0& \\implies \\lambda= \\frac{1}{\\sigma^2} \\sum_{i=1}^{n_1} (Y_i-\\mu) \\\\\\\\\n\\frac 1 {\\sigma^2} \\sum_{i=1}^{n_2} (X_i-\\mu) + \\frac{1}{\\sigma^2} \\sum_{i=1}^{n_1} (Y_i-\\mu)= 0\n&\\implies \\hat \\mu_{RMLE} = \\frac{\\sum_{i=1}^{n_2} X_i + \\sum_{i=1}^{n_1} Y_i}{n_1+n_2} = \\frac{n_2 \\bar X + n_1 \\bar Y}{n_1+n_2}\n\\end{aligned}\\]\nSimplifying \\(\\hat \\lambda_{RMLE}\\), \\[\\begin{aligned}\n\\hat \\lambda&= \\frac{1}{\\sigma^2} \\sum_{i=1}^{n_1} (Y_i-\\mu) \\\\\n&= \\frac{1}{\\sigma^2} \\sum_{i=1}^{n_1} (Y_i)-n_1 \\left(\\frac{\\sum_{i=1}^{n_1} X_i + \\sum_{i=1}^{n_2} Y_i}{n_1+n_2}\\right) \\\\\n&= \\frac{1}{\\sigma^2} \\left(\\frac{(n_1+n_2)\\sum_{i=1}^{n_1} (Y_i)-n_1 \\left(\\sum_{i=1}^{n_1} X_i + \\sum_{i=1}^{n_2} Y_i\\right)}{n_1+n_2}\\right) \\\\\n&= \\frac{1}{\\sigma^2} \\left(\\frac{n_2\\sum_{i=1}^{n_1} Y_i-n_1 \\sum_{i=1}^{n_1} X_i}{n_1+n_2}\\right) \\\\\n&= \\frac{1}{\\sigma^2} \\left(\\frac{n_1n_2}{n_1+n_2} \\right)(\\bar Y - \\bar X)\n\\end{aligned}\\]\nThe score statistic is: \\[\\begin{aligned}\nT_S &= \\hat \\lambda_{RMLE}^T H(\\hat \\theta_{RMLE})I_T^{-1}(\\hat \\theta_{RMLE}) \\hat \\lambda_{RMLE}^T \\\\\n&= \\left[\\frac{1}{\\sigma^2} \\left(\\frac{n_1n_2}{n_1+n_2} \\right)(\\bar Y - \\bar X)\\right]^2\\sigma^2 \\left(\\frac 1 {n_1} +\\frac 1 {n_2}\\right) \\\\\n&= \\frac{1}{\\sigma^2} \\left[\\left(\\frac{n_1n_2}{n_1+n_2} \\right)(\\bar Y - \\bar X)\\right]^2 \\left(\\frac {n_1+n_2} {n_1n_2}\\right) \\\\\n&= \\frac{(\\bar Y - \\bar X)^2 }{\\sigma^2\\left(\\frac 1 {n_1} +\\frac 1{n_2}\\right)}\n\\end{aligned}\\]\nAnd the LRT is:\n\\[\\begin{aligned}T_{LR} &= -2 [\\ell (\\hat \\theta_{RMLE}) - \\ell(\\hat \\theta_{RMLE})] \\\\\n&=-2 [2c- \\frac 1 2 (n_1+n_2)\\log \\sigma_{RMLE}^2 - \\frac 1 {2\\sigma_{RMLE}^2} \\sum_{i=1}^{n_1}(Y_i-\\mu)^2 - \\frac 1 {2\\sigma_{RMLE}^2} \\sum_{i=1}^{n_2}(X_i-\\mu)^2] \\\\\n&~~~~~~+2 [2c-\\frac 1 2 (n_1+n_2)\\log \\sigma_{MLE}^2 - \\frac 1 {2\\sigma_{MLE}^2} \\sum_{i=1}^{n_1}(Y_i-\\mu_1)^2 - \\frac 1 {2\\sigma_{MLE}^2} \\sum_{i=1}^{n_2}(X_i-\\mu_2)^2] \\\\\n&=(n_1+n_2) \\log \\left(\\frac{\\sigma_{RMLE}^2}{\\sigma_{MLE}^2} \\right)\\\\\n&= (n_1+n_2) \\log \\left(\\frac{\\sigma^2_{MLE}+\\frac{n_1(\\bar Y- \\mu)^2+n_2(\\bar X- \\mu)^2  }{n_1+n_2}}{\\sigma_{MLE}^2} \\right)\\\\\n&= (n_1+n_2) \\log \\left(1+\\frac{\\frac 1{n_1+n_2}\\left(n_1(\\bar Y- \\frac{n_2 \\bar X + n_1 \\bar Y}{n_1+n_2})^2+n_2(\\bar X- \\frac{n_2 \\bar X + n_1 \\bar Y}{n_1+n_2})^2  \\right)}\n{\\sigma^2_{MLE}} \\right)\\\\\n&= (n_1+n_2) \\log \\left(1+\\frac{\\frac 1{n_1+n_2}\\left(n_1(\\frac{n_2 \\bar Y- n_2 \\bar X}{n_1+n_2})^2+n_2(\\frac{n_1 \\bar X - n_1 \\bar Y}{n_1+n_2})^2  \\right)}{\\sigma_{MLE}^2 } \\right)\\\\\n&= (n_1+n_2) \\log \\left(1+\\frac{\\frac 1{(n_1+n_2)^3}\\left(n_1n_2^2( \\bar Y- \\bar X)^2+n_1^2n_2(\\bar X - \\bar Y)^2  \\right)}{\\sigma_{MLE}^2 } \\right)\\\\\n&= (n_1+n_2) \\log \\left(1+\\frac{\\frac{n_1n_2}{(n_1 + n_2)^2} \\left(\\bar Y - \\bar X\\right)^2}\n{\\sigma_{MLE}^2 } \\right)\\\\\n&= (n_1+n_2) \\log \\left(1+\\frac{ \\left(\\bar Y - \\bar X\\right)^2}\n{(n_1+n_2)\\sigma_{MLE}^2 \\left(\\frac 1{n_1} + \\frac 1 {n_2}\\right)} \\right)\\\\\n&= (n_1+n_2) \\log \\left(1+\\frac{ \\left(\\bar Y - \\bar X\\right)^2}\n{(n_1+n_2-2)S_p^2 \\left(\\frac 1{n_1} + \\frac 1 {n_2}\\right)} \\right)\\\\\n&= (n_1+n_2) \\log \\left(1+\\frac{t^2}{(n_1+n_2-2)} \\right)\n\\end{aligned}\n\\]\n\n\n\nLet \\(n_1= n_2=5\\). These tests reject \\(H_0\\) at approximate level .05 if they are larger than 3.84. Find exact expressions for \\(P(T_W \\geq 3.84), P(T_S \\geq 3.84),\\) and \\(P(T_{LR} \\geq 3.84)\\) using the fact that \\(t^2\\) has an \\(F(1,n_1+n_2-2)\\) under \\(H_0\\).\nSolution:\n\\[\\begin{aligned}\nP(T_W \\geq 3.84)= P(T_S \\geq 3.84) &= P\\left(\\frac{(\\bar Y- \\bar X)^2}{\\hat \\sigma^2_{MLE} \\left(\\frac 1 {5} +\\frac 1 {5}\\right)} \\geq 3.84\\right) \\\\\n&= 1-\\chi^2_{(1)}(3.84) \\\\\nP(T_{LR} \\geq 3.84) &= P\\left((5+5) \\log \\left(1+\\frac{t^2}{(5+5-2)} \\right) \\geq 3.84\\right) \\\\\n&= P\\left(1+t^2/8  \\geq \\exp(0.384)\\right)\\\\\n&= P\\left(t^2  \\geq 8(\\exp(0.384)-1)\\right)\\\\\n&= 1-F_{8\\exp(0.384)-8}(1,8)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch3.html#section-2",
    "href": "chapters/ch3.html#section-2",
    "title": "Chapter 3: Likelihood-Based Tests and Confidence Regions",
    "section": "3.9",
    "text": "3.9\nSuppose that \\(Y_1,\\dots, Y_n\\) are independently distributed as Poisson random variables with means \\(\\lambda_1, \\dots, \\lambda_n\\), respectively. Thus, \\(P(Y_i=y) = f(y;\\lambda_i) =\\frac{\\lambda_i^y e^{-\\lambda_i}}{y!}I(y \\in \\mathbb N)\\). Show that the score statistic for testing \\(H_0: \\lambda_1 = \\lambda_2 = \\dots = \\lambda_n=\\lambda\\) (i.e., that the \\(Y_i\\) ’s all have the same distribution) is given by \\(T_S = \\sum_{i=1}^n \\frac{(Y_i-\\bar Y)^2}{\\bar Y}\\).\nSolution:\n\\[\\begin{aligned}\n\\mathcal L(\\boldsymbol \\lambda) &=\\prod_{i=1}^n \\frac{\\lambda_i^{y_i} e^{-\\lambda_i}}{y_i!} \\\\\n\\ell (\\boldsymbol \\lambda) &=\\sum_{i=1}^n y_i \\log \\lambda_i -\\lambda_i -\\log(y_i!) \\\\\nS_i (\\boldsymbol \\lambda) &= \\frac {y_i} {\\lambda_i}-1 \\\\\nI_T (\\boldsymbol \\lambda) &= diag\\left(-E\\left(\\frac{-y_i}{\\lambda_i^2}\\right)\\right) = diag(1/\\lambda_i) \\\\\nI_T^{-1} (\\boldsymbol \\lambda)&= diag(\\lambda_i)\n\\end{aligned}\\]\n\\[\\begin{aligned}\n0 &=S_i (\\boldsymbol \\lambda) = \\frac {y_i} {\\lambda_i}-1 \\\\\n\\implies \\hat \\lambda_{i_{MLE}} &=y_i~~~,~~~~ \\hat \\lambda_{RMLE} =\\bar y\n\\end{aligned}\\]\n\\[\\begin{aligned}\nT_S &= \\mathbf {S(\\boldsymbol{\\hat \\lambda}_{RMLE})^T I_T^{-1} (\\boldsymbol {\\hat \\lambda_{RMLE}})S(\\boldsymbol{\\hat \\lambda}_{RMLE})}\\\\\n&= \\left\\{\\frac {Y_i} {\\bar Y}-1 \\right\\}diag(\\bar Y)\\left\\{\\frac {Y_i} {\\bar Y}-1 \\right\\}\\\\\n&= \\sum_{i=1}^n \\bar Y\\left(\\frac {Y_i} {\\bar Y}-1 \\right)^2=\\sum_{i=1}^n \\bar Y\\left(\\frac {Y_i - \\bar Y} {\\bar Y}\\right)^2\\\\\n&= \\sum_{i=1}^n \\frac {(Y_i-\\bar Y)^2} {\\bar Y}\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch3.html#section-3",
    "href": "chapters/ch3.html#section-3",
    "title": "Chapter 3: Likelihood-Based Tests and Confidence Regions",
    "section": "3.12",
    "text": "3.12\nShow that the \\(T_W\\), \\(T_S\\) and \\(T_{LR}\\) statistics defined in Example 3.2 (p. 129) are asymptotically equivalent under \\(H_0\\) by showing their differences converge to 0 in probability.\nSolution:\n\\[\\begin{aligned}\n\\ell (\\hat p) &= \\ell(p_0) + \\ell'(p_0)(\\hat p - p_0) +\\frac 1 2 \\ell^{2}(p_0)(\\hat p - p_0)^2+ \\frac 1 6 \\ell^3 (p^*)(\\hat p - p_o)^3 \\\\\n\\implies T_{LR} &=-2\\left[\\ell (\\hat p_0) - \\ell (\\hat p) \\right] \\\\\n&=-2\\left[\\ell (\\hat p_0) - \\ell(p_0) - \\ell'(p_0)(\\hat p - p_0) -\\frac 1 2 \\ell^{2}(p_0)(\\hat p - p_0)^2- \\frac 1 6 \\ell^3 (p^*)(\\hat p - p_o)^3 \\right] \\\\\n&=2\\left[S(p_0)(\\hat p - p_0) -\\frac 1 2 I_T(p_0)(\\hat p - p_0)^2+ \\frac 1 6 \\ell^3 (p^*)(\\hat p - p_o)^3 \\right] \\\\\n&=2\\left[\\frac{n \\hat p - n p_0}{p_0 (1-p_0)}(\\hat p -p_0)-\\frac 1 2 \\left(\\frac{n \\hat p}{p_0^2}+ \\frac{n-n\\hat p}{(1-p_0)^2}\\right)(\\hat p - p_0)^2+ \\frac 1 6 \\ell^3 (p^*)(\\hat p - p_o)^3 \\right] \\\\\n&=n (\\hat p -p_0)^2\\left[\\frac{2}{p_0 (1-p_0)}-\\left(\\frac{\\hat p}{p_0^2}+ \\frac{1-\\hat p}{(1-p_0)^2}\\right)+ \\frac 1 {3n} \\ell^3 (p^*)(\\hat p - p_o) \\right] \\\\\n\\implies T_W- T_{LR} &= \\frac{n(\\hat p- p_0)^2}{\\hat p (1-\\hat p)} -n (\\hat p -p_0)^2\\left[\\frac{2}{p_0 (1-p_0)}-\\left(\\frac{\\hat p}{p_0^2}+ \\frac{1-\\hat p}{(1-p_0)^2}\\right)+ \\frac 1 {3n} \\ell^3 (p^*)(\\hat p - p_o) \\right] \\\\\n&= n(\\hat p- p_0)^2\\left[\\frac 1 {\\hat p (1-\\hat p)}-\\frac{2}{p_0 (1-p_0)}+\\frac{\\hat p}{p_0^2}+ \\frac{1-\\hat p}{(1-p_0)^2}+ \\frac 1 {3n} \\ell^3 (p^*)(\\hat p - p_o) \\right]\n\\end{aligned}\\]\nNote that \\(n(\\hat p- p_0)^2 \\overset d \\to \\chi^2_{something}\\) and,\n\\[\\begin{aligned}\n&\\lim_{n \\to \\infty}\\left[\\frac 1 {\\hat p (1-\\hat p)}-\\frac{2}{p_0 (1-p_0)}+\\frac{\\hat p}{p_0^2}+ \\frac{1-\\hat p}{(1-p_0)^2}+ \\frac 1 {3n} \\ell^3 (p^*)(\\hat p - p_0) \\right] \\\\\n&=\\frac 1 {p_0 (1-p_0)}-\\frac{2}{p_0 (1-p_0)}+\\frac{p_0}{p_0^2}+ \\frac{1-p_0}{(1-p_0)^2}+0\\\\\n&=-\\frac{1}{p_0 (1-p_0)}+\\frac{1}{p_0}+ \\frac{1}{1-p_0}\\\\\n&= -\\frac{1}{p_0 (1-p_0)}+\\frac{1-p_0+p_0}{p_0(1-p_0)}\\\\\n&= 0\n\\end{aligned}\n\\]\nTherefore, by Slutsky’s theorem \\(T_w - T_{LR} \\overset P \\to 0\\).\n\\[\\begin{aligned}\nT_W - T_S &= \\frac{n(\\hat p- p_0)^2}{\\hat p (1-\\hat p)} - \\frac{n(\\hat p- p_0)^2}{p_0 (1-p_0)}\\\\\n&= n \\left(\\frac{p_0 (1-p_0)(\\hat p- p_0)^2-\\hat p (1-\\hat p)(\\hat p- p_0)^2}{\\hat p p_0(1-\\hat p) (1-p_0)} \\right)\\\\\n&= n \\left(\\frac{[(p_0-p_0^2)-(\\hat p-\\hat p^2)](\\hat p- p_0)^2}{\\hat p p_0(1-\\hat p) (1-p_0)} \\right)\\\\\n&= \\left[\\sqrt n (\\hat p- p_0) \\right]^2\\left(\\frac{p_0 -\\hat p + \\hat p^2-p_0^2}{\\hat p p_0(1-\\hat p) (1-p_0)} \\right)\\\\\n\\text{Note that } &p_0 - \\hat p \\overset p \\to 0 \\text{ by WLLN, so}\n\\left(\\frac{p_0 -\\hat p + \\hat p^2-p_0^2}{\\hat p p_0(1-\\hat p) (1-p_0)} \\right) \\overset p \\to 0\\\\\n\\text{and, }&\\left[\\sqrt n (\\hat p- p_0) \\right]^2 \\overset d \\to \\chi^2_{something} \\text{ by CLT} \\\\\n\\implies T_W - T_S &\\overset P \\to 0 \\text{ by Slutsky's Theorem}\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch3.html#section-4",
    "href": "chapters/ch3.html#section-4",
    "title": "Chapter 3: Likelihood-Based Tests and Confidence Regions",
    "section": "3.16",
    "text": "3.16\nDerive the score statistic \\(T_S\\) in (3.21, p. 148)\n\\[\\begin{aligned}\nlogit(p_{1+2(j-1)}) = \\beta_j + \\beta_{k+1} ~~&,~~logit(p_{2+2(j-1)}) = \\beta_j\\\\\np_{1+2(j-1)} =  \\frac{\\exp(\\beta_j + \\beta_{k+1})}{1+\\exp(\\beta_j + \\beta_{k+1})}~~\n&,~~p_{2+2(j-1)} =  \\frac{\\exp(\\beta_j)}{1+\\exp(\\beta_j)} \\\\\nH_0: \\beta_{k+1} = 0\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\mathcal L(\\boldsymbol{\\beta}_j, \\beta_{k+1}) &= \\prod_{j=1}^k \\left(p_{1+2(j-1)}\\right)^{a_j}\n\\left(1-p_{1+2(j-1)}\\right)^{c_j}\n\\left(p_{2+2(j-1)}\\right)^{b_j}\n\\left(1-p_{2+2(j-1)}\\right)^{d_j} \\\\\n&= \\prod_{j=1}^k \\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)^{a_j}\n\\left(1-\\frac{\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)^{c_j}\\\\\n&~~~~~~~~~~~~~~~ \\times\n\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)^{b_j}\n\\left(1-\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)^{d_j} \\\\\n&= \\prod_{j=1}^k \\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)^{a_j}\n\\left(\\frac{1}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)^{c_j}\\\\\n&~~~~~~~~~~~~~~~ \\times\n\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)^{b_j}\n\\left(\\frac{1}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)^{d_j} \\\\\n\\ell(\\boldsymbol{\\beta}_j, \\beta_{k+1})&= \\sum_{j=1}^k\na_j \\log \\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)\n-c_j\\log\\left({1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)\\\\\n&~~~~~~~~~~~~~~~ +\nb_j\\log\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)\n-d_j\\log\\left({1+\\exp(\\boldsymbol{\\beta}_j)}\\right) \\\\\nS(\\boldsymbol{\\beta}_j, \\beta_{k+1})&= \\begin{pmatrix}\n\\left[\na_j \\left(1- \\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}\\right)\n-c_j\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)+\nb_j\\left(1-\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)-\nd_j\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)\\right]_{\\substack{j^{th}~ entry~ of \\\\ k x 1 ~vector}} \\\\\n\\sum_{j=1}^k\na_j \\left(1- \\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}\\right)\n-c_j\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\left[\na_j +b_j  -(a_j + c_j)\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}\\right)\n-(b_j+d_j)\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)\\right]_{\\substack{j^{th}~ entry~ of \\\\ k x 1 ~vector}} \\\\\n\\sum_{j=1}^k\na_j-(a_j+c_j)\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\left[n_{1j}  -m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}\\right)\n-m_{2j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right) \\right]_{\\substack{j^{th}~ entry~ of \\\\ k x 1 ~vector}} \\\\\n\\sum_{j=1}^k\na_j-m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1})}\\right)\n\\end{pmatrix}\n\\end{aligned}\\]\n\\[\\begin{aligned}\nI_T (\\boldsymbol{\\beta}_j, \\beta_{k+1} ) &= - \\begin{pmatrix}\ndiag \\left\\{-m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{(1+\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1}))^2}\\right)-m_{2j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{(1+\\exp(\\boldsymbol{\\beta}_j))^2}\\right)\\right\\}_{k\\times k}\n&  \\left\\{-m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{(1+\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1}))^2}\\right)\\right\\}_{k\\times 1}\\\\\n\\left\\{-m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{(1+\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1}))^2}\\right)\\right\\}_{1\\times k}\n& \\left\\{\\sum_{j=1}^k\n-m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j + \\boldsymbol{\\beta}_{k+1})}{(1+\\exp(\\boldsymbol{\\beta}_j + \\beta_{k+1}))^2}\\right) \\right\\}_{1\\times 1}\n\\end{pmatrix}\n\\end{aligned}\\]\nNote, under the restricted MLE, \\(\\beta_{k+1} = 0\\) implies \\[ \\begin{aligned}\np_j&=\\frac{\\exp(\\boldsymbol{\\beta}_j )}{(1+\\exp(\\boldsymbol{\\beta}_j)}= \\frac{b_{j}}{c_j}= \\frac{n_{1j}}{t_j}\\\\\n1-p_j&= \\frac{n_{2j}}{t_j} = 1-\\frac{\\exp(\\boldsymbol{\\beta}_j )}{(1+\\exp(\\boldsymbol{\\beta}_j)}=\\frac{1}{(1+\\exp(\\boldsymbol{\\beta}_j)}\\\\\n\\implies p_j(1-p_j) &= \\frac{n_{1j}n_{2j}}{t_j^2}\n= \\frac{\\exp(\\boldsymbol{\\beta}_j)}{(1+\\exp(\\boldsymbol{\\beta}_j))^2}\n\\end{aligned}\\]\nThus, \\[\\begin{aligned}\n\\overset \\sim I_T (\\boldsymbol{\\beta}_j, \\beta_{k+1} ) &= \\begin{pmatrix}\ndiag \\left\\{m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{(1+\\exp(\\boldsymbol{\\beta}_j))^2}\\right)+m_{2j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{(1+\\exp(\\boldsymbol{\\beta}_j))^2}\\right)\\right\\}_{k\\times k}\n&  \\left\\{m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{(1+\\exp(\\boldsymbol{\\beta}_j))^2}\\right)\\right\\}_{k\\times 1}\\\\\n\\left\\{m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j )}{(1+\\exp(\\boldsymbol{\\beta}_j ))^2}\\right)\\right\\}_{1\\times k}\n& \\left\\{\\sum_{j=1}^k\nm_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j )}{(1+\\exp(\\boldsymbol{\\beta}_j))^2}\\right) \\right\\}_{1\\times 1}\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\ndiag\\left\\{\\frac{(m_{1j}+ m_{2j})n_{1j}n_{2j}}{t_j^2} \\right\\}_{k\\times k}\n&  \\left\\{m_{1j}\\left(\\frac{n_{1j}n_{2j}}{t_j^2} \\right)\\right\\}_{k\\times 1}\\\\\n\\left\\{m_{1j}\\left(\\frac{n_{1j}n_{2j}}{t_j^2} \\right)\\right\\}_{1\\times k}\n& \\left\\{\\sum_{j=1}^k\nm_{1j}\\left(\\frac{n_{1j}n_{2j}}{t_j^2} \\right) \\right\\}_{1\\times 1}\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\ndiag\\left\\{\\frac{t_jn_{1j}n_{2j}}{t_j^2} \\right\\}_{k\\times k}\n&  \\left\\{m_{1j}\\left(\\frac{n_{1j}n_{2j}}{t_j^2} \\right)\\right\\}_{k\\times 1}\\\\\n\\left\\{m_{1j}\\left(\\frac{n_{1j}n_{2j}}{t_j^2} \\right)\\right\\}_{1\\times k}\n& \\left\\{\\sum_{j=1}^k\nm_{1j}\\left(\\frac{n_{1j}n_{2j}}{t_j^2} \\right) \\right\\}_{1\\times 1}\n\\end{pmatrix} \\\\\n\\implies \\overset \\sim I_{T,22}^{-1} &= [\\overset \\sim I_{T,22} -\\overset \\sim I_{T,21} \\overset \\sim I_{T,11}^{-1} \\overset \\sim I_{T,12} ]^{-1} \\\\\n&= \\left(\\sum_{j=1}^k\n\\frac{m_{1j}n_{1j}n_{2j}}{t_j^2} - \\left(\\frac{m_{1j}n_{1j}n_{2j}}{t_j^2}\\right)^2\\frac{t_j}{n_{1j}n_{2j}}\\right)^{-1}\\\\\n&= \\left(\\sum_{j=1}^k\n\\frac{m_{1j}t_jn_{1j}^2n_{2j}^2-m_{1j}^2n_{1j}^2n_{2j}^2}{t_j^3n_{1j}n_{2j}}\\right)^{-1}\\\\\n&= \\left(\\sum_{j=1}^k\n\\frac{m_{1j}(t_j-m_{1j})n_{1j}n_{2j}}{t_j^3}\\right)^{-1}\\\\\n&=\\frac{1}{\\sum_{j=1}^km_{1j}m_{2j}n_{1j}n_{2j}/t_j^3}\\\\\n\\end{aligned}\n\\]\nNow, to compute the score under the null hypothesis, \\[\\begin{aligned}\n\\overset \\sim S(\\beta_j, \\beta_{k+1})&= \\begin{pmatrix}\n\\left[n_{1j}  -m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)\n-m_{2j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right) \\right]_{k \\times 1} \\\\\n\\sum_{j=1}^k\na_j-m_{1j}\\left(\\frac{\\exp(\\boldsymbol{\\beta}_j)}{1+\\exp(\\boldsymbol{\\beta}_j)}\\right)\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\left[n_{1j}  -m_{1j}\\left(\\frac{n_{1j}}{t_j}\\right)\n-m_{2j}\\left(\\frac{n_{1j}}{t_j}\\right) \\right]_{k \\times 1} \\\\\n\\sum_{j=1}^k\na_j-m_{1j}\\left(\\frac{n_{1j}}{t_j}\\right)\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n\\left[n_{1j}  -m_{1j}\\left(\\frac{n_{1j}}{t_j}\\right)\n-m_{2j}\\left(\\frac{n_{1j}}{t_j}\\right) \\right]_{k \\times 1} \\\\\n\\sum_{j=1}^k a_j-m_{1j}\\left(\\frac{n_{1j}}{t_j}\\right)\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}0 \\\\\n\\sum_{j=1}^k a_j-m_{1j}\\left(\\frac{n_{1j}}{t_j}\\right) \\end{pmatrix} \\\\\n\\implies \\overset \\sim S(\\beta_j, \\beta_{k+1})^T \\overset \\sim S(\\beta_j, \\beta_{k+1})&=\n\\left[\\sum_{j=1}^k a_j-m_{1j}\\left(\\frac{n_{1j}}{t_j}\\right)\\right]^2\n\\end{aligned}\\]\nAnd the score statistic follows (matching Wikipedia, not the book OUR CLASS CONCLUDED TEXTBOOK HAS ERROR FROM A PAPER THAT HAD A TYPO):\n\\[\\begin{aligned}\nT_S &= \\overset \\sim S(\\beta_j, \\beta_{k+1})^T [\\overset \\sim I_{T,22} -\\overset \\sim I_{T,21} \\overset \\sim I_{T,11}^{-1} \\overset \\sim I_{T,12} ]^{-1}\\overset \\sim S(\\beta_j, \\beta_{k+1})\\\\\n&=\\frac{\\left[\\sum_{j=1}^k a_j-m_{1j}n_{1j}/t_j\\right]^2}{\\sum_{j=1}^km_{1j}m_{2j}n_{1j}n_{2j}/t_j^3}\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch3.html#section-5",
    "href": "chapters/ch3.html#section-5",
    "title": "Chapter 3: Likelihood-Based Tests and Confidence Regions",
    "section": "3.18",
    "text": "3.18\nConsider having two independent iid samples, the first with a \\(normal(\\mu_1,1)\\) distribution and sample size \\(n_1\\), the second with a \\(normal(\\mu_2,1)\\) distribution and sample size \\(n_2\\). For \\(H_0: \\mu_1= \\mu_2\\) versus \\(H_a: \\mu_1 &lt; \\mu_2\\), find \\(T_{LR}\\) and the testing procedure at \\(\\alpha = 0.05\\).\nSolution:\nNote that under \\(H_a\\) the maximum likelihood estimators are the usual ones if \\(\\bar Y_1 \\leq \\bar Y_2\\), but \\(\\hat \\mu_1 = \\hat \\mu_2 = (n_1 \\bar Y_1 +n_2 \\bar Y _2)/(n_1+n_2)\\) if \\(\\bar Y_1 &gt; \\bar Y_2\\). Also, note that \\(P(2,2) = 1/2\\) in (3.23, p. 152) for this case since the probability is 1/2 that the restricted estimators are the usual sample means with \\(l=2\\) distinct values.\n\\[\\begin{aligned}\n\\mathcal L(\\mu_1,\\mu_2) &= \\prod_{i=1}^{n_1}\\frac{1}{\\sqrt{2 \\pi}} e ^{-\\frac 1 2 (y_{i1}- \\mu_1)^2}\\prod_{j=1}^{n_2}\\frac{1}{\\sqrt{2 \\pi}} e ^{-\\frac 1 2 (y_{j2}- \\mu_2)^2} \\\\\n\\ell(\\mu_1,\\mu_2) &= C -\\frac 1 2\\sum_{i=1}^{n_1} (y_{i1}- \\mu_1)^2-\\frac 1 2\\sum_{j=1}^{n_2} (y_{j2}- \\mu_2)^2\\\\\nT_{LR} &= -2 (\\ell (\\hat \\theta_{RMLE})-\\ell (\\hat \\theta_{MLE})) \\\\\n&=\\begin{cases} -2 \\bigg(\\left[-\\frac 1 2\\sum_{i=1}^{n_1} (y_{i1}- \\bar y_1)^2-\\frac 1 2\\sum_{j=1}^{n_2} (y_{j2}- \\bar y_2)^2\\right] \\\\\n~~~~~~~~~~~~~~-\\left[-\\frac 1 2\\sum_{i=1}^{n_1} (y_{i1}- \\bar y_1)^2-\\frac 1 2\\sum_{j=1}^{n_2} (y_{j2}- \\bar y_2)^2\\right] \\bigg), & \\bar Y_1 \\leq \\bar Y_2\\\\\n-2 \\bigg(\\left[-\\frac 1 2\\sum_{i=1}^{n_1} \\left(y_{i1}-  \\frac{(n_1 \\bar Y_1 +n_2 \\bar Y _2)}{(n_1+n_2)}\\right)^2-\\frac 1 2\\sum_{j=1}^{n_2} \\left(y_{j2}- \\frac{(n_1 \\bar Y_1 +n_2 \\bar Y _2)}{(n_1+n_2)}\\right)^2\\right]\\\\\n~~~~~~~~~~~~~~-\\left[-\\frac 1 2\\sum_{i=1}^{n_1} y_{i1}- \\bar y_1)^2-\\frac 1 2\\sum_{j=1}^{n_2} (y_{j2}- \\bar y_2)^2\\right] \\bigg) , &\\bar Y_1 &gt; \\bar Y_2 \\end{cases} \\\\\n&=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\n\\sum_{i=1}^{n_1} \\left(y_{i1}-  \\frac{n_1 \\bar Y_1 +n_2 \\bar Y _2}{n_1+n_2}\\right)^2+\\sum_{j=1}^{n_2} \\left(y_{j2}- \\frac{n_1 \\bar Y_1 +n_2 \\bar Y _2}{n_1+n_2}\\right)^2\\\\\n~~~~~~~~~~~~~~-\\sum_{i=1}^{n_1} (y_{i1}^2- 2 y_{i1}\\bar y_1 + \\bar y_1^2)-\\sum_{j=1}^{n_2} (y_{j2}^2-2y_{j2}\\bar Y_2 +\\bar Y_2^2) , &\\bar Y_1 &gt; \\bar Y_2 \\end{cases}\n\\end{aligned}\\] \\[\\begin{aligned}\n&=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\n\\sum_{i=1}^{n_1} y_{i1}^2-  2n_1\\bar Y_1 \\frac{n_1 \\bar Y_1 +n_2 \\bar Y _2}{n_1+n_2}+n_1\\left(\\frac{n_1 \\bar Y_1 +n_2 \\bar Y _2}{n_1+n_2}\\right)^2+\\sum_{j=1}^{n_2} y_{j2}^2-2n_2 \\bar Y_2 \\frac{n_1 \\bar Y_1 +n_2 \\bar Y _2}{n_1+n_2} + n_2 \\left(\\frac{n_1 \\bar Y_1 +n_2 \\bar Y _2}{n_1+n_2}\\right)^2\\\\\n~~~~~~~~~~~~~~-\\sum_{i=1}^{n_1} y_{i1}^2 + 2 n_1 \\bar Y_1^2 - n_1\\bar Y_1^2-\\sum_{j=1}^{n_2} y_{j2}^2+2n_2\\bar Y_2^2 -n_2\\bar Y_2^2 , &\\bar Y_1 &gt; \\bar Y_2 \\end{cases} \\\\\n&=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\n-  2\\frac{(n_1 \\bar Y_1 +n_2 \\bar Y _2)^2}{n_1+n_2}+(n_1+n_2)\\left(\\frac{n_1 \\bar Y_1 +n_2 \\bar Y _2}{n_1+n_2}\\right)^2+  n_1 \\bar Y_1^2 +n_2\\bar Y_2^2 , &\\bar Y_1 &gt; \\bar Y_2 \\end{cases} \\\\\n&=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\nn_1 \\bar Y_1^2 +n_2\\bar Y_2^2 -  \\frac{(n_1 \\bar Y_1 +n_2 \\bar Y _2)^2}{n_1+n_2}, &\\bar Y_1 &gt; \\bar Y_2\n\\end{cases}\\\\\n&=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\n\\frac{(n_1+n_2)(n_1 \\bar Y_1^2 +n_2\\bar Y_2^2) -(n_1 \\bar Y_1 +n_2 \\bar Y _2)^2}{n_1+n_2}, &\\bar Y_1 &gt; \\bar Y_2 \\end{cases} \\\\\n&=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\n\\frac{(n_1^2 \\bar Y_1^2 + n_1n_2 \\bar Y_1^2 +n_1n_2 \\bar Y_2^2 +n_2^2\\bar Y_2^2) -(n_1^2 \\bar Y_1^2 +2n_1n_2 \\bar Y_1 \\bar Y_2 +n_2^2 \\bar Y _2^2)}{n_1+n_2}, &\\bar Y_1 &gt; \\bar Y_2 \\end{cases} \\\\\n  &=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\n\\left(\\frac{n_1n_2}{n_1+n_2}\\right)(\\bar Y_1^2 -2 \\bar Y_1 \\bar Y_2 + \\bar Y_2^2), &\\bar Y_1 &gt; \\bar Y_2 \\end{cases} \\\\\n  &=\\begin{cases} 0, & \\bar Y_1 \\leq \\bar Y_2\\\\\n\\left(\\frac{n_1n_2}{n_1+n_2}\\right)(\\bar Y_1-\\bar Y_2)^2, &\\bar Y_1 &gt; \\bar Y_2 \\end{cases}\n\\end{aligned}\\]\nFrom 3.6.1a (p. 151), the \\(T_{LR}\\) follows a non-chi-square distribution, \\(\\bar \\chi^2_{k=2}\\). Note, that the following page states \\(P(\\bar \\chi^2_{k=2} \\geq C) = \\frac 1 2 I(C=0) + \\frac 1 2P(\\chi^2_{k=2} \\geq C)\\)\nSo I need to find the value \\(C\\) such that:\n\\[.05 = \\alpha =P(\\bar \\chi^2_{k=2} \\geq C) = \\frac 1 2 I(C=0) + \\frac 1 2P(\\chi^2_{2} \\geq C)\\] Note that \\(C \\neq 0\\), because if it were, we would get \\(1= P(\\bar \\chi^2_{k=2} \\geq C)\\).\nThus, I am left with, \\[ \\begin{aligned}\n.05 &=  \\frac 1 2P(\\chi^2_{2} \\geq C) \\\\\n\\implies .1 &= P(\\chi^2_{2} \\geq C) \\\\\n\\implies C &= 4.60517 &\\text{by r command qchisq(.1, df = 2, lower.tail = F)}\n\\end{aligned}\\]\nTherefore the testing procedure is to reject \\(H_0\\) if \\(\\bar Y_1&gt; Y_2\\) AND \\(\\left(\\frac{n_1n_2}{n_1+n_2}\\right)(\\bar Y_1-\\bar Y_2)^2 &gt; 4.60517\\)."
  },
  {
    "objectID": "chapters/ch5.html#section",
    "href": "chapters/ch5.html#section",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.1",
    "text": "5.1\nSuppose that \\(Y_1, \\dots, Y_n\\) are identically distributed with mean \\(E(Y_1)= \\mu\\), \\(Var(Y_1)= \\sigma^2\\) and covariances given by \\(Cov(Y_{i},Y_{i+j})= \\begin{cases} \\rho\\sigma^2, & |j| \\leq2 \\\\ 0, &|j| &gt;2 \\end{cases}\\). Prove that \\(\\bar Y \\overset{p}{\\to} \\mu\\) as \\(n \\to \\infty\\).\nSolution:\n\\[\n\\begin{aligned}\nVar(\\bar Y) &= \\frac{1}{n^2}\\left[\\sum_{i=1}^n \\sigma_i^2 + 2\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sigma_{ij}\\right] \\\\\n&= \\frac{1}{n^2}\\left[\\sum_{i=1}^n \\sigma^2 + 2 \\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} \\sigma_{ij}\\right]\\\\\n&= \\frac{1}{n^2}\\left[\\sum_{i=1}^n \\sigma^2 + 2 \\sum_{i=1}^{n-1}\\left(\\sigma_{i,i+1} +\\sigma_{i,i+2}+\\sigma_{i,i+3} + \\dots+\\sigma_{i,n}\\right)\\right] \\\\\n&= \\frac{1}{n^2}\\left[\\sum_{i=1}^n \\sigma^2 + 2 \\sum_{i=1}^{n-1}\\left(\\rho \\sigma^2 +\\rho \\sigma^2\\right)\\right] \\\\\n&= \\frac{1}{n^2}\\left[n\\sigma^2 + 4 (n-1) \\rho \\sigma^2\\right] \\\\\n& \\to 0 \\text{ as } n \\to \\infty.\n\\end{aligned}\n\\] Thus, by Theorem 5.3, \\(\\bar Y - \\bar \\mu \\overset p \\to 0\\) as \\(n \\to \\infty\\). As, in this case, \\(\\bar \\mu = \\mu\\), it is shown that \\(\\bar Y \\overset p \\to \\mu\\) as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "chapters/ch5.html#section-1",
    "href": "chapters/ch5.html#section-1",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.2",
    "text": "5.2\nSuppose that \\(Y_1, \\dots, Y_n\\) are independent random variables with \\(Y_n \\sim N(\\mu,\\sigma_n^2)\\), where the sequence \\(\\sigma_n^2 \\to \\sigma^2 &gt;0\\) as \\(n \\to \\infty\\). Prove that there is no random variable \\(Y\\) such that \\(Y_n \\overset{p}{\\to}Y\\). (Hint: Assume there is such a \\(Y\\) and obtain a contradiction from \\(|Y_n-Y_{n+1}| \\leq |Y_n-Y|+|Y_{n+1}-Y|\\).)\nSolution:\nAssume \\(Y_n \\overset{p}{\\to}Y\\), then \\(|Y_n-Y_{n+1}| \\leq |Y_n-Y|+|Y_{n+1}-Y|\\) by the triangle inequality.\nNote that \\[\\begin{aligned}\nY_n - Y_{n+1} &\\sim N(0,\\sigma_n^2+ \\sigma_{n+1}^2) \\\\\n\\implies \\lim_{n \\to \\infty} Y_n - Y_{n+1} &\\sim N(0,2\\sigma^2) \\\\\n&\\sim \\sqrt{2\\sigma}N(0,1) \\\\\n\\implies \\lim_{n \\to \\infty} (Y_n - Y_{n+1})^2 &\\sim {2\\sigma^2}\\chi^2(1) \\\\\n\\implies E \\left[\\lim_{n \\to \\infty}(Y_n - Y_{n+1})^2\\right] &= {2\\sigma^2} &gt; 0.\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n|Y_n-Y_{n+1}| &\\leq |Y_n-Y|+|Y_{n+1}-Y| \\\\\n\\implies |Y_n-Y_{n+1}|^2 &\\leq (|Y_n-Y|+|Y_{n+1}-Y|)^2 \\\\\n\\implies (Y_n-Y_{n+1})^2 &\\leq (Y_n-Y)^2+2|Y_n-Y||Y_{n+1}-Y|+(Y_{n+1}-Y)^2 \\\\\n\\end{aligned}\n\\] Thus, \\[\n\\begin{aligned}\n0 &lt; {2 \\sigma^2} &= E\\left[\\lim_{n\\to \\infty}(Y_n-Y_{n+1})^2\\right] \\\\\n&\\leq E\\left[\\lim_{n\\to \\infty}(Y_n-Y)^2+2|Y_n-Y||Y_{n+1}-Y|+(Y_{n+1}-Y)^2\\right] \\\\\n&\\leq E\\left[\\lim_{n\\to \\infty}(0)^2+2|0||0|+(0)^2\\right] ~~~~~~~~~~~~~~~~~~~~~~~(Y_n \\overset p \\to Y) \\\\\n&= 0\n\\end{aligned}\n\\]\nWe have achieved a contradiction. Thus, there is no random variable \\(Y\\) such that \\(Y_n \\overset{p}{\\to}Y\\)."
  },
  {
    "objectID": "chapters/ch5.html#section-2",
    "href": "chapters/ch5.html#section-2",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.3",
    "text": "5.3\nShow that \\(Y_n \\overset d \\to c\\) for some constant \\(c\\) implies \\(Y_n \\overset p \\to c\\) by directly using the definitions of convergence in probability and in distribution. Start with \\(P(|Y_n-c| &gt; \\epsilon)\\).\nSolution:\nAssume \\(Y_n \\overset d \\to c\\), then \\(\\lim_{n \\to \\infty} F_{Y_n}(y) = c\\) for all points \\(y\\) where \\(F_{Y_n}(y)\\) is continuous.\n\\[ \\begin{aligned}\n\\lim_{n \\to \\infty}P(|Y_n-c| &gt; \\epsilon) &\\leq \\lim_{n \\to \\infty}\\frac{E(Y_n-c)^2}{\\epsilon^2} \\\\\n&\\leq \\lim_{n \\to \\infty}\\frac{\\int_y(Y_n-c)^2 \\frac{d}{dy}\\left\\{F_{Y_n}(y)\\right\\}dy}{\\epsilon^2} \\\\\n&\\leq \\lim_{n \\to \\infty}\\frac{\\int_y(Y_n-c)^2 (0)dy}{\\epsilon^2} ~~~since~\\lim_{n \\to \\infty} F_{Y_n}(y) = c\\\\\n&= 0\n\\end{aligned}\\]\nThus, \\(Y_n \\overset p \\to c\\) by definition."
  },
  {
    "objectID": "chapters/ch5.html#section-3",
    "href": "chapters/ch5.html#section-3",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.5",
    "text": "5.5\nConsider the simple linear regression setting, \\(Y_i = \\alpha + \\beta x_i + e_i, ~~~i=1,\\dots, n\\), where the \\(x_i\\) are known constants and \\(e_1, \\dots, e_n\\) are iid with mean \\(0\\) and finite variance \\(\\sigma^2\\). After a little algebra, the least squares estimator has the following representation, \\(\\hat \\beta - \\beta = \\frac{\\sum_{i=1}^n (x_i-\\bar x)e_i}{\\sum_{i=1}^n (x_i-\\bar x)^2}\\). Using that representation, prove that \\(\\hat \\beta \\overset p \\to \\beta\\) as \\(n \\to \\infty\\) if \\(\\sum_{i=1}^n(x_i-\\bar x)^2 \\to \\infty\\)\nSolution:\n\\[ \\begin{aligned}\n\\lim_{n \\to \\infty} P(|\\hat \\beta - \\beta| &gt; \\epsilon) &\\leq \\lim_{n \\to \\infty} \\frac{E|\\hat \\beta - \\beta|^r}{\\epsilon^r} & \\text{ by the Markov Inequality} \\\\\n&\\leq \\lim_{n \\to \\infty} E \\left|\\frac{\\sum_{i=1}^n (x_i-\\bar x)e_i}{\\epsilon \\sum_{i=1}^n (x_i-\\bar x)^2}\\right|^r \\\\\n&\\leq \\lim_{n \\to \\infty}E \\left |\\frac{\\sum_{i=1}^n x_ie_i- n\\bar x \\bar e}{\\epsilon \\sum_{i=1}^n (x_i-\\bar x)^2}\\right|^r \\\\\n&\\leq \\lim_{n \\to \\infty}E \\left |\\frac{\\sum_{i=1}^n x_ie_i}{\\epsilon \\sum_{i=1}^n (x_i-\\bar x)^2}\\right|^r & \\bar e \\to 0 ~ by ~ WLLN\\\\\n&\\leq \\lim_{n \\to \\infty}\\left |\\frac{\\sum_{i=1}^n x_iE(e_i)}{\\epsilon \\sum_{i=1}^n (x_i-\\bar x)^2}\\right|^r & note,~E(e_i) =0, so~\\lim_{n \\to \\infty}\\sum_{i=1}^n x_iE(e_i)=0\\\\\n&=0 & \\text{ if } \\lim_{n \\to \\infty} \\sum_{i=1}^n (x_i-\\bar x)^2= \\infty\n\\end{aligned}\\]\nThus, by definition, \\(\\hat \\beta \\overset p \\to \\beta\\) as \\(n \\to \\infty\\) if \\(\\sum_{i=1}^n(x_i-\\bar x)^2 \\to \\infty\\)."
  },
  {
    "objectID": "chapters/ch5.html#section-4",
    "href": "chapters/ch5.html#section-4",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.9",
    "text": "5.9\nLet \\(X_1, \\dots, X_n\\) be iid from a distribution with mean \\(\\mu\\), variance \\(\\sigma^2\\), and finite central moments \\(\\mu_3\\) and \\(\\mu_4\\). Consider \\(\\hat \\theta = \\bar X /s_n\\), a measure of “effect size” used in meta-analysis. Prove that \\(\\hat \\theta \\overset p \\to \\theta = \\mu/\\sigma\\) as \\(n \\to \\infty\\).\nSolution:\n\\[ \\begin{aligned}\n\\bar X  & \\overset p \\to \\mu  & (WLLN) \\\\\ns_n^2 & \\overset p \\to \\sigma^2 & (Example~5.12,~p.~ 227) \\\\\n\\frac{1}{s_n} & \\overset p \\to \\frac 1 \\sigma & (Continuous~Mapping~Theorem) \\\\\n\\implies \\frac{\\bar X}{s_n} & \\overset p \\to \\frac{\\mu} \\sigma & (Slutsky's~Theorem)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch5.html#section-5",
    "href": "chapters/ch5.html#section-5",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.24",
    "text": "5.24\nWhen two independent binomials, \\(X_1\\) is binomial\\((n_1,p_1)\\) and \\(X_2\\) is binomial\\((n_2,p_2)\\), are put in the form of a \\(2\\times 2\\) table (see Example 5.31, p. 240), then one often estimates the odds ratio \\[\\theta = \\frac{\\frac{p_1}{1-p_1}}{\\frac{p_2}{1-p_2}} = \\frac{p_1(1-p_2)}{p_2(1-p_1)}.\\] The estimate \\(\\hat \\theta\\) is obtained by inserting \\(\\hat{p_1} = X_1/n_1\\) and \\(\\hat p_2 = X_2/n_2\\) in the above expression. Show that \\(\\log(\\hat \\theta)\\) has asymptotic variance \\[\\frac{1}{n_1p_1(1-p_1)}+\\frac{1}{n_1p_2(1-p_2)}.\\]\nSolution:\nLet \\(\\lambda = \\frac{n_1}{n_1+n_2}= \\frac{n_1}{n}\\) Then, following class notes and by central limit theorem,\n\\[\n\\sqrt n  \\ \\begin{pmatrix} \\left(\\frac{X_1}{n_1} - p_1 \\right) \\\\\n\\left( \\frac{X_2}{n_2} - p_2 \\right) \\end{pmatrix} \\overset d \\to MVN \\left(\\mathbf 0, \\begin{bmatrix} \\frac{p_1 \\left(1 - p_1 \\right)}{\\lambda} & 0 \\\\\n0 & \\frac{p_2 \\left(1 - p_2 \\right)}{1-\\lambda} \\end{bmatrix}  \\right)\n\\] Define \\[ \\begin{aligned}\ng \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} &= \\log \\left(\\frac{p_1(1-p_2)}{p_2(1-p_1)}\\right) \\\\\n\\implies g' \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}  &= \\begin{pmatrix} \\frac{p_2(1-p_1)}{p_1(1-p_2)} \\frac{p_2(1-p_1)(1-p_2)- p_1(1-p_2)(-p_2)}{p_2^2(1-p_1)^2} \\\\ \\frac{p_2(1-p_1)}{p_1(1-p_2)} \\frac{p_2(1-p_1)(-p_1)- p_1(1-p_2)(1-p_1)}{p_2^2(1-p_1)^2}  \\end{pmatrix} \\\\\n&=\\begin{pmatrix} \\frac{p_2(1-p_1)}{p_1(1-p_2)}  \\frac{(1-p_1)(1-p_2)+ p_1(1-p_2)}{p_2(1-p_1)^2} \\\\ \\frac{p_2(1-p_1)}{p_1(1-p_2)} \\frac{p_2(-p_1)- p_1(1-p_2)}{p_2^2(1-p_1)}  \\end{pmatrix} \\\\\n&=\\begin{pmatrix} \\frac{p_2(1-p_1)}{p_1(1-p_2)} \\frac{1-p_2}{p_2(1-p_1)^2} \\\\ \\frac{p_2(1-p_1)}{p_1(1-p_2)} \\frac{-p_1}{p_2^2(1-p_1)}  \\end{pmatrix} \\\\\n&=\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ \\frac{-1}{(1-p_2)p_2}\\end{pmatrix}\n\\end{aligned}\\]\nThus, by delta method, \\[\\begin{aligned}\n\\log(\\hat \\theta) &= g \\begin{pmatrix} \\frac{X_1}{n_1} - p_1 \\\\ \\frac{X_2}{n_2} - p_2 \\end{pmatrix} \\\\\n&\\sim AN \\left( \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix},\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ \\frac{-1}{(1-p_2)p_2}\\end{pmatrix}^T \\begin{pmatrix} \\frac{p_1 \\left(1 - p_1 \\right)}{\\lambda} & 0 \\\\\n0 &  \\frac{p_2 \\left(1 - p_2 \\right)}{1-\\lambda} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ \\frac{-1}{(1-p_2)p_2}\\end{pmatrix}/n\\right) \\\\\n&\\sim AN \\left( \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix},\\begin{pmatrix} \\frac{1}{\\lambda} & \\frac{1}{1-\\lambda}  \\end{pmatrix} \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ \\frac{-1}{(1-p_2)p_2}\\end{pmatrix}/n\\right)\\\\\n&\\sim AN \\left( \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix},\\left( \\frac{1}{\\lambda p_1(1-p_1)} + \\frac{1}{(1-p_2)p_2 (1-\\lambda)}\\right)/n\\right)\\\\\n&\\sim AN \\left( \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix},\\left( \\frac{1}{\\left(\\frac{n_1}{n}\\right) p_1(1-p_1)} + \\frac{1}{(1-p_2)p_2 \\left(1-\\left(\\frac{n_1}{n_1+n_2}\\right)\\right)}\\right)/n\\right)\\\\\n&\\sim AN \\left( \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix},\\left( \\frac{1}{n_1 p_1(1-p_1)} + \\frac{1}{n_2p_2(1-p_2) }\\right)\\right)\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch5.html#a",
    "href": "chapters/ch5.html#a",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.27 a)",
    "text": "5.27 a)\nFor an iid sample \\(Y_1,\\dots, Y_n\\), consider finding the asymptotic joint distribution of \\((\\bar Y, s_n, s_n/ \\bar Y)\\) using Theorem 5.20 (p. 239) and (5.34, p. 256). Find the matrices \\(\\mathbf{g'(\\boldsymbol \\theta)}\\) and \\(\\boldsymbol \\Sigma\\) used to compute the asymptotic covariance \\(\\mathbf{g'(\\boldsymbol \\theta) \\boldsymbol \\Sigma g'(\\boldsymbol \\theta)^T}\\).\nSolution:\nBy Theorem 5.20, \\[ \\begin{aligned}\n\\sqrt n \\begin{pmatrix} \\bar Y - \\mu \\\\  s_n^2-\\sigma^2\\end{pmatrix} &\\overset d \\to N\\left(\\mathbf 0, \\begin{pmatrix} \\sigma^2 & \\mu_3 \\\\ \\mu_3 & \\mu_4-\\sigma^4 \\end{pmatrix} \\right) \\\\\n\\implies \\begin{pmatrix} \\bar Y \\\\  s_n^2 \\end{pmatrix} & is~ AN\\left( \\begin{pmatrix} \\mu \\\\  \\sigma^2\\end{pmatrix}, \\begin{pmatrix} \\sigma^2 & \\mu_3 \\\\ \\mu_3 & \\mu_4-\\sigma^4 \\end{pmatrix}/n \\right)\n\\end{aligned}\\]\nDefine \\(g \\begin{pmatrix} a \\\\b \\end{pmatrix} = \\begin{pmatrix} a \\\\ \\sqrt b \\\\ \\frac {\\sqrt b} a \\end{pmatrix}\\) Then, \\(g' \\begin{pmatrix} a \\\\b \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{2 \\sqrt b} \\\\ \\frac{-\\sqrt b}{a^2} & \\frac{1}{2a\\sqrt b}\\end{pmatrix}\\).\nBy Delta Method,\n\\[\\begin{aligned}\ng\\begin{pmatrix} \\bar Y \\\\  s_n^2 \\end{pmatrix} & is~ AN\\left( g \\begin{pmatrix} \\mu \\\\  \\sigma^2\\end{pmatrix}, \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{2 \\sqrt b} \\\\ \\frac{-\\sqrt b}{a^2} & \\frac{1}{2a\\sqrt b}\\end{pmatrix}\\begin{pmatrix} \\sigma^2 & \\mu_3 \\\\ \\mu_3 & \\mu_4-\\sigma^4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{2 \\sqrt b} \\\\ \\frac{-\\sqrt b}{a^2} & \\frac{1}{2a\\sqrt b}\\end{pmatrix}^T/n\\right)\n\\end{aligned}\\]\nIn conclusion, \\[ \\begin{aligned}\ng'(\\boldsymbol \\theta) =\\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{2 \\sqrt b} \\\\ \\frac{-\\sqrt b}{a^2} & \\frac{1}{2a\\sqrt b}\\end{pmatrix} \\\\\n\\boldsymbol \\Sigma = \\begin{pmatrix} \\sigma^2 & \\mu_3 \\\\ \\mu_3 & \\mu_4-\\sigma^4 \\end{pmatrix} \\\\\ng'(\\boldsymbol \\theta)^T =\\begin{pmatrix} 1 & 0 &  \\frac{-\\sqrt b}{a^2} \\\\ 0 & \\frac{1}{2 \\sqrt b} & \\frac{1}{2a\\sqrt b}\\end{pmatrix} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch5.html#section-6",
    "href": "chapters/ch5.html#section-6",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.29",
    "text": "5.29\nIn most of Chapter 5 we have dealt with iid samples of size \\(n\\) of either univariate or multivariate random variables. Another situation of interest is when we have a number of different independent samples of different sizes. For simplicity, consider the case of two iid samples, \\(X_1, \\dots, X_m\\) and \\(Y_1, \\dots Y_n\\), with common variance \\(\\sigma^2\\) and under a null hypothesis they have a common mean, say \\(\\mu\\). Then the two-sample pooled t statistic is \\[t_p = \\frac{\\bar X- \\bar Y}{\\sqrt{s_p^2(\\frac{1}{m}+ \\frac 1 n)}},\\] where \\[s_p^2= \\frac{(m-1)s_X^2 +(n-1)s_Y^2}{m+n-2}\\] and \\[s_X^2 = \\frac{1}{m-1} \\sum_{i=1}^m (X_i-\\bar X)^2,~~s_Y^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2.\\] It can be shown that \\(t_p \\overset d \\to N(0,1)\\) as \\(\\min(m,n) \\to \\infty\\). However, the proof is fairly tricky. Instead it is common to assume that both sample sizes go to 1 at a similar rate, i.e., \\(\\lambda_{m,n} = m/(m+n) \\to \\lambda &gt;0\\) as \\(\\min(m,n) \\to \\infty\\). Under this assumption prove that \\(t_p \\overset d \\to N(0,1)\\). Hint: show that \\(t_p = \\left[\\sqrt{1- \\lambda_{m,n}}\\sqrt m (\\bar X-\\mu)- \\sqrt{1- \\lambda_{m,n}}\\sqrt n (\\bar Y-\\mu) \\right]/s_p\\).\nSolution:\n\\[\\begin{aligned}\nt_p &= \\frac{\\bar X- \\bar Y}{\\sqrt{s_p^2(\\frac{1}{m}+ \\frac 1 n)}} \\\\\n&= \\frac{(\\bar X - \\mu)- (\\bar Y-\\mu)}{s_p\\sqrt{\\frac{n+m}{nm}}} \\\\\n&= \\frac{\\sqrt{\\frac{nm}{n+m}}(\\bar X - \\mu)- \\sqrt{\\frac{nm}{n+m}}(\\bar Y-\\mu)}{s_p} \\\\\n&= \\frac{\\sqrt{\\frac{n+m-m}{n+m}} \\sqrt m(\\bar X - \\mu)- \\sqrt{\\frac{m}{n+m}}\\sqrt n(\\bar Y-\\mu)}{s_p} \\\\\n&= \\frac{\\sqrt{1-\\lambda_{m,n}} \\sqrt m(\\bar X - \\mu)- \\sqrt{\\lambda_{m,n}}\\sqrt n(\\bar Y-\\mu)}{s_p} \\\\\n\\end{aligned}\\]\nNote that: \\[\\begin{aligned}\ns_p^2 &= \\frac{(m-1)s_X^2 +(n-1)s_Y^2}{m+n-2} \\\\\n&\\overset P \\to \\frac{(m-1)\\sigma^2 +(n-1)\\sigma^2}{m+n-2} \\\\\n&= \\frac{(m+n-2)\\sigma^2}{m+n-2} = \\sigma^2 \\\\\n\\implies s_p &\\overset P \\to \\sigma \\\\\n\\sqrt m(\\bar X - \\mu) &\\overset d \\to N(0,\\sigma^2) \\\\\n\\sqrt n(\\bar Y - \\mu) &\\overset d \\to N(0,\\sigma^2)\n\\end{aligned}\\]\n\\[\\begin{aligned}\nt_p &= \\frac{\\sqrt{1-\\lambda_{m,n}} \\sqrt m(\\bar X - \\mu)- \\sqrt{\\lambda_{m,n}}\\sqrt n(\\bar Y-\\mu)}{s_p} \\\\\n& \\overset d \\to \\frac{\\sqrt{1-\\lambda}N(0,\\sigma^2)- \\sqrt{\\lambda}N(0,\\sigma^2)} \\sigma \\\\\n& \\overset d \\to N(0,1-\\lambda)-N(0, \\lambda) \\\\\n& \\overset d \\to N(0,1)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch5.html#section-7",
    "href": "chapters/ch5.html#section-7",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.33",
    "text": "5.33\nLet \\((X_1,Y_1),\\dots ,(X_n,Y_n)\\) be iid pairs with \\(E(X_1) = \\mu_1\\), \\(E(Y_1) =\\mu_2\\), \\(Var(X_1) = \\sigma_1^2\\), \\(Var(Y_1) = \\sigma_2^2\\), and \\(Cov(X_1, Y_1) = \\sigma_{12}\\).\n\na.\nWhat can we say about the asymptotic distribution of \\((\\bar X, \\bar Y)^T\\)?\nSolution:\nBy Central Limit theorem, \\((\\bar X, \\bar Y)^T\\)is \\(AN\\left( \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 & \\sigma_{12} \\\\ \\sigma_{12} & \\sigma_2^2 \\end{pmatrix} \\bigg/n \\right)\\).\n\n\nb. Suppose that \\(\\mu_1=\\mu_2=0\\) and let \\(T = (\\bar X)(\\bar Y)\\). Show that \\(nT \\overset d \\to Q\\) as \\(n \\to \\infty\\) and describe the random variable Q.\nSolution:\nAttempt 1: INCORRECT if X and Y are correlated\nBy CLT, \\(\\sqrt n \\bar X \\overset d \\to N(0,\\sigma_1^2)\\), \\(\\sqrt n \\bar Y \\overset d \\to N(0,\\sigma_2^2)\\). Thus, \\[\\begin{aligned}\\frac{\\sqrt n \\bar X}{\\sigma_1} &\\overset d \\to N(0,1)\\\\\n\\frac{\\sqrt n \\bar Y}{\\sigma_2} &\\overset d \\to N(0,1)\\\\\n\\implies \\left(\\frac{\\sqrt n \\bar X}{\\sigma_1} \\right)\\left(\\frac{\\sqrt n \\bar Y}{\\sigma_2} \\right) &\\overset d \\to \\chi^2(1) ~~~ \\mathbf{assumes}~~ \\sigma_{12}=0\\\\\n\\implies n \\bar X \\bar Y &\\overset d \\to \\sigma_1\\sigma_2\\chi^2(1)\n\\end{aligned} \\]\nAttempt 2: INCORRECT if \\(\\sigma_{12} =0\\), we should get a \\(\\chi^2\\) distribution\nNote that \\(\\bar X -0= \\frac{1}{n} \\sum_{i=1}^n h_1(X_i,Y_i) + R_{n1}\\), where \\(h_1(X_i) = X_i\\) and \\(R_{n1} = 0\\). Similarly, \\(\\bar Y - 0 = \\frac{1}{n} \\sum_{i=1}^n h_2(X_i,Y_i) + R_{n2}\\), where \\(h_2(Y_i) = Y_i\\) and \\(R_{n2} = 0\\). Note, in this case \\(E(h_1(X_i,Y_i)) =E(h_2(X_i,Y_i)) =0\\).\nDefine \\(g((a,b)^T) = ab\\), which implies \\(g'(a,b) = (b,a)^T\\). Note that g is a real valued function with partial derivatives existing in the neighborhood of \\(\\mathbf 0\\) and is continuous at \\(\\mathbf 0\\). By Thm 5.27, \\[\\begin{aligned}\ng((\\bar X, \\bar Y)) -g(\\boldsymbol{\\theta}) &= \\frac{1}{n} \\sum_{i=1}^n \\left[g_1' (\\boldsymbol \\theta)h_1(X_i,Y_i)+g_2'(\\boldsymbol \\theta)h_2(X_i,Y_i)\\right] + R_n, ~~where~\\sqrt n R_n \\overset p \\to 0 \\\\\n\\bar X \\bar Y -g(\\mathbf 0) &= \\frac{1}{n} \\sum_{i=1}^n \\left[g_1' (\\mathbf 0)X_i+g_2'(\\mathbf 0)Y_i\\right] + R_n \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n [0] + R_n \\\\\n\\end{aligned}\\] Thus, by Theorem 5.23, \\(\\bar X \\bar Y \\overset d \\to N(0,0)\\)\nAttempt 3: LEADS NOWHERE\nDefine \\(g((a,b)^T) = ab\\). Then, by (Taylor, 1712)\n\\[\\begin{aligned}\ng((\\bar X,  \\bar Y)^T) - g((\\mu_1,\\mu_2)^T) &= \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix}^T \\begin{pmatrix} \\mu_2 \\\\ \\mu_1 \\end{pmatrix} + \\frac 1 2 \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix}^T \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix} \\\\\n&~~~~~~~~~+ \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix}^T \\begin{pmatrix} 0 & 0 & 0 & 0\\\\ 0 & 0& 0 &0 \\end{pmatrix} \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix} \\otimes\\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix}^T \\begin{pmatrix} \\mu_2 \\\\ \\mu_1 \\end{pmatrix} + \\frac 1 2 \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix}^T \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\bar X - \\mu_1  \\\\ \\bar Y- \\mu_2\\end{pmatrix}\n\\end{aligned}\\] Thus, in the case where \\(\\mu_1=\\mu_2= 0\\) \\[\\begin{aligned}\ng((\\bar X,  \\bar Y)^T) - g((0,0)^T) &=  \\begin{pmatrix} \\bar X  & \\bar Y \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac 1 2 \\begin{pmatrix} \\bar X & \\bar Y\\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\bar X  \\\\ \\bar Y \\end{pmatrix} \\\\\n\\implies \\bar X \\bar Y &= 0 + \\frac 1 2 \\begin{pmatrix} \\bar Y & \\bar X\\end{pmatrix} \\begin{pmatrix} \\bar X  \\\\ \\bar Y \\end{pmatrix} =  \\bar X \\bar Y\n\\end{aligned}\\]\nSo doing a Taylor expansion is not useful.\n\n\nc.\nSuppose that \\(\\mu_1 = 0, \\mu_2 \\neq 0\\) and let \\(T = (\\bar X) (\\bar Y)\\). Show that \\(\\sqrt n T \\overset d \\to R\\) as \\(n \\to \\infty\\) and describe the random variable \\(R\\).\nSolution:\nDefine \\(g (a , b)^T = ab \\implies g( \\bar X, \\bar Y)= \\bar X \\bar Y\\), \\(g'\\begin{pmatrix} a\\\\ b \\end{pmatrix} = \\begin{pmatrix} b \\\\ a \\end{pmatrix}\\). By Theorem 5.26,\n\\[\\begin{aligned}\ng\\begin{pmatrix} \\bar X \\\\ {\\bar Y} \\end{pmatrix} - g\\begin{pmatrix} {0} \\\\ \\mu_2 \\end{pmatrix}\n&= \\frac 1 n \\sum_{i=1}^n \\begin{pmatrix} \\mu_2 & 0 \\end{pmatrix} \\begin{pmatrix} X_i \\\\ Y_i - \\mu_2 \\end{pmatrix} + R_n,  ~~ where \\sqrt n R_n \\overset p \\to 0 \\\\\n&= \\mu_2 \\bar X + R_n \\\\\n\\implies  \\sqrt n \\left( g\\begin{pmatrix} \\bar X \\\\ {\\bar Y} \\end{pmatrix} - g\\begin{pmatrix} {0} \\\\ \\mu_2 \\end{pmatrix} \\right) &=  \\mu_2(\\sqrt n \\bar X) +\\sqrt n R_n \\\\\n\\implies  \\sqrt n  \\bar X {\\bar Y}  &=  \\mu_2(\\sqrt n \\bar X) +\\sqrt n R_n \\\\\n& \\overset d \\to N(0, \\sigma_1^2 + \\mu_2^2)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch5.html#section-8",
    "href": "chapters/ch5.html#section-8",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.32",
    "text": "5.32\nSuppose that \\(\\hat \\theta_1, \\dots, \\hat \\theta_k\\) each satisfy the assumptions of Theorem 5.23 (p. 242): \\[\\hat \\theta_i - \\theta_i = \\frac 1 n \\sum_{j=1}^n h_i(X_j) + R_{in},~~~\\sqrt n R_{in} \\overset p \\to 0,\\] and \\(E[h_i(X_1)] = 0\\) and \\(var[h_i(X_1)] = \\sigma_{hi}^2 &lt; \\infty\\). Let \\(T = \\sum_{i=1}^k c_i \\hat \\theta_i\\) for any set of constants \\(c_1, \\dots, c_k\\). Find the correct approximating function \\(h_T\\) for \\(T\\), show that Theorem 5.23 may be used (verify directly without using later theorems), and find the limiting distribution of \\(T\\).\nSolution:\n\\[\\begin{aligned}\n\\hat \\theta_i - \\theta_i &= \\frac 1 n \\sum_{j=1}^n h_i(X_j) + R_{in},~~~\\sqrt n R_{in} \\overset p \\to 0 \\\\\n\\implies c_i(\\hat \\theta_i - \\theta_i) &= c_i\\left(\\frac 1 n \\sum_{j=1}^n h_i(X_j) + R_{in} \\right),~~~\\sqrt n R_{in} \\overset p \\to 0 \\\\\n\\implies c_i \\hat \\theta_i - c_i\\theta_i &= \\frac 1 n \\sum_{j=1}^n c_ih_i(X_j) + c_iR_{in},~~~\\sqrt n R_{in} \\overset p \\to 0 \\\\\n\\end{aligned}\\]\nNote that \\(c_i \\sqrt nR_{in} \\overset p \\to 0\\) by Slutsky’s Theorem. \\(E[c_i h_i (X_i)] = 0\\), \\(Var[c_i h_i (X_1)] = c_i^2 \\sigma_{hi}^2 &lt; \\infty\\).\n\\[\\begin{aligned}\nc_i \\hat \\theta_i - c_i\\theta_i &= \\frac 1 n \\sum_{j=1}^n c_ih_i(X_j) + c_iR_{in},~~~\\sqrt n c_iR_{in} \\overset p \\to 0 \\\\\n\\implies \\sum_{i=1}^k c_i \\hat \\theta_i - c_i\\theta_i &= \\sum_{i=1}^k \\frac 1 n \\sum_{j=1}^n c_ih_i(X_j) + c_iR_{in},~~~\\sqrt n c_iR_{in} \\overset p \\to 0 \\\\\n\\implies T -\\sum_{i=1}^k c_i\\theta_i &= \\frac 1 n \\sum_{j=1}^n \\left(\\left[\\sum_{i=1}^k c_ih_i(X_j) \\right] + \\left[\\sum_{i=1}^k c_iR_{in} \\right]\\right),~~~\\sqrt n c_iR_{in} \\overset p \\to 0 \\\\\n\\end{aligned}\\]\nAs \\(\\left[\\sum_{i=1}^k c_iR_{in} \\right] \\overset p \\to 0\\) because a finite sum of random variables that converge in probability to zero will converge in probability to 0 by Slutsky’s Theorem.\nTherefore, the correct approximating of T is \\(h_T =\\left[\\sum_{i=1}^k c_ih_i(X_j) \\right]\\), which has mean 0 and variance \\(\\sum_{i=1}^k c_i^2 \\sigma_{hi}^2 &lt; \\infty\\).\nThus, all conditions apply to use Theorem 5.23 and \\(\\sqrt n\\left(T -\\sum_{i=1}^k c_i\\theta_i \\right) \\overset d \\to N\\left(0, \\sum_{i=1}^k c_i^2 \\sigma_{hi}^2\\right)\\). In other words, the limiting distribution of \\(T\\) is \\(AN\\left(0, \\frac 1 n \\sum_{i=1}^k c_i^2 \\sigma_{hi}^2\\right)\\)."
  },
  {
    "objectID": "chapters/ch5.html#section-9",
    "href": "chapters/ch5.html#section-9",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.40",
    "text": "5.40\nFormulate an extension of Theorem 5.27 (p. 247) for the situation of two independent samples \\(X_1, \\dots, X_m\\) and \\(Y_1, \\dots, Y_n\\). The statistic of interest is \\(T = g(\\hat \\theta_1, \\hat \\theta_2)\\), and the conclusion is \\[g(\\hat \\theta_1, \\hat \\theta_2) - g(\\theta_1, \\theta_2) = \\frac 1 m \\sum_{i=1}^m g_1'(\\boldsymbol \\theta) h_1(X_i) + \\frac 1 n \\sum_{i=1}^n g_2'(\\boldsymbol \\theta) h_2(Y_i) + R_{mn},~~ \\sqrt{\\max(m,n)} R_{m,n} \\overset p \\to 0.\\]\nSolution:\nI define \\(\\boldsymbol \\theta = (\\theta_1, \\theta_2)^T\\)\n\\[ \\begin{aligned}\ng(\\hat \\theta_1, \\hat \\theta_2) - g(\\theta_1, \\theta_2) &= (\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2) g'(\\theta_1,\\theta_2) + \\frac{1}{2}(\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2) g^{(2)}(\\theta^*) (\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2)^T \\\\\n\\end{aligned}\n\\] For some \\(\\boldsymbol \\theta^*\\) in the neighborhood between \\(\\boldsymbol {\\hat \\theta}\\) and \\(\\boldsymbol \\theta\\). Looking carefully at the first component of the right hand side, \\[ \\begin{aligned}\n(\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2) g'(\\theta_1,\\theta_2) &=\n(\\hat \\theta_1 - \\theta_1)g_1'(\\boldsymbol \\theta) +(\\hat \\theta_2 - \\theta_2)g_2'(\\boldsymbol \\theta) \\\\\nby~ Theorem~5.27&= \\left(\\frac 1 m \\sum_{i=1}^m h_1(X_i) +R_{m1} \\right)g_1'(\\boldsymbol \\theta) +\\left(\\frac 1 n \\sum_{i=1}^n h_2(Y_i) +R_{n2} \\right)g_2'(\\boldsymbol \\theta),~~where~ \\substack{\\sqrt n R_{n2} \\overset p \\to 0 \\\\ \\sqrt m R_{m1} \\overset p \\to 0 } \\\\\n&= \\frac 1 m \\sum_{i=1}^m h_1(X_i)  g_1'(\\boldsymbol \\theta) +\\frac 1 n \\sum_{i=1}^n h_2(Y_i)  g_2'(\\boldsymbol \\theta)+R_{m1}g_1'(\\boldsymbol \\theta)+R_{n2}g_2'(\\boldsymbol \\theta)\n\\end{aligned}\n\\] Combining this with the second component, we get:\n\\[g(\\hat \\theta_1, \\hat \\theta_2) - g(\\theta_1, \\theta_2) = \\frac 1 m \\sum_{i=1}^m h_1(X_i)  g_1'(\\boldsymbol \\theta) +\\frac 1 n \\sum_{i=1}^n h_2(Y_i)  g_2'(\\boldsymbol \\theta) + R_{mn}\\] where, \\[R_{mn} = R_{m1}g_1'(\\boldsymbol \\theta)+R_{n2}g_2'(\\boldsymbol \\theta) + \\frac{1}{2}(\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2) g^{(2)}(\\theta^*) (\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2)^T.\\]\nNote that \\(\\sqrt m R_{m1}g_1'(\\boldsymbol \\theta) \\overset p \\to 0\\) and \\(\\sqrt n R_{n2}g_2'(\\boldsymbol \\theta) \\overset p \\to 0\\) by Slutsky’s Theorem. To finish this prove I only need to show \\(\\frac{1}{2}(\\hat \\theta_1 - \\theta_1 , \\hat \\theta_2 - \\theta_2) g^{(2)}(\\theta^*) (\\hat \\theta_1 - \\theta_1 , \\hat \\theta_2 - \\theta_2)^T \\overset p \\to 0\\)\n\\[\n\\begin{aligned}\n\\frac{1}{2}&(\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2) g^{(2)}(\\boldsymbol \\theta^*) (\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2)^T = \\frac{1}{2}(\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2)\\begin{pmatrix}g_{11}^{(2)}(\\boldsymbol \\theta^*) &g_{12}^{(2)}(\\boldsymbol \\theta^*) \\\\ g_{21}^{(2)}(\\boldsymbol \\theta^*) &g_{22}^{(2)}(\\boldsymbol \\theta^*)\\end{pmatrix} (\\hat \\theta_1  - \\theta_1 , \\hat \\theta_2 - \\theta_2)^T \\\\\n&= \\frac{1}{2}\\left((\\hat \\theta_1  - \\theta_1)g_{11}^{(2)}(\\boldsymbol \\theta^*) +(\\hat \\theta_2  - \\theta_2)g_{21}^{(2)}(\\boldsymbol \\theta^*), (\\hat \\theta_1  - \\theta_1)g_{12}^{(2)}(\\boldsymbol \\theta^*) +(\\hat \\theta_2  - \\theta_2)g_{22}^{(2)}(\\boldsymbol \\theta^*)\\right) \\begin{pmatrix} \\hat \\theta_1  - \\theta_1 \\\\ \\hat \\theta_2  - \\theta_2\\end{pmatrix} \\\\\n&= \\frac{1}{2}\\left((\\hat \\theta_1  - \\theta_1)^2g_{11}^{(2)}(\\boldsymbol \\theta^*) +(\\hat \\theta_1  - \\theta_1)(\\hat \\theta_2  - \\theta_2)g_{21}^{(2)}(\\boldsymbol \\theta^*) + (\\hat \\theta_1  - \\theta_1)(\\hat \\theta_2  - \\theta_2)g_{12}^{(2)}(\\boldsymbol \\theta^*) +(\\hat \\theta_2  - \\theta_2)^2g_{22}^{(2)}(\\boldsymbol \\theta^*)\\right)\n\\end{aligned}\n\\] We know that \\(\\sqrt m (\\hat \\theta_1 - \\theta_1) \\to 0\\) and \\(\\sqrt n (\\hat \\theta_2 - \\theta_2) \\to 0\\). This implies \\(\\sqrt {\\max(m,n)} (\\hat \\theta_1 - \\theta_1) \\to 0\\) and \\(\\sqrt {\\max(m,n)} (\\hat \\theta_2 - \\theta_2) \\to 0\\).\nAccordingly, \\(\\max(m,n) (\\hat \\theta_1 - \\theta_1)^2 \\to 0\\), \\(\\max(m,n) (\\hat \\theta_2 - \\theta_2)^2 \\to 0\\), and \\(\\max(m,n) (\\hat \\theta_1 - \\theta_1)(\\hat \\theta_2 - \\theta_2) \\to 0\\).\nThus, \\[\\frac{\\max(m,n)}{2}\\left((\\hat \\theta_1  - \\theta_1)^2g_{11}^{(2)}(\\boldsymbol \\theta^*) +(\\hat \\theta_1  - \\theta_1)(\\hat \\theta_2  - \\theta_2)g_{21}^{(2)}(\\boldsymbol \\theta^*) + (\\hat \\theta_1  - \\theta_1)(\\hat \\theta_2  - \\theta_2)g_{12}^{(2)}(\\boldsymbol \\theta^*) +(\\hat \\theta_2  - \\theta_2)^2g_{22}^{(2)}(\\boldsymbol \\theta^*)\\right) \\overset p \\to 0\\]\nI have shown \\[g(\\hat \\theta_1, \\hat \\theta_2) - g(\\theta_1, \\theta_2) = \\frac 1 m \\sum_{i=1}^m h_1(X_i)  g_1'(\\boldsymbol \\theta) +\\frac 1 n \\sum_{i=1}^n h_2(Y_i)  g_2'(\\boldsymbol \\theta) + R_{mn},\\] where \\(\\sqrt{max(m,n)} R_{mn} \\overset p \\to 0\\)."
  },
  {
    "objectID": "chapters/ch5.html#section-10",
    "href": "chapters/ch5.html#section-10",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.42",
    "text": "5.42\nThinking of the \\(k^{th}\\) central moment as a functional, \\(T_k(F)= \\int (t-\\mu)^k d F(t)\\), show that the Gateaux derivative is given by \\(T_k(F;\\Delta) = \\int \\left\\{t-T_1(F)\\right\\}^k d \\Delta(t) - T_1(F;\\Delta) \\int k \\left\\{t-T_1(F) \\right\\}^{k-1} d F(t)\\), where \\(T_1(F;\\Delta) = \\int t d \\Delta(t)\\) is the Gateaux derivative for the mean functional diven in Example 5.5.8i (p.253). Then, substitute \\(\\Delta(t)= \\delta_x(t)- F(t)\\) and obtain \\(h_k\\) given in Theorem 5.24 (p. 243).\nSolution:\n\\[\n\\begin{aligned}\nT_k(F) &= \\int (t-\\mu)^k d F(t) =\\int \\left(t-\\int_s s dF(s)\\right)^k dF(t) \\\\\n\\implies T_k(F;\\Delta) &= \\frac{\\partial}{\\partial \\epsilon} T_k(F+\\epsilon \\Delta) \\bigg|_{\\epsilon = 0^+} = \\int\\frac{\\partial}{\\partial \\epsilon} \\left\\{ \\left(t-\\int_s s d\\{F(s)+\\epsilon \\Delta(s) \\}\\right)^k d \\{F(t)+\\epsilon \\Delta(t) \\} \\right\\}  \\bigg|_{\\epsilon = 0^+} \\\\\n&= \\int k\\left(t-\\int_s s d\\{F(s)+\\epsilon \\Delta(s) \\}\\right)^{k-1}\\left(-\\int_s s d \\Delta(s)\\right) d \\{F(t)+\\epsilon \\Delta(t) \\} \\\\\n&~~~~~~~~~~~~~~~~~~~+\\left(t-\\int_s s d\\{F(s)+\\epsilon \\Delta(s) \\}\\right)^k d\\Delta(t) \\bigg|_{\\epsilon = 0^+} \\\\\n&= \\int k\\left(t-\\int_s s dF(s)\\right)^{k-1}\\left(-\\int_s s d \\Delta(s)\\right) d F(t) +\\left(t-\\int_s s dF(s)\\right)^k d\\Delta(t)  \\\\\n&=\\int \\left(t-T_1(F)\\right)^k d\\Delta(t) -T_1(F;\\Delta)\\int k\\left(t-T_1(F)\\right)^{k-1} d F(t)   \\\\\n\\end{aligned}\n\\] Substituting \\(\\Delta(t)= \\delta_x(t)- F(t)\\), and noting that \\(T_1(F;\\Delta) = x-\\mu\\) \\[\n\\begin{aligned}\n&= \\int \\left(t-T_1(F)\\right)^k d\\left\\{\\delta_x(t)- F(t)\\right\\} -T_1(F;\\Delta)\\int k\\left(t-T_1(F)\\right)^{k-1} d F(t) \\\\\n&= \\int \\left(t-\\mu\\right)^k d\\delta_x(t)- \\int \\left(t-\\mu\\right)^kdF(t) -(x-\\mu)\\int k\\left(t-\\mu\\right)^{k-1} d F(t) \\\\\n&= \\left(x-\\mu\\right)^k - \\mu_k -k (x-\\mu)\\mu_{k-1}  \n\\end{aligned}\n\\] Which is the same \\(h_k\\) given in Theorem 5.24 (p. 243)."
  },
  {
    "objectID": "chapters/ch5.html#section-11",
    "href": "chapters/ch5.html#section-11",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.44",
    "text": "5.44\nA location M-estimator may be represented as \\(T(F_n)\\), where \\(T(\\cdot)\\) satisfies \\[\\int \\psi (t- T(F)) d F(t) = 0,\\] and \\(\\psi\\) is a known differentiable function. Using implicit differentiation, show that the Gateaux derivative is \\(T(F;\\Delta) = \\frac{\\int \\psi (t- T(F))d\\Delta(t)}{\\int \\psi' (t- T(F))dF(t)}\\), then substitute \\(\\Delta(t) = \\delta_x (t) - F(t)\\) and obtain the influence function \\(h(x)\\).\nSolution:\n\\[\n\\begin{aligned}\n0=T(F;\\Delta)  &= \\frac{\\partial}{\\partial \\epsilon} T(F+\\epsilon \\Delta) \\bigg|_{\\epsilon = 0^+}=\\frac{\\partial}{\\partial \\epsilon} \\int \\psi (t- T(F(t)+\\epsilon \\Delta(t))) d \\{F(t)+\\epsilon \\Delta(t) \\}  \\bigg|_{\\epsilon = 0^+}\\\\\n&=\\int \\psi' (t- T(F(t)+\\epsilon \\Delta(t)))\\left(-T'(F(t)+\\epsilon \\Delta(t))\\right)\\Delta(t)d \\{F(t)+\\epsilon \\Delta(t) \\}  \\bigg|_{\\epsilon = 0^+}\\\\\n&~~~~~~~~~~~~~~~~~+\\int \\psi (t- T(F(t)+\\epsilon \\Delta(t)))d \\Delta(t)  \\bigg|_{\\epsilon = 0^+}\\\\\n0&=\\int -T'(F(t))\\psi' (t- T(F(t)))\\Delta(t)dF(t)  +\\int \\psi (t- T(F(t)))d \\Delta(t)\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\implies \\int T'(F)\\psi' (t- T(F))\\Delta(t)dF&= \\int \\psi (t- T(F(t)))d \\Delta(t)\\\\\n\\\\\n\\implies T'(F) = T(F; \\Delta) &= \\frac{\\int \\psi (t- T(F))d\\Delta(t)}{\\int \\psi' (t- T(F))dF(t)}\n\\end{aligned}\n\\] Substituting \\(\\Delta(t) = \\delta_x (t) - F(t)\\),\n\\[\n\\begin{aligned}\n\\frac{\\int \\psi (t- T(F))d\\Delta(t)}{\\int \\psi' (t- T(F))dF(t)} &= \\frac{\\int \\psi (t- T(F))d\\{\\delta_x (t) - F(t)\\}}{\\int \\psi' (t- T(F))dF(t)} \\\\\n&= \\frac{\\psi (x- T(F)) - \\int \\psi (t- T(F))dF(t)}{\\int \\psi' (t- T(F))dF(t)} \\\\\n&= \\frac{\\psi (x- T(F))}{\\int \\psi' (t- T(F))dF(t)} \\\\\n\\end{aligned}\n\\]\nThus, \\(h(x) = \\frac{\\psi (x- T(F))}{\\int \\psi' (t- T(F))dF(t)}\\)"
  },
  {
    "objectID": "chapters/ch5.html#section-12",
    "href": "chapters/ch5.html#section-12",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.45",
    "text": "5.45\nOne representation of a “smooth” linear combination of order statistics is \\(T(F_n)\\), where \\(T(F)= \\int_0^1 J(p) F^{-1} (p)dp\\), and \\(J\\) is a weighting function. Using the results in Example 5.5.8j (p. 254), find the influence function \\(h(x)\\).\nSolution:\n\\[\n\\begin{aligned}\nT(F)&= \\int_0^1 J(p) F^{-1} (p)dp \\\\\nT(F;\\Delta)  &= \\frac{\\partial}{\\partial \\epsilon} T(F+\\epsilon \\Delta) \\bigg|_{\\epsilon=0^+} \\\\\n&= \\frac{\\partial}{\\partial \\epsilon} \\int_0^1 J(p) (F+\\epsilon \\Delta)^{-1} (p)dp \\bigg|_{\\epsilon=0^+} \\\\\n&=\\int_0^1 \\left[\\frac{\\partial}{\\partial \\epsilon} \\{J(p)\\} (F+\\epsilon \\Delta)^{-1} (p)\\right] +\\left[ J(p) \\frac{\\partial}{\\partial \\epsilon}\\{(F+\\epsilon \\Delta)^{-1} (p)\\}\\right]dp \\bigg|_{\\epsilon=0^+} \\\\\n&=\\int_0^1  J(p) \\frac{\\partial}{\\partial \\epsilon}\\{(F+\\epsilon \\Delta)^{-1} (p)\\}dp \\bigg|_{\\epsilon=0^+} \\\\\n&=-\\int_0^1  J(p) \\frac{\\Delta (p)}{F'(p)} dp ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\text{by Example 5.5.8j results}\\\\\n\\end{aligned}\n\\] Setting \\(\\Delta(p) = \\delta_{x}(p) - F(p)\\) \\[\n\\begin{aligned}\nh(x) &= -\\int_0^1  J(p) \\frac{\\Delta (p)}{F'(p)} dp \\\\\n&= -\\int_0^1  J(p) \\frac{\\delta_{x}(p) - F(p)}{F'(p)} dp \\\\\n&= \\int_0^1  J(p) \\frac{F(p)-\\delta_{x}(p)}{F'(p)} dp\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch5.html#section-13",
    "href": "chapters/ch5.html#section-13",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.48",
    "text": "5.48\nUse Theorem 5.4 (p. 219), the univariate CLT and Theorem 5.31 (p. 256) to prove Theorem 5.7 (p. 255, the multivariate CLT). Perhaps it is easier to use an alternate statement of the conclusion of the univariate CLT than given in Theorem 5.4: \\(\\sqrt n (\\bar X - \\mu) \\overset d \\to Y\\), where \\(Y \\sim N(0,\\sigma^2)\\).\nSolution:\nCLT: \\(\\sqrt n (\\bar X - \\mu ) \\overset d \\to Y, Y \\sim N(0,\\sigma^2)\\).\nCramer-Wold Device: \\(\\mathbf Y_n \\overset d \\to \\mathbf Y\\) iff \\(\\mathbf c^T \\mathbf Y_n \\overset d \\to \\mathbf c ^T \\mathbf Y ,~ \\forall \\mathbf c \\in \\mathbb R^K\\).\nProof: Let \\(\\mathbf{X_1,\\dots, X_n}\\) be iid random k-vectors with finite mean \\(E(\\mathbf X_1)= \\boldsymbol \\mu\\) and covariance matrix \\(\\boldsymbol \\Sigma\\).\nConsider a vector \\(\\mathbf c \\in \\mathbb R ^k\\). Then \\(\\mathbf c^T \\mathbf X_n\\) is some scalar and \\[E(\\mathbf c^T \\mathbf X_n) = \\mathbf c^T E(\\mathbf X_n) = \\mathbf c^T \\boldsymbol  \\mu\\] \\[Var(\\mathbf c^T \\mathbf X_n) = \\mathbf c^T Var(\\mathbf X_n) \\mathbf c = \\mathbf c^T \\boldsymbol \\Sigma \\mathbf c\\] Thus, by the CLT (scalar case):\n\\[\n\\begin{aligned}\n\\sqrt n \\left(\\frac {\\sum_{i=1}^n \\mathbf c^T \\mathbf X_i}{n} - \\mathbf c^T \\boldsymbol  \\mu \\right) &\\overset d \\to W,~~~~W \\sim N(0,\\mathbf c^T \\boldsymbol \\Sigma \\mathbf c) \\\\\n\\implies \\mathbf c^T \\sqrt n \\left(\\frac {\\sum_{i=1}^n\\mathbf X_i}{n} - \\boldsymbol  \\mu \\right)& \\overset d \\to \\mathbf c^T Y,~~~~Y \\sim N(0, \\boldsymbol \\Sigma) &\\text{by properties of normal distribution}\\\\\n\\implies \\sqrt n \\left(\\mathbf {\\bar X} - \\boldsymbol  \\mu \\right)& \\overset d \\to \\mathbf Y,~~~~Y \\sim MVN(\\mathbf 0, \\boldsymbol \\Sigma) & \\text{by Cramer-Wold Device}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapters/ch5.html#section-14",
    "href": "chapters/ch5.html#section-14",
    "title": "Chapter 5: Large Sample Theory: The Basics",
    "section": "5.52",
    "text": "5.52\nConsider a Gauss-Markov linear model \\(\\mathbf{Y= X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\mathbf Y\\) is \\(n \\times 1\\), the components of \\(\\mathbf e = (e_1,\\dots, e_n)^T\\) are \\(iid(0,\\sigma^2)\\), and \\(\\mathbf X\\) is \\(n \\times p_n\\). Note that the number of predictors, \\(p_n\\) depends on \\(n\\). Let \\(\\mathbf{H=X(X^TX)^{-1}X^T}\\) denote the projection (or “hat”) matrix with entries \\(h_{i,j}\\). Note that \\(h_{i,j}\\) also depend on \\(n\\). We are interested in the asymptotic properties of the \\(i^{th}\\) residual, \\(Y_i- \\hat{Y}_i\\), from this regression model, for a fixed \\(i\\). Prove that if \\(n\\) and \\(p_n \\to \\infty\\) such that \\(h_{i,i} \\to c_i\\) for some \\(0\\leq c_i &lt;1\\), and \\(\\max_{\\overset{1\\leq j \\leq n}{j\\neq i}} |h_{i,j}| \\to 0\\), then \\(Y_i- \\hat{Y}_i \\overset d \\to (1-c_i)e_i+\\{(1-c_i)c_i\\}^{1/2}\\sigma Z\\), where \\(Z\\) is a standard normal random variable independent of \\(e_i\\). There is a hint in the textbook.\nSolution:\n\\[\\begin{aligned}\nY_i - \\hat{Y}_i &=  Y_i - HY_i \\\\\n&= \\mathbf{(I-H)Y}_i\\\\\n&= \\mathbf{(I-H)(X \\boldsymbol \\beta + e)}_i \\\\\n&=\\mathbf{[(IX-HX) \\boldsymbol \\beta + (I-H)e]}_i\\\\\n&=\\mathbf{[(X-X(X^TX)^{-1}X^TX) \\boldsymbol \\beta + (I-H)e]}_i \\\\\n&=\\mathbf{[(I-H)e]}_i \\\\\n&= \\left[\\mathbf e - \\begin{bmatrix} h_{1,1} & \\dots & h_{1,n} \\\\\n\\vdots &\\ddots &\\vdots \\\\\nh_{n1} & \\dots &h_{n,n}\n\\end{bmatrix} \\begin{pmatrix} e_1 \\\\ \\vdots \\\\e_n \\end{pmatrix} \\right]_i\\\\\n&= e_i - \\sum_{j=1}^n h_{i,j}e_i \\\\\n&= (1-h_{i,i})e_i - \\sum_{j=1, i \\neq j}^n h_{i,j}e_i\n\\end{aligned}\\]\nFrom Slutsky’s \\((1-h_{i,i})e_i \\overset d \\to (1-c_i) e_i\\).\nSo I need to figure out what \\(\\sum_{j=1, i \\neq j}^n h_{i,j}e_i\\) converges to. This is a double-array, so I will need to use the Lindberg Condition, which requires the variance to be finite and the mean to be zero. The mean is zero by Slutsky’s Theorem.\nSo, to show the variance is finite I can compute the variance.\nFirst, I want to show \\(\\mathbf H\\) is idempotent:\n\\[\\begin{aligned}\n\\mathbf H^2 &= \\mathbf{[X(X^TX)^{-1}X^T][X(X^TX)^{-1}X^T]} \\\\\n&=\\mathbf{X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T} \\\\\n&=\\mathbf{X(X^TX)^{-1}X^T} \\\\\n&= \\mathbf H\n\\end{aligned}\\]\nThus,\n\\[\n\\begin{aligned}\n\\mathbf H^2  &= \\begin{bmatrix} h_{1,1} & \\dots & h_{1,n} \\\\\n\\vdots &\\ddots &\\vdots \\\\\nh_{n1} & \\dots &h_{n,n}\n\\end{bmatrix} \\begin{bmatrix} h_{1,1} & \\dots & h_{1,n} \\\\\n\\vdots &\\ddots &\\vdots \\\\\nh_{n1} & \\dots &h_{n,n}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix} \\sum_{i=1}^n h_{1,i}^2 & \\sum_{i=1}^n h_{1,i}h_{2,i}  &\\dots & \\sum_{i=1}^n h_{1,i}h_{n,i} \\\\\n\\vdots &\\ddots &&\\vdots \\\\\n\\sum_{i=1}^n h_{n,i}h_{1,i}  && \\dots &\\sum_{i=1}^n h_{n,i}^2\n\\end{bmatrix} \\\\\n&= \\mathbf H =  \\begin{bmatrix} h_{1,1} & \\dots & h_{1,n} \\\\\n\\vdots &\\ddots &\\vdots \\\\\nh_{n1} & \\dots &h_{n,n}\n\\end{bmatrix}\n\\end{aligned}\n\\]\nFrom the main diagonal, it is clear that \\(\\sum_{i=1}^n{h_{j,i}^2}=h_{j,j}\\).\nThus, \\(\\sum_{i=1}^n{h_{j,i}^2} = h_{i,i}^2+\\sum_{i=1,i\\neq j}^n{h_{i,j}^2}=h_{i,i}\\) and \\(\\sum_{i=1,i\\neq j}^n{h_{i,j}^2} = h_{i,i} -h_{i,i}^2\\).\nTherefore, because of independence, \\[ \\begin{aligned}\nVar \\left( \\sum_{i=1,i\\neq j}^n h_{i,j} e_j  \\right) &= (h_{i,i} -h_{i,i}^2)Var(e_j) \\\\\n&=(h_{i,i} -h_{i,i}^2)\\sigma^2\n\\end{aligned}\n\\]\nNow that I have shown the variance is finite, and it is clear that the mean \\(E(h_{i,j} e_j) =0\\), I can move on to setting up the Lindeberg-Feller Condition.\nI will define \\(X_{j,i} = \\frac{h_{i,j}e_i}{\\sigma\\sqrt{h_{i,i}-h_{i,i}^2}}\\), (denominator is just the sqrt of variance found earlier)\n\\[\n\\begin{aligned}\n\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n  E\\left[X_{j,i}^2 I(|X_{j,i}|&gt; \\delta)\\right]\n&=\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n  E\\left[\\left(\\frac{h_{i,j}e_i}{\\sigma\\sqrt{h_{i,i}-h_{i,i}^2}}\\right)^2 I\\left(\\left|\\frac{h_{i,j}e_i}{\\sigma\\sqrt{h_{i,i}-h_{i,i}^2}}\\right| &gt; \\delta\\right)\\right] \\\\\n&=\\frac{1}{\\sigma^2(c_i-c_i^2)}\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n E\\left[h_{i,j}^2e_i^2 I\\left(\\left|h_{i,j}e_i\\right| &gt; \\delta \\left|\\sigma\\sqrt{h_{i,i}-h_{i,i}^2} \\right| \\right)\\right] \\\\\n&=\\frac{1}{\\sigma^2(c_i-c_i^2)}\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n h_{i,j}^2E\\left[e_i^2 I\\left(\\left|e_i\\right| &gt; \\frac{\\delta \\left|\\sigma\\sqrt{h_{i,i}-h_{i,i}^2} \\right|}{|h_{i,j}|}\\right)\\right] \\\\\n\\end{aligned}\n\\]\nNoting that if \\(\\max_{\\overset{1\\leq j \\leq n}{j\\neq i}} |h_{i,j}| \\to 0\\), then all \\(|h_{i,j}| \\to 0\\). Therefore, we get:\n\\[\\begin{aligned}\n&\\frac{1}{\\sigma^2(c_i-c_i^2)}\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n h_{i,j}^2E\\left[e_i^2 I\\left(\\left|e_i\\right| &gt; \\frac{\\delta \\left|\\sigma\\sqrt{c_i-c_i^2} \\right|}{|h_{i,j}|}\\right)\\right]\\\\\n&=\\frac{1}{\\sigma^2(c_i-c_i^2)}\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n h_{i,j}^2E\\left[e_i^2 I\\left(\\left|e_i\\right| &gt; \\infty \\right)\\right] \\\\\n&=\\frac{1}{\\sigma^2(c_i-c_i^2)}\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n h_{i,j}^2 \\cdot 0 \\\\\n&= 0\n\\end{aligned}\\]\nTherefore, by Lindberg-Feller, \\(\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n \\frac{h_{i,j}e_i}{\\sigma\\sqrt{h_{i,i}-h_{i,i}^2}} \\overset d \\to N(0,1)\\),\nAnd so, \\(\\lim_{n \\to \\infty} \\sum_{j=1, i \\neq j}^n h_{i,j}e_i \\overset d \\to N(0,(c_i -c_i^2)\\sigma^2)\\)\nThus, \\(Y_i- \\hat{Y}_i \\overset d \\to (1-c_i)e_i+\\{(1-c_i)c_i\\}^{1/2}\\sigma Z\\)."
  },
  {
    "objectID": "chapters/ch6.html#section",
    "href": "chapters/ch6.html#section",
    "title": "Chapter 6: Large Sample Results for Likelihood-Based Methods",
    "section": "6.2",
    "text": "6.2\nConsider the density \\[f(y;\\sigma) = \\frac{2y}{\\sigma^2}\\exp\\left(-\\frac {y^2}{\\sigma^2}\\right) I(y&gt;0,\\sigma&gt;0)\\] Verify that the regularity conditions 1. to 5. of Theorem 6.6 (p. 284) hold for the asymptotic normality of the maximum likelihood estimator of \\(\\sigma\\).\nSolution:\nCondition 1: Identifiability \\(\\theta_1 \\neq \\theta_2 \\implies \\exists y ~s.t.~ F(y;\\theta_1) \\neq F(y;\\theta_2)\\).\n\\[\\begin{aligned}\nF(y;\\sigma) &= \\int_{-\\infty}^y\\frac{2t}{\\sigma^2}\\exp\\left(-\\frac {t^2}{\\sigma^2}\\right) I(t&gt;0,\\sigma&gt;0)dt & \\substack{let~ u = \\frac {t^2}{\\sigma^2} \\\\du = \\frac {2t}{\\sigma^2}dt}\\\\\n&= \\int_{0}^{y^2/\\sigma^2} e^{-u} \\,I(u,\\sigma&gt;0) du \\\\\n&= [-e^{y^2/\\sigma^2}+e^0]  \\,I(y,\\sigma&gt;0) \\\\\n&= [1-e^{y^2/\\sigma^2}]I(y,\\sigma&gt;0) \\\\\n\\end{aligned}\\]\nThus, if \\(\\sigma_1 \\neq \\sigma_2\\), then\n\\[\\begin{aligned}\nF(y; \\sigma_1) &= [1-e^{-y^2/\\sigma_1^2}]I(y,\\sigma_1&gt;0) \\\\\nF(y; \\sigma_2) &= [1-e^{-y^2/\\sigma_2^2}]I(y,\\sigma_2&gt;0) \\\\\nF(y; \\sigma_1) -F(y; \\sigma_2)&=  e^{y^2/\\sigma_2^2}I(y,\\sigma_2&gt;0)-e^{y^2/\\sigma_1^2}I(y,\\sigma_1&gt;0) \\\\\n&\\neq 0  \\\\\n\\implies F(y; \\sigma_1) &\\neq F(y; \\sigma_2)\n\\end{aligned}\n\\]\nCondition 2: \\(\\forall \\theta \\in \\Theta, F(y;\\theta)\\) has the same support not depending on \\(\\theta\\).\nThis is true as the support is all \\((y, \\sigma) \\in \\mathbb R^+ \\times \\mathbb R^+\\), which is independent of \\(\\sigma\\).\nCondition 3: \\(\\forall \\theta \\in \\Theta, F(y;\\theta)\\) the first three partial derivatives of \\(\\log f(y;\\theta)\\) with respect to \\(\\theta\\) exist for y is the support of \\(F(y;\\theta)\\)\n\\[\\begin{aligned}\nf(y;\\sigma) &= \\frac{2y}{\\sigma^2}\\exp\\left(-\\frac {y^2}{\\sigma^2}\\right) \\\\\n\\log f(y;\\sigma) &= \\log (2y)- 2\\log(\\sigma)  -y^2\\sigma^{-2}  \\\\\n\\frac{\\partial \\log f(y;\\sigma)}{\\partial \\sigma} &= - 2 \\sigma^{-1} +2y^2\\sigma^{-3} \\\\\n\\frac{\\partial^2 \\log f(y;\\sigma)}{\\partial \\sigma^2} &= 2 \\sigma^{-2} -6y^2\\sigma^{-4} \\\\\n\\frac{\\partial^3 \\log f(y;\\sigma)}{\\partial \\sigma^3} &= -4 \\sigma^{-3} +24y^2\\sigma^{-5} \\\\\n\\end{aligned}\n\\]\nThe first three partial derivatives are defined for all strictly positive values of \\(y\\) and \\(\\sigma\\), which is the support of \\(F(y;\\sigma)\\).\nCondition 4: \\(\\forall \\theta_0 \\in \\Theta\\), there exist a function \\(g(y)\\) such that \\(\\forall \\theta\\) in a neighborhood of \\(\\theta\\), \\(|\\partial^3 \\log f(y;\\theta)/\\partial \\theta^3| \\leq g(y)\\) for all \\(y\\) where \\(\\int g(y)dF(y;\\theta_0) &lt; \\infty\\).\nLet \\(g(y)= |\\partial^3 \\log f(y;\\sigma)/\\partial \\sigma^3| = |-4 \\sigma^{-3} +24y^2\\sigma^{-5}|\\).\nThen,\n\\[\\begin{aligned}\n|-4 \\sigma_0^{-3} +24y^2\\sigma_0^{-5}| &\\leq g(y) & \\text{by def } g(y) \\\\\n\\int g(y)dF(y;\\theta_0) &= \\int |-4 \\sigma_0^{-3} +24y^2\\sigma_0^{-5}|dF(y; \\sigma_0) \\\\\n&\\leq \\int [|4 \\sigma_0^{-3}| +|24y^2\\sigma_0^{-5}|]f(y;\\sigma_0)dy & \\text{by Triangle Ineq.} \\\\\n&\\leq \\int [4 \\sigma_0^{-3} +24y^2\\sigma_0^{-5}]f(y;\\sigma_0)dy & \\text{by Triangle Ineq.} \\\\\n&\\leq E(4 \\sigma_0^{-3}) + E(24\\sigma_0^{-5}Y^2) \\\\\n&\\leq 4 \\sigma_0^{-3} + 24\\sigma_0^{-5}E(Y^2)\\\\\n&\\leq 4 \\sigma_0^{-3} + 24\\sigma_0^{-5}\\sigma_0^2 \\\\\n&\\leq 28\\sigma_0^{-3} \\\\\n&&lt; \\infty\n\\end{aligned}\\]\nCondition 5: \\(\\forall \\theta \\in \\Theta, E[\\partial \\log f(Y_1;\\theta)/\\partial \\theta] = 0, I(\\theta)=E([\\partial \\log f(Y_1;\\theta)/\\partial \\theta]^2) = E[-\\partial^2 \\log f(Y_1;\\theta)/\\partial \\theta^2]\\)\n\\[\\begin{aligned}\nE[\\partial \\log f(Y;\\sigma)/\\partial \\sigma] &= E(- 2 \\sigma^{-1} +2Y^2\\sigma^{-3})\\\\\n&= - 2 \\sigma^{-1} +2E(Y^2)\\sigma^{-3} &\\substack{X= Y^2 \\implies E(Y^2)= E(X) = \\sigma^2\\\\ X \\sim Exponential(\\sigma^2)}\\\\\n&= - 2 \\sigma^{-1} +2\\sigma^{-1}\\\\\n&= 0\n\\end{aligned}\\]\n\\[\\begin{aligned}\nI(\\sigma) &= E([\\partial \\log f(Y;\\theta)/\\partial \\theta]^2) \\\\\n&= E([- 2 \\sigma^{-1} +2Y^2\\sigma^{-3}]^2)\\\\\n&= E(4\\sigma^{-2} -8Y^2\\sigma^{-4} +4Y^4\\sigma^{-6}) \\\\\n&= -4\\sigma^{-2} +4E(Y^4)\\sigma^{-6} & \\substack{X= Y^2 \\implies E(Y^4)= E(X^2) = Var(X) + [E(X)]^2\\\\ X \\sim Exponential(\\sigma^2) \\implies E(X^4) = (\\sigma^2)^2+(\\sigma^2)^2=2\\sigma^4}\\\\\n&= -4\\sigma^{-2} +4(2\\sigma^{4})\\sigma^{-6} \\\\\n&= -4\\sigma^{-2} +8\\sigma^{-2}\\\\\n&= 4\\sigma^{-2} \\\\\n&= 6\\sigma^{-2}-2\\sigma^{-2} \\\\\n&= E[6\\sigma^2\\sigma^{-4}-2\\sigma^{-2}]\\\\\n&= E[-(2\\sigma^{-2}-6Y^2\\sigma^{-4})]\\\\\n&= E[-\\partial^2 \\log f(Y_1;\\theta)/\\partial \\theta^2]\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch6.html#section-1",
    "href": "chapters/ch6.html#section-1",
    "title": "Chapter 6: Large Sample Results for Likelihood-Based Methods",
    "section": "6.3",
    "text": "6.3\nIn Condition 5 of Theorem 6.6 (p. 284) we have the assumption that \\(E[\\partial \\log f(Y_1;\\theta)/\\partial \\theta] = 0\\). For continuous distributions this mean zero assumption follows if \\[\\int\\left[\\frac \\partial {\\partial \\theta} f(y;\\theta)\\right] dy=\\frac \\partial {\\partial \\theta}\\int\\left[ f(y;\\theta)dy\\right] \\] because this latter integral is one by the definition of a density function. The typical proof that this interchange of differentiability and integration is allowed assumes that for each \\(\\theta_0 \\in \\Theta\\), there is a bounding function \\(g_1(y)\\) (possibly depending on \\(\\theta_0\\)) and a neighborhood of \\(\\theta_0\\) such that for all \\(y\\) and for all \\(\\theta\\) in the neighborhood of \\(|\\partial f(y;\\theta)/\\partial \\theta| \\leq g_1(y)\\) and \\(\\int g_1(y)dy &lt; \\infty\\). Use the dominated convergence theorem to show that this condition allows the above interchange.\nSolution:\nI begin by assuming what was stated in “the typical proof).” That for each \\(\\theta_0 \\in \\Theta\\), there is a bounding function \\(g_1(y)\\) (possibly depending on \\(\\theta_0\\)) and a neighborhood of \\(\\theta_0\\) such that for all \\(y\\) and for all \\(\\theta\\) in the neighborhood of \\(|\\partial f(y;\\theta)/\\partial \\theta| \\leq g_1(y)\\).\nNote that: \\[\\begin{aligned}\n\\frac{f(y;\\theta+h_n) - f(y;\\theta)}{h_n}&= \\frac{\\partial f(y;\\theta)}{\\partial \\theta}\\bigg|_{\\theta_* \\in (\\theta, \\theta + h_n)} &\\text{ by mean value theorem}\\\\\n&\\leq g_1(y) & since~|\\partial f(y;\\theta)/\\partial \\theta| &\\leq g_1(y)\n\\end{aligned}\\]\nThus, \\(\\frac{f(y;\\theta+h_n) - f(y;\\theta)}{h_n} \\to \\frac{f(y;\\theta+h) - f(y;\\theta)}{h}\\) satisfies conditions for the dominated convergence theorem.\nLet \\(h_n\\) be a sequence converging to 0:\n\\[\\begin{aligned}\n\\frac \\partial {\\partial \\theta}\\int\\left[ f(y;\\theta)dy\\right]\n&= \\lim_{h_n \\to 0} \\frac{\\int f(y;\\theta+h_n)dy - \\int f(y;\\theta)dy}{h_n} \\\\\n&= \\lim_{h_n \\to 0} \\int \\frac{f(y;\\theta+h_n) - f(y;\\theta)}{h_n} dy \\\\\n&= \\lim_{n \\to \\infty} \\int \\frac{f(y;\\theta+h_n) - f(y;\\theta)}{h_n} dy \\\\\n&=\\int\\lim_{n \\to \\infty}  \\frac{f(y;\\theta+h_n) - f(y;\\theta)}{h_n} dy &\\text{dominated convergence theorem} \\\\\n&=\\int\\lim_{h_n \\to 0}  \\frac{f(y;\\theta+h_n) - f(y;\\theta)}{h_n} dy \\\\\n&=\\int\\left[\\frac \\partial {\\partial \\theta} f(y;\\theta)\\right] dy\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch6.html#section-2",
    "href": "chapters/ch6.html#section-2",
    "title": "Chapter 6: Large Sample Results for Likelihood-Based Methods",
    "section": "6.6",
    "text": "6.6\nThe proof of the asymptotic normality of the maximum likelihood estimator does not use an approximation by averages. Show, however, that one can extend the proof to obtain an approximation by averages result for the maximum likelihood estimator. Hint: add and subtract the numerator of (6.10, p. 285) divided by the probability limit of the denominator of (6.10, p. 285).\nSolution:\nFrom (6.10, p.285), where \\(\\hat \\theta\\) is the MLE and \\(\\theta_0\\) is the true value,\n\\[\\begin{aligned}\n\\sqrt n (\\hat \\theta -\\theta) &= \\frac{-S(\\theta)/\\sqrt n}{\\frac 1 n S'(\\theta) + \\frac 1 {2n} S''(\\hat \\theta^*)( \\hat \\theta - \\theta)} \\\\\n&= \\frac{-S(\\theta)/\\sqrt n}{\\frac 1 n S'(\\theta) + \\frac 1 {2n} S''(\\hat \\theta^*)( \\hat \\theta - \\theta)} +\\frac{S(\\theta)/\\sqrt n-S(\\theta)/\\sqrt n}{-I(\\theta)} \\\\\n\\implies \\hat \\theta - \\theta&= \\frac{-S(\\theta)/ n}{\\frac 1 n S'(\\theta) + \\frac 1 {2n} S''(\\hat \\theta^*)( \\hat \\theta - \\theta)} +\\frac{S(\\theta)/n-S(\\theta)/ n}{-I(\\theta)} \\\\\n&= \\frac{-S(\\theta)/ n}{-I(\\theta)} + R_n ~~~~ where~ R_n =\\frac{-S(\\theta)/ n}{\\frac 1 n S'(\\theta) + \\frac 1 {2n} S''(\\hat \\theta^*)( \\hat \\theta - \\theta)}+\\frac{S(\\theta)/n}{-I(\\theta)}\\\\\n&= \\frac 1 n \\sum_{i=1}^n \\frac{-\\partial \\log f(Y_i; \\theta)/\\partial \\theta}{-I(\\theta)} + R_n \\\\\n&= \\frac 1 n \\sum_{i=1}^n \\frac{\\partial \\log f(Y_i; \\theta)/\\partial \\theta}{I(\\theta)} + R_n\n\\end{aligned}\\]\nNote \\(\\sqrt n R_n \\to 0\\) since the denominators probability limits go to the negative information (see textbook).\nAlso note \\(E\\left[\\frac{\\partial \\log f(Y_i; \\theta)/\\partial \\theta}{I(\\theta)}\\right] = 0\\) since the expected value of a score is zero.\nLastly, \\[\\begin{aligned}\nVar\\left[\\frac{\\partial \\log f(Y_i; \\theta)/\\partial \\theta}{I(\\theta)}\\right]\n&=\\frac{E[(\\partial \\log f(Y_i; \\theta)/\\partial \\theta)^2]}{I^2(\\theta)}\\\\\n&= \\frac{I(\\theta)}{I^2(\\theta)} \\\\\n&= I^{-1}(\\theta)\n\\end{aligned}\n\\]\nThus, the approximation by averages of \\(\\hat \\theta_{MLE}\\) is: \\[\\hat \\theta -\\theta =\\frac 1 n \\sum_{i=1}^n \\frac{\\partial \\log f(Y_i; \\theta)/\\partial \\theta}{I(\\theta)} + R_n\\] And \\(\\sqrt n (\\hat \\theta -\\theta) \\overset d \\to N(0, I^{-1}(\\theta))\\) by Theorem 5.23 (p.242)."
  },
  {
    "objectID": "chapters/ch7.html#section",
    "href": "chapters/ch7.html#section",
    "title": "Chapter 7: M-Estimation (Estimating Equations)",
    "section": "7.3",
    "text": "7.3\nSuppose that \\(Y_1, \\dots , Y_n\\) are iid from a gamma\\((\\alpha, \\beta)\\) distribution.\n\na.\nOne version of the method of moments is to set \\(\\bar Y\\) equal to \\(E(Y_1) = \\alpha \\beta\\) and \\(n^{-1} \\sum_{i=1}^n Y_i^2\\) equal to \\(E(Y_1^2) = \\alpha \\beta^2 + (\\alpha \\beta)^2\\) and solve for the estimators. Use Maple (at least it’s much easier if you do) to find \\(V= A^{-1} B \\{A^{-1}\\}^T\\). Here it helps to know that \\(E(Y_1^3) = \\alpha(1+\\alpha )(2+\\alpha ) \\beta^3\\) and \\(E(Y_1^4) = \\alpha(1+\\alpha)(2+\\alpha)(3+\\alpha)\\beta^4\\). Show your derivation of A and B and attach Maple output.\nSolution:\n\\[\\begin{aligned}\n\\theta &= (\\alpha, \\beta)^T ~~~~~\n\\psi(Y_i, \\theta) &=  \\begin{pmatrix} Y_i - \\alpha \\beta \\\\ Y_i^2 - \\alpha \\beta^2 - (\\alpha \\beta)^2 \\end{pmatrix} ~~~~~~~~~\n\\frac{\\partial \\psi(Y_i, \\theta)}{\\partial \\theta} &=  \\begin{bmatrix} -  \\beta & - \\alpha \\\\\n- \\beta^2 - 2\\alpha \\beta^2 & - 2\\alpha \\beta - 2\\alpha^2 \\beta \\end{bmatrix}\n\\end{aligned}\n\\] \\[ \\begin{aligned}\nA &= E[- \\psi'(Y_i, \\theta)] =  \\begin{bmatrix}   \\beta &  \\alpha \\\\\n\\beta^2 + 2\\alpha \\beta^2 &  2\\alpha \\beta + 2\\alpha^2 \\beta \\end{bmatrix} \\\\\nB &= E[\\psi (Y_i, \\theta)\\psi (Y_i, \\theta)^T] =  E\\left[\\begin{pmatrix} Y_i - \\alpha \\beta \\\\ Y_i^2 - \\alpha \\beta^2 - (\\alpha \\beta)^2 \\end{pmatrix} \\begin{pmatrix} Y_i - \\alpha \\beta & Y_i^2 - \\alpha \\beta^2 - (\\alpha \\beta)^2 \\end{pmatrix} \\right]\\\\\n&= E\\begin{bmatrix} (Y_i -\\alpha \\beta)^2 & (Y_i -\\alpha \\beta)(Y_i^2 -\\alpha \\beta^2 -\\alpha^2\\beta^2) \\\\\n(Y_i -\\alpha \\beta)(Y_i^2 -\\alpha \\beta^2 -\\alpha^2\\beta^2) & (Y_i^2 -\\alpha \\beta^2 -\\alpha^2\\beta^2)^2\\end{bmatrix} \\\\\n&= E\\begin{bmatrix} Y_i^2 - 2 \\alpha \\beta Y_i +\\alpha^2 \\beta^2 &\nY_i^3-\\alpha \\beta Y_i^2 - \\alpha^2 \\beta^2 Y_i -\\alpha \\beta^2 Y_i + \\alpha^3 \\beta^3 + \\alpha^2\\beta^3\\\\\nY_i^3-\\alpha \\beta Y_i^2 - \\alpha^2 \\beta^2 Y_i -\\alpha \\beta^2 Y_i + \\alpha^3 \\beta^3 + \\alpha^2\\beta^3 &\nY_i^4 -2 \\alpha^2 \\beta^2 Y_i^2 - 2 \\alpha \\beta^2 Y_i^2 + \\alpha^4 \\beta^4 +\\alpha^2 \\beta^4 + 2 \\alpha^3 \\beta^4\\end{bmatrix} \\\\\n&= \\begin{bmatrix} (\\alpha \\beta^2 + \\alpha^2 \\beta^2) - 2 \\alpha \\beta \\alpha \\beta  +\\alpha^2 \\beta^2 &\n\\alpha(1+\\alpha)(2+\\alpha)\\beta^3-\\alpha \\beta (\\alpha \\beta^2 + \\alpha^2 \\beta^2) - \\alpha^2 \\beta^2 \\alpha \\beta  -\\alpha \\beta^2 \\alpha \\beta  + \\alpha^3 \\beta^3 + \\alpha^2\\beta^3\\\\\n\\dots Symmetric \\dots &\nE(Y_i^4) -2 \\alpha^2 \\beta^2 (\\alpha \\beta^2 + \\alpha^2 \\beta^2) - 2 \\alpha \\beta^2 (\\alpha \\beta^2 + \\alpha^2 \\beta^2) + \\alpha^4 \\beta^4 +\\alpha^2 \\beta^4 + 2 \\alpha^3 \\beta^4\\end{bmatrix} \\\\\n&= \\begin{bmatrix} \\alpha \\beta^2 &\n\\alpha(1+\\alpha)(2+\\alpha)\\beta^3-\\alpha^2 \\beta^3 - \\alpha^3 \\beta^3 \\\\\n\\alpha(1+\\alpha)(2+\\alpha)\\beta^3-\\alpha^2 \\beta^3 - \\alpha^3 \\beta^3 &\n\\alpha(1+\\alpha)(2+\\alpha)(3+\\alpha)\\beta^4  -2 \\alpha^3 \\beta^4 - \\alpha^4 \\beta^4 -  \\alpha^2 \\beta^4 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} \\alpha \\beta^2 &\n(\\alpha^3+3\\alpha^2+2\\alpha)\\beta^3-\\alpha^2 \\beta^3 - \\alpha^3 \\beta^3 \\\\\n(\\alpha^3+3\\alpha^2+2\\alpha)\\beta^3-\\alpha^2 \\beta^3 - \\alpha^3 \\beta^3 &\n(\\alpha^4 + 6 \\alpha^3+ 11 \\alpha^2 + 6\\alpha)\\beta^4 -2 \\alpha^3 \\beta^4 - \\alpha^4 \\beta^4 -  \\alpha^2 \\beta^4 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} \\alpha \\beta^2 &\n2(\\alpha^2+\\alpha)\\beta^3 \\\\\n2(\\alpha^2+\\alpha)\\beta^3 &\n(4 \\alpha^3+ 10 \\alpha^2 + 6\\alpha)\\beta^4  \\end{bmatrix} \\\\\nV &= A^{-1} B \\{ A^{-1} \\}^T ~~~~ Using~Microsoft~Mathematics \\\\\n&= \\begin{bmatrix} 2(\\alpha^2 +\\alpha) & -2(1+ \\alpha) \\beta \\\\\n-2(1+ \\alpha) \\beta & \\left(2 + \\frac 3 \\alpha \\right)\\beta^2 \\end{bmatrix}\n\\end{aligned}\\]\n\n\nb.\nThe second version of the method of moments (and perhaps the easier method) is to set \\(\\bar Y\\) equal to \\(\\alpha \\beta\\) and \\(s^2\\) equal to \\(var(Y_1) = \\alpha \\beta^2\\) and solve for the estimators. You could use either the “\\(n-1\\)” or “\\(n\\)” version of \\(s^2\\), but here we want to use the “\\(n\\)” version in order to fit into the M-estimator theory. Compute \\(V\\) as in a) except that the second component of the function is different from a) (but V should be same). Here it helps to know that \\(\\mu_3= 2\\alpha\\beta^3\\) and \\(\\mu_4 = 3[\\alpha^2+2\\alpha]\\beta^4\\).\nSolution:\n\\[\\begin{aligned}\n\\psi(Y_i, \\theta) &=  \\begin{pmatrix} Y_i - \\alpha \\beta \\\\ (Y_i - \\alpha \\beta)^2 - \\alpha \\beta^2\\end{pmatrix}\n~~~~~\\frac{\\partial \\psi(Y_i, \\theta)}{\\partial \\theta} =\n\\begin{bmatrix}  -  \\beta  & - \\alpha\\\\\n-2 \\beta (Y_i - \\alpha \\beta) - \\beta^2 & -2\\alpha(Y_i - \\alpha \\beta) -2\\alpha \\beta\\end{bmatrix}\n\\end{aligned}\\]\n\\[ \\begin{aligned}\nA &= E[- \\psi'(Y_i, \\theta)] = E \\begin{bmatrix}    \\beta  &  \\alpha\\\\\n2 \\beta (Y_i - \\alpha \\beta) + \\beta^2 & 2\\alpha(Y_i - \\alpha \\beta) +2\\alpha \\beta\\end{bmatrix} \\\\\n&=\\begin{bmatrix}    \\beta  &  \\alpha\\\\\n\\beta^2 &  2\\alpha \\beta\\end{bmatrix} \\\\\nB &= E[\\psi (Y_i, \\theta)\\psi (Y_i, \\theta)^T] =  E\\left[\\begin{pmatrix} Y_i - \\alpha \\beta \\\\ (Y_i - \\alpha \\beta)^2 - \\alpha \\beta^2 \\end{pmatrix} \\begin{pmatrix} Y_i - \\alpha \\beta & (Y_i - \\alpha \\beta)^2 - \\alpha \\beta^2 \\end{pmatrix} \\right]\\\\\n&=  E \\begin{bmatrix} (Y_i - \\alpha \\beta)^2 & (Y_i - \\alpha \\beta)[(Y_i-\\alpha \\beta)^2 - \\alpha \\beta^2] \\\\\n(Y_i - \\alpha \\beta)[(Y_i-\\alpha \\beta)^2 - \\alpha \\beta^2] & [(Y_i-\\alpha \\beta)^2 - \\alpha \\beta^2]^2\\end{bmatrix}\\\\\n&=   \\begin{bmatrix} Var(Y_i) & \\mu_3 \\\\\n\\mu_3 & \\mu_4 -2 \\alpha \\beta^2 Var(Y_i) + \\alpha^2\\beta^4\\end{bmatrix}\n=   \\begin{bmatrix} \\alpha \\beta^2 & 2 \\alpha \\beta^3 \\\\\n2 \\alpha \\beta^3  & 3(\\alpha^2+2\\alpha)\\beta^4-2 \\alpha \\beta^2(\\alpha \\beta^2) + \\alpha^2\\beta^4\\end{bmatrix}\\\\\n&=   \\begin{bmatrix} \\alpha \\beta^2 & 2 \\alpha \\beta^3 \\\\\n2 \\alpha \\beta^3  & (2\\alpha^2+6\\alpha)\\beta^4 \\end{bmatrix}\\\\\nV &= A^{-1} B \\{ A^{-1} \\}^T ~~~~ Using~Microsoft~Mathematics \\\\\n&= \\begin{bmatrix} 2(\\alpha^2 +\\alpha) & -2(1+ \\alpha) \\beta \\\\\n-2(1+ \\alpha) \\beta & \\left(2 + \\frac 3 \\alpha \\right)\\beta^2 \\end{bmatrix}\n\\end{aligned}\\]\n\n\nc.\nThe asymptotic variance of the MLEs for \\(\\alpha\\) and \\(\\beta\\) are \\(Avar(\\hat \\alpha_{MLE})= 1.55/n\\) for \\(\\alpha = 1.0\\) and \\(Avar(\\hat \\alpha_{MLE})= 6.90/n\\) for \\(\\alpha = 2.0\\). Similarly, \\(Avar(\\hat \\beta_{MLE})= 2.55\\beta^2/n\\) for \\(\\alpha = 1.0\\) and \\(Avar(\\hat \\beta_{MLE})= 3.45\\beta^2/n\\) for \\(\\alpha = 2.0\\). Now calculate the asymptotic relative efficiencies of the MLEs to the method of moment estimators for \\(\\alpha = 1.0\\) and \\(\\alpha = 2.0\\) using results from a.\nSolution:\n\\[\\begin{aligned}\nAvar(\\hat \\alpha_{MOM}) |_{\\alpha = 1.0} &= 2(\\alpha^2 +\\alpha)/n \\big|_{\\alpha = 1.0} = 4/n \\\\\nAvar(\\hat \\alpha_{MOM}) |_{\\alpha = 2.0} &= 2(\\alpha^2 +\\alpha)/n \\big|_{\\alpha = 2.0} = 12/n \\\\\nAvar(\\hat \\beta_{MOM}) |_{\\alpha = 1.0} &=\\left(2 + \\frac 3 \\alpha \\right)\\beta^2 \\big|_{\\alpha = 1.0} = 5 \\beta^2 /n \\\\\nAvar(\\hat \\beta_{MOM}) |_{\\alpha = 2.0} &= \\left(2 + \\frac 3 \\alpha \\right)\\beta^2 \\big|_{\\alpha = 2.0} = 7 \\beta^2/(2n) \\\\ \\\\\nARE(\\hat \\alpha_{MLE},\\hat \\alpha_{MOM})|_{\\alpha = 1.0} &= 1.55/4 = 0.3875 \\\\\nARE(\\hat \\alpha_{MLE},\\hat \\alpha_{MOM})|_{\\alpha = 2.0} &= 6.9/12 = 0.575 \\\\\nARE(\\hat \\beta_{MLE},\\hat \\beta_{MOM})|_{\\alpha = 1.0} &= 2.55/(5) = 0.52 \\\\\nARE(\\hat \\beta_{MLE},\\hat \\beta_{MOM})|_{\\alpha = 2.0} &= 3.45/(7/2) \\approx 0.986 \\\\\n\\end{aligned}\\]\nAs expected, the MLE is more efficient in all cases."
  },
  {
    "objectID": "chapters/ch7.html#section-1",
    "href": "chapters/ch7.html#section-1",
    "title": "Chapter 7: M-Estimation (Estimating Equations)",
    "section": "7.4",
    "text": "7.4\nSuppose that \\(Y_1, \\dots, Y_n\\) are iid and \\(\\boldsymbol{\\hat \\theta}\\) satisfies \\(\\sum_{i=1}^n \\boldsymbol \\psi (Y_i, \\boldsymbol{\\hat \\theta}) = \\mathbf c_n\\) where we assume:\n\n\\(\\boldsymbol{\\hat \\theta} \\overset p \\to \\boldsymbol \\theta_0\\)\n\\(\\mathbf c_n / \\sqrt n \\overset p \\to 0\\)\nThe remainder term \\(R_n\\) from the expansion \\[G_n (\\boldsymbol{\\hat \\theta}) = n^{-1} \\sum_{i=1}^n \\boldsymbol \\psi (Y_i, \\boldsymbol{\\hat \\theta}) = G_n(\\boldsymbol \\theta_0) +G_n'(\\boldsymbol \\theta_0)(\\boldsymbol{\\hat \\theta} -\\boldsymbol \\theta_0) + \\mathbf R_n\\] satisfies \\(\\sqrt n \\mathbf R_n \\overset p \\to 0\\).\n\nShow that \\(\\boldsymbol{\\hat \\theta}\\) is \\(AN(\\boldsymbol \\theta_0, V(\\boldsymbol \\theta_0)/n)\\), i.e., the same result as for the usual case when \\(\\mathbf c_n = 0\\).\nSolution:\nLet \\(G_n (\\boldsymbol{\\hat \\theta}) = n^{-1} \\sum_{i=1}^n \\boldsymbol \\psi (Y_i, \\boldsymbol{\\hat \\theta}) = n^{-1} \\mathbf c_n\\).\nExpanding using Taylor’s Theorem about \\(\\boldsymbol \\theta_0\\) \\[\\begin{aligned}\nn^{-1} \\mathbf c_n &= G_n (\\boldsymbol{\\hat \\theta}) =G_n(\\boldsymbol \\theta_0) +G_n'(\\boldsymbol \\theta_0)(\\boldsymbol{\\hat \\theta} -\\boldsymbol \\theta_0) + \\mathbf R_n \\\\\n\\implies \\frac {\\mathbf c_n} {\\sqrt{n}} &= \\sqrt n G_n(\\boldsymbol \\theta_0) + \\sqrt n G_n'(\\boldsymbol \\theta_0)(\\boldsymbol{\\hat \\theta} -\\boldsymbol \\theta_0) + \\sqrt n \\mathbf R_n\n\\end{aligned}\\] \\[\\begin{aligned}\n\\implies  \\sqrt n G_n(\\boldsymbol \\theta_0) + \\sqrt n G_n'(\\boldsymbol \\theta_0)(\\boldsymbol{\\hat \\theta} -\\boldsymbol \\theta_0) &\\overset p \\to 0 & \\text{by ii and iii}\\\\\n\\implies \\sqrt n G_n'(\\boldsymbol \\theta_0)(\\boldsymbol{\\hat \\theta} -\\boldsymbol \\theta_0) \\overset p \\to -\\sqrt n G_n(\\boldsymbol \\theta_0) \\\\\n\\implies \\sqrt n (\\boldsymbol{\\hat \\theta} -\\boldsymbol \\theta_0) \\overset p \\to -G_n'^{-1}(\\boldsymbol \\theta_0)\\sqrt n   G_n(\\boldsymbol \\theta_0)\n\\end{aligned}\\]\nNote that, by CLT \\[\\begin{aligned}\n\\sqrt n   [G_n(\\boldsymbol \\theta_0)- E[G_n(\\boldsymbol \\theta_0)]]  &\\overset d \\to N(0, E(G_n(\\boldsymbol \\theta_0)G_n(\\boldsymbol \\theta_0)^T)) \\\\\n\\implies \\sqrt n   \\left[G_n(\\boldsymbol \\theta_0)- E[n^{-1} \\sum_{i=1}^n \\boldsymbol \\psi (Y_i, \\boldsymbol \\theta_0)]\\right]  &\\overset d \\to N\\left(0, E\\left[n^{-2} \\sum_{i=1}^n \\boldsymbol \\psi (Y_i, \\boldsymbol \\theta_0)\\boldsymbol \\sum_{i=1}^n \\psi^T (Y_i, \\boldsymbol \\theta_0)\\right] \\right) \\\\\n\\implies \\sqrt n   \\left[G_n(\\boldsymbol \\theta_0)- n^{-1} \\mathbf c_n\\right]  &\\overset d \\to N\\left(0, E\\left[ \\boldsymbol \\psi (Y_1, \\boldsymbol \\theta_0)\\boldsymbol \\psi^T (Y_2, \\boldsymbol \\theta_0)\\right] \\right) \\\\\n\\implies \\sqrt n   G_n(\\boldsymbol \\theta_0) &\\overset d \\to N\\left(0, \\mathbf B(\\boldsymbol \\theta_0) \\right)\n\\end{aligned}\\]\nAlso, by SLLN \\[\\begin{aligned}\nG_n'(\\boldsymbol \\theta_0) &=  n^{-1} \\sum_{i=1}^n \\boldsymbol \\psi' (Y_i, \\boldsymbol{\\hat \\theta}) \\overset p \\to E(\\psi' (Y_1, \\boldsymbol \\theta_0)) = - A(\\boldsymbol \\theta_0)\n\\end{aligned}\\]\nThus, by Slutsky’s Theorem,\n\\[\\begin{aligned}\n\\implies \\sqrt n (\\boldsymbol{\\hat \\theta} -\\boldsymbol \\theta_0) &\\overset p \\to -G_n'^{-1}(\\boldsymbol \\theta_0)\\sqrt n   G_n(\\boldsymbol \\theta_0) \\\\\n&\\overset d \\to N\\left(0, \\mathbf A^{-1}(\\boldsymbol \\theta_0) \\mathbf B(\\boldsymbol \\theta_0) \\left\\{\\mathbf A^{-1}(\\boldsymbol \\theta_0) \\right\\}^T \\right) \\\\\n&\\overset d \\to N\\left(0, \\mathbf V(\\boldsymbol \\theta_0) \\right) \\\\\n\\implies \\boldsymbol{\\hat \\theta} ~is ~ & ~~ AN\\left( \\boldsymbol \\theta_0, \\frac{\\mathbf V(\\boldsymbol \\theta_0)}{n} \\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch7.html#section-2",
    "href": "chapters/ch7.html#section-2",
    "title": "Chapter 7: M-Estimation (Estimating Equations)",
    "section": "7.6",
    "text": "7.6\n(Delta Theorem via M-estimation). Suppose that \\(\\boldsymbol{\\hat \\theta}\\) is a b-dimensional M-estimator with defining function \\(\\boldsymbol \\psi(y, \\boldsymbol \\theta)\\) such that the usual quantities \\(\\mathbf A\\) and \\(\\mathbf B\\) exist. Here we want to essentially reproduce Theorem 5.19 (p. 238) for \\(g(\\boldsymbol{\\hat \\theta})\\), where \\(g\\) satisfies the assumptions of Theorem 5.19 and \\(b_n^2 \\boldsymbol \\Sigma = n^{-1} \\mathbf V(\\boldsymbol \\theta)\\), where \\(\\mathbf V(\\boldsymbol \\theta )= \\mathbf A(\\boldsymbol \\theta)^{-1} \\mathbf B(\\boldsymbol \\theta)\\{\\mathbf A(\\boldsymbol \\theta)^{-1}\\}^T\\) So add the \\(\\boldsymbol \\psi\\) function \\(g(\\boldsymbol \\theta) - \\theta_{b+1}\\) to \\(\\boldsymbol \\psi(y, \\boldsymbol \\theta)\\), compute the relevant matrices, say \\(\\mathbf {A}^* , \\mathbf{B}^*,\\) and \\(\\mathbf{V}^*\\), and show that the last diagonal element of \\(\\mathbf{V}^*\\) is \\(g'(\\boldsymbol \\theta) \\mathbf V(\\boldsymbol \\theta)g'(\\boldsymbol \\theta)^T\\).\nSolution:\nDefine \\(\\theta^* = E[g(\\boldsymbol \\theta)], \\boldsymbol \\beta = (\\boldsymbol \\theta, \\theta^*)^T\\) {% raw %} \\[\\begin{aligned}\n\\psi^* (Y_i, \\boldsymbol \\beta) &= \\begin{pmatrix} \\psi(Y, \\boldsymbol \\theta) \\\\ g(\\boldsymbol \\theta) - \\theta^* \\end{pmatrix} \\\\\n\\mathbf A^* &= - E \\begin{bmatrix} \\frac{\\partial \\psi^* (Y_i, \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} \\end{bmatrix}\n= - E \\begin{bmatrix} \\frac{\\partial \\psi(Y, \\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta}  &\n\\frac{\\partial \\psi(Y, \\boldsymbol \\theta)}{\\partial \\theta^*} \\\\\n\\frac{\\partial}{\\partial \\boldsymbol \\theta} \\left\\{ g(\\boldsymbol \\theta) - \\theta^*\\right\\} &\n\\frac{\\partial}{\\partial \\theta^*} \\left\\{ g(\\boldsymbol \\theta) - \\theta^*\\right\\} \\end{bmatrix} \\\\\n&= \\begin{bmatrix} \\mathbf A & 0 \\\\ -g'(\\boldsymbol \\theta) & 1\\end{bmatrix} \\\\\n\\mathbf B^*&= E\\begin{bmatrix} \\psi^* {\\psi^*}^T \\end{bmatrix}\n= E\\begin{bmatrix} \\begin{pmatrix} \\psi(Y, \\boldsymbol \\theta) \\\\ g(\\boldsymbol \\theta) - \\theta^* \\end{pmatrix}\n\\begin{pmatrix} \\psi^T(Y, \\boldsymbol \\theta) & g(\\boldsymbol \\theta) - \\theta^* \\end{pmatrix} \\end{bmatrix} \\\\\n&=E\\begin{bmatrix} \\psi(Y, \\boldsymbol \\theta) \\psi^T(Y, \\boldsymbol \\theta) &\n\\psi(Y, \\boldsymbol \\theta)[g(\\boldsymbol \\theta) - \\theta^*] \\\\\n\\psi(Y, \\boldsymbol \\theta)[g(\\boldsymbol \\theta) - \\theta^*] &\n[g(\\boldsymbol \\theta) - \\theta^*]^2 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} \\mathbf B & 0 \\\\ 0 & 0 \\end{bmatrix} \\\\\n\\mathbf V^* &= {\\mathbf A^*}^{-1}\\mathbf B^* \\left\\{{\\mathbf A^*}^{-1}\\right\\}^T \\\\\n&= \\begin{bmatrix} \\mathbf A & 0 \\\\ -g'(\\boldsymbol \\theta) & 1\\end{bmatrix}^{-1}\n\\begin{bmatrix} \\mathbf B & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\left\\{\\begin{bmatrix} \\mathbf A & 0 \\\\ -g'(\\boldsymbol \\theta) & 1\\end{bmatrix}^{-1}\\right\\}^T \\\\\n&= \\mathbf A^{-1}\\begin{bmatrix} 1 & 0 \\\\  g'(\\boldsymbol \\theta)& \\mathbf A\\end{bmatrix}\n\\begin{bmatrix} \\mathbf B & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\left\\{\\mathbf A^{-1}\\begin{bmatrix} 1 & 0\\\\  g'(\\boldsymbol \\theta)& \\mathbf A\\end{bmatrix}\\right\\}^T \\\\\n&= \\begin{bmatrix} \\mathbf A^{-1} & 0\\\\  g'(\\boldsymbol \\theta)\\mathbf A^{-1}& 1\\end{bmatrix}\n\\begin{bmatrix} \\mathbf B & 0 \\\\ 0 & 0 \\end{bmatrix}\n\\left\\{\\begin{bmatrix} \\mathbf A^{-1} & 0\\\\  g'(\\boldsymbol \\theta)\\mathbf A^{-1}& 1\\end{bmatrix}\\right\\}^T \\\\\n&= \\begin{bmatrix} \\mathbf {A^{-1}BA^{-1}}^T & \\mathbf {A^{-1}BA^{-1}}^T g'(\\boldsymbol \\theta)^T \\\\\ng'(\\boldsymbol \\theta)\\mathbf {A^{-1}BA^{-1}}^T  & g'(\\boldsymbol \\theta) \\mathbf {A^{-1}BA^{-1}}^T g'(\\boldsymbol \\theta)^T\\end{bmatrix} ~~~~\\text{using Microsoft Mathematics for matrix multiplication}\n\\end{aligned}\\] {% endraw %}\nThe \\(g'(\\hat {\\boldsymbol \\theta})\\) is \\(AN \\left( \\boldsymbol \\theta_0, \\frac{g'(\\boldsymbol \\theta)\\mathbf V g'(\\boldsymbol \\theta)^T } n \\right)\\), where \\(\\mathbf V = \\mathbf {A^{-1}BA^{-1}}^T\\)"
  },
  {
    "objectID": "chapters/ch7.html#section-3",
    "href": "chapters/ch7.html#section-3",
    "title": "Chapter 7: M-Estimation (Estimating Equations)",
    "section": "7.7",
    "text": "7.7\nThe generalized method of moments (GMM) is an important estimation method found mainly in the econometrics literature and closely related to M-estimatiion. Suppose that we have iid random variables \\(Y_1, \\dots, Y_n\\) and a \\(p\\) dimensional unknown parameter \\(\\boldsymbol \\theta\\). The key idea is that there are a set of \\(g \\geq p\\) possible estimating equations \\[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta) = 0, ~~~~~~~j=1, \\dots, q\\] motivated by the fact that \\(E_{\\psi_j}(Y_1; \\boldsymbol \\theta_0)\\) where \\(\\boldsymbol \\theta_0\\) is the true value. These motivating zero expectations come from the theory in the subject area being studied. But notice that if \\(q &gt; p\\), then we have too many equations. The GMM approach is to minimize the objective function\n\\[T= \\left[\\frac 1 n \\sum_{i=1}^n \\boldsymbol \\psi (Y_i; \\boldsymbol \\theta)\\right]^T \\mathbf W \\left[\\frac 1 n \\sum_{i=1}^n \\boldsymbol \\psi (Y_i; \\boldsymbol \\theta)\\right]\\]\nwhere \\(\\boldsymbol \\psi = (\\psi_1, \\dots, \\psi_q)^T\\) and \\(\\mathbf W\\) is a matrix of weights. Now let’s simplify the problem by letting \\(q=2, p=1\\) so that \\(\\theta\\) is real-valued, and \\(\\mathbf W = \\text{diag}(w_1,w_2)\\). Then T reduces to\n\\[T= w_1\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 (Y_i; \\boldsymbol \\theta)\\right]^2 +w_2\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 (Y_i; \\boldsymbol \\theta)\\right]^2\\]\nTo find \\(\\hat \\theta\\) we just take the partial derivative of T with respect to \\(\\theta\\) and set it equal to 0:\n\\[\\begin{aligned}\nS(\\mathbf Y; \\theta) &= 2w_1\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i; \\boldsymbol \\theta)\\right] +2w_2\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i; \\boldsymbol \\theta)\\right] \\\\\n&= 0\n\\end{aligned}\\]\n\na.\nProve that \\(S(\\mathbf Y ; \\theta_0) \\overset p \\to 0\\) making any moment assumptions that you need. (This should suggest to you that the solution of the equation \\(S(\\mathbf Y; \\theta)=0\\) is consistent.)\nSolution:\n\\[\\begin{aligned}\n\\frac 1 n \\sum_{i=1}^n \\psi_j(Y_i; \\theta_0) &\\overset p \\to E\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j(Y_i; \\theta_0) \\right] = 0 & \\text{ by WLLN} \\\\\n\\frac 1 n \\sum_{i=1}^n \\psi_j'(Y_i; \\theta_0) &\\overset p \\to E\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j'(Y_i; \\theta_0) \\right] & \\text{ by WLLN} \\\\\n\\text{Assuming } E\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j'(Y_i; \\theta_0) \\right] &&lt; \\infty,\\\\\nw_j \\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]&\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j '(Y_i; \\boldsymbol \\theta)\\right] \\overset p \\to 0 & \\text{by Slutsky's Theorem}\n\\end{aligned}\\] \\[\\begin{aligned}\n\\implies S(\\mathbf Y; \\theta) &= 2w_1\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i; \\boldsymbol \\theta)\\right] +2w_2\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i; \\boldsymbol \\theta)\\right] \\\\\n&\\overset p \\to 0\n\\end{aligned}\\]\n\n\nb.\nTo get asymptotic normality for \\(\\hat \\theta\\), a direct approach is to expand \\(S(\\mathbf Y; \\hat \\theta)\\) around \\(\\theta_0\\) and solve for \\(\\hat \\theta - \\theta_0\\). \\[\\hat \\theta - \\theta_0 = \\left[-\\frac{\\partial S(\\mathbf Y; \\theta_0)}{\\partial \\theta^T} \\right]^{-1}S(\\mathbf Y; \\theta_0)+ \\left[-\\frac{\\partial S(\\mathbf Y; \\theta_0)}{\\partial \\theta^T} \\right]^{-1}R_n\\] Then one ignores the remainder term and uses Slutsky’s Theorem along with asymptotic normality of \\(S(\\mathbf Y; \\theta_0)\\). But how to get the asymptotic normality of \\(S(\\mathbf Y; \\theta_0)\\)? Find \\(h(Y_i; \\theta_0)\\) such that \\[S(\\mathbf Y; \\theta_0) = \\frac 1 n \\sum_{i=1}^n h(Y_i; \\theta_0) + R_n^*\\] No proofs are required.\nSolution:\n\\[\\begin{aligned}\nS(\\mathbf Y; \\theta) &= 2w_1\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i; \\boldsymbol \\theta)\\right] +2w_2\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i; \\boldsymbol \\theta)\\right] \\\\\n&= \\sum_{j=1}^2 2w_j\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j '(Y_i; \\boldsymbol \\theta)\\right] \\\\\n&= \\sum_{j=1}^2 2w_j\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j '(Y_i; \\boldsymbol \\theta) + E\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right)- E\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right)\\right] \\\\\n&=\\sum_{j=1}^2 2w_jE\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right)\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]+ 2w_j\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j '(Y_i; \\boldsymbol \\theta)- E\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right)\\right] \\\\\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\text{Let }R_n^* &= \\sum_{j=1}^2 2w_j\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j '(Y_i; \\boldsymbol \\theta)- E\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right)\\right] \\\\\n\\implies \\sqrt n R_n^* &= \\sum_{j=1}^2 2w_j\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]\\sqrt n\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j '(Y_i; \\boldsymbol \\theta)- E\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right)\\right] \\\\\n&\\overset p \\to 0 ~~~ \\text{by WLLN, Central Limit Theorem, and Slutsky's Theorem}\n\\end{aligned}\\] \\[\\begin{aligned}\n\\implies S(\\mathbf Y; \\theta) &= \\sum_{j=1}^2 2w_jE\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right)\\left[\\frac 1 n \\sum_{i=1}^n \\psi_j (Y_i; \\boldsymbol \\theta)\\right]+ R_n \\\\\n&= \\frac 1 n \\sum_{i=1}^n \\sum_{j=1}^2 2w_j\\psi_j (Y_i; \\boldsymbol \\theta)E\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right) + R_n \\\\\n\\implies h(Y_i; \\theta_0) &=\\sum_{j=1}^2 2w_j\\psi_j (Y_i; \\boldsymbol \\theta)E\\left(\\psi_j '(Y_i; \\boldsymbol \\theta) \\right) \\\\\n&=2w_1\\psi_1 (Y_i; \\boldsymbol \\theta)E\\left(\\psi_1 '(Y_i; \\boldsymbol \\theta) \\right)+2w_2\\psi_2 (Y_i; \\boldsymbol \\theta)E\\left(\\psi_2 '(Y_i; \\boldsymbol \\theta) \\right)\n\\end{aligned}\\]\n\n\nc. \nThe equation \\(S(\\mathbf Y; \\theta) = 0\\) is not in the form for using M-estimation results (because the product of sums is not a simple sum). Show how to get it in M-estimation form by adding two new parameters, \\(\\theta_2\\) and \\(\\theta_3\\), and two new equations so that the result is a system of three equations with three \\(\\psi\\) functions; call them \\(\\psi_1^*, \\psi_2^*\\), and \\(\\psi_3^*\\) because \\(\\psi_1^*\\) is actually a function of the original \\(\\psi_1\\) and \\(\\psi_2\\).\nSolution:\n\\[\\begin{aligned}\nS(\\mathbf Y; \\theta) &= 2w_1\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 (Y_i;  \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i;  \\theta)\\right] +2w_2\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 (Y_i;  \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i;  \\theta)\\right] \\\\\n\\psi_1^* &= 2w_1 \\psi_1 (Y_i;  \\theta) \\theta_2 +2w_2\\psi_2 (Y_i;  \\theta)\\theta_3\\\\\n\\psi_2^* &= \\psi_1'(Y_i; \\theta) - \\theta_2 \\\\\n\\psi_3^* &= \\psi_2'(Y_i; \\theta) - \\theta_3\n\\end{aligned}\\]\nAlmost trivially, the solutions to \\(\\sum_{i=1}^n \\psi_2^* = 0\\) and \\(\\sum_{i=1}^n \\psi_3^* = 0\\) are respectively \\(\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i; \\theta)\\right]\\) and \\(\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i; \\theta)\\right]\\).\nThus, \\[\\begin{aligned}\n0&= \\sum_{i=1}^n \\psi_1^*,\\sum_{i=1}^n \\psi_2^* = 0,\\sum_{i=1}^n \\psi_3^* = 0\\\\\n\\implies 0 &= \\sum_{i=1}^n \\left(2w_1 \\psi_1 (Y_i;  \\theta) \\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i;  \\theta)\\right] +2w_2\\psi_2 (Y_i;  \\theta)\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i;  \\theta)\\right] \\right) \\\\\n\\implies 0 &= \\frac 1 n\\sum_{i=1}^n \\left(2w_1 \\psi_1 (Y_i;  \\theta) \\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i;  \\theta)\\right] +2w_2\\psi_2 (Y_i;  \\theta)\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i;  \\theta)\\right] \\right) \\\\\n&= 2w_1\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 (Y_i;  \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_1 '(Y_i;  \\theta)\\right] +2w_2\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 (Y_i;  \\theta)\\right]\\left[\\frac 1 n \\sum_{i=1}^n \\psi_2 '(Y_i;  \\theta)\\right] \\\\\n&= S(\\mathbf Y; \\theta)\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch8.html#extra-problem-1",
    "href": "chapters/ch8.html#extra-problem-1",
    "title": "Chapter 8: Hypothesis Tests under Misspecification and Relaxed Assumptions",
    "section": "Extra Problem 1",
    "text": "Extra Problem 1\nProve Equation (8.9) on Page 341\nSolution:\nNote that \\(\\frac \\partial {\\partial \\boldsymbol \\theta^T} \\log f(y;\\boldsymbol \\theta_g) = \\mathbf 0\\) as it is a score equation.\nAlso note that \\(\\frac {\\partial^2} {\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^T} \\{\\log f(y;{\\boldsymbol \\theta}_g)\\} \\overset p \\to E \\left( \\frac {\\partial^2} {\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^T} \\{\\log f(y;{\\boldsymbol \\theta}_g)\\} \\right)= - \\mathbf A\\)\nBegin by taking a Taylor expansion of the score function of \\(S(y;\\boldsymbol \\theta) = \\frac \\partial {\\partial \\boldsymbol \\theta^T} \\log f(y;\\boldsymbol \\theta)\\).\n\\[\\begin{aligned}\n\\frac \\partial {\\partial \\boldsymbol \\theta^T} \\log f(y;\\boldsymbol \\theta) &= \\frac \\partial {\\partial \\boldsymbol \\theta^T} \\log f(y;\\boldsymbol \\theta_g) + n\\frac {\\partial^2} {\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^T} \\{\\log f(y;{\\boldsymbol \\theta}_g)\\}(\\boldsymbol \\theta -  {\\boldsymbol \\theta}_g) \\\\\n\\sqrt n(\\boldsymbol \\theta -  {\\boldsymbol \\theta}_g) &= \\left[\\frac {\\partial^2} {\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^T} \\{\\log f(y;{\\boldsymbol \\theta}_g)\\}\\right]^{-1}\\begin{pmatrix} \\mathbf S_1 (\\boldsymbol \\theta)/ \\sqrt n \\\\ 0 \\end{pmatrix} \\\\\n& \\overset d \\to -\\mathbf A^{-1} \\begin{pmatrix} \\mathbf Z \\\\ \\mathbf 0 \\end{pmatrix} ~~~~ where~ Z \\sim MVN\\left(\\mathbf 0,\\mathbf V_{gS_1}\\right)\n\\end{aligned}\\]\nTaking a Taylor Expansion of \\(\\log f(y;\\boldsymbol \\theta)\\), we get:\n\\[\\begin{aligned}\n\\log f(y;\\boldsymbol \\theta) &= \\log f(y;{\\boldsymbol \\theta}_g) + \\frac \\partial {\\partial \\boldsymbol \\theta^T} \\{\\log f(y;  {\\boldsymbol \\theta}_g)\\}(\\boldsymbol \\theta - {\\boldsymbol \\theta}_g) + \\sqrt n(\\boldsymbol \\theta - \\boldsymbol \\theta_g)^T\\frac {\\partial^2} {\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^T} \\{\\log f(y;{\\boldsymbol \\theta}_g)\\}\\sqrt n(\\boldsymbol \\theta -  {\\boldsymbol \\theta}_g) \\\\\n\\implies T_{LR} &= -2[\\ell(\\boldsymbol \\theta)- \\ell (\\boldsymbol \\theta_g)] \\\\\n&= -\\sqrt n(\\boldsymbol \\theta - \\boldsymbol \\theta_g)^T\\frac {\\partial^2} {\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^T} \\{\\log f(y;  {\\boldsymbol \\theta}_g)\\}\\sqrt n(\\boldsymbol \\theta -  {\\boldsymbol \\theta}_g) \\\\\n& \\overset d \\to \\begin{pmatrix} \\mathbf Z & \\mathbf 0 \\end{pmatrix} {\\mathbf A^{-1}}^T{\\mathbf {AA}^{-1}} \\begin{pmatrix} \\mathbf Z \\\\ \\mathbf 0 \\end{pmatrix} \\\\\n& \\overset d \\to  \\mathbf Z ^T{\\mathbf {A}^{-1}}_{11} \\mathbf Z \\\\\n& \\overset d \\to  \\mathbf Z ^T[\\mathbf{A_{11}- A_{12}A_{22}^{-1}A_{21}}]^{-1} \\mathbf Z\n\\end{aligned}\\]"
  },
  {
    "objectID": "chapters/ch8.html#section",
    "href": "chapters/ch8.html#section",
    "title": "Chapter 8: Hypothesis Tests under Misspecification and Relaxed Assumptions",
    "section": "8.10",
    "text": "8.10\nSuppose we have data \\(X_1, \\dots, X_n\\) that are iid. The sign test for \\(H_0: median = 0\\) is to count the number of \\(X\\)’s above \\(0\\), say \\(Y\\), and compare \\(Y\\) to a binomial(\\(n,p=1/2\\)) distribution. Starting with the defining M-estimator equation for the sample median (see Example 7.4.2),\nDefining M-estimator for the sample median (Example 7.4.2):\n\\(\\hat \\theta = \\hat \\eta_{1/2} = F_n^{-1}(1/2)\\) satisfies \\(\\sum \\left[\\frac 1 2 - I(Y_i \\leq \\hat \\theta) \\right] = c_n\\), where \\(|c_n| = n\\left|F_n^{-1}(\\hat \\theta)- \\frac 1 2\\right| \\leq 1\\).\n\\[\\begin{aligned}\n\\implies \\psi(X_i, \\theta) &= \\frac 1 2 - I(X_i \\leq \\theta) \\\\\nA(\\theta_0) = f(\\theta_0) ~~~~&,~~~B(\\theta_0) = \\frac 1 2\\left(1-\\frac 1 2\\right)=\\frac 1 4 \\\\\nV(\\theta_0) &= \\frac{1/4}{f^2(\\theta_0)}\n\\end{aligned}\\]\n\na.\nDerive the generalized score statistic \\(T_{GS}\\) for \\(H_0: median = 0\\) and note that it is the large sample version of the two-sided sign test statistic.\nSolution:\n\\[\\begin{aligned}\nT_{GS} &= n^{-1} \\left[\\sum \\psi(X_i, \\overset \\sim \\theta) \\right] \\overset \\sim V_{\\psi}^{-1} \\left[\\sum \\psi(X_i, \\overset \\sim \\theta) \\right]  & \\text{(8.20) pg. 347, scalar } \\psi \\text{ so no transpose}\\\\\nV_{\\psi} &= B = \\frac 1 4  & \\text{one dimensional case reduction, pg. 347} \\\\\n\\implies T_{GS} &= \\frac {1/\\overset \\sim B} n \\left(\\sum \\frac 1 2 - I(X_i \\leq \\overset \\sim \\theta) \\right)^2 \\\\\n&= \\frac {1} {n \\overset \\sim p(1- \\overset \\sim p)} \\left(\\frac n 2 - \\sum I(X_i \\leq 0) \\right)^2\\\\\n&= \\frac {1} {n\\bar X \\left(1- \\bar X \\right)} \\left(\\frac n 2 - \\sum I(X_i \\leq 0) \\right)^2\n\\end{aligned}\\]\n\n\nb.\nUsing the expression for the asymptotic variance of the sample median, write down the form of a generalized Wald statistic \\(T_{GW}\\), and explain why it is not as attractive to use her as \\(T_{GS}\\)\nSolution:\n\\[\\begin{aligned}\nT_{GW} &= n(\\hat \\theta - \\hat \\theta_0)\\hat V^{-1}(\\hat \\theta - \\theta_0)\\\\\n&=\\frac{n \\hat p(1-\\hat p)(\\hat p- 1/2)^2}{f^2(\\hat p)} \\\\\n\\end{aligned}\\]\nThe generalized score statistic is more attractive as we don’t rely on \\(f\\), which could be misspecified."
  },
  {
    "objectID": "chapters/ch8.html#extra-problem-2",
    "href": "chapters/ch8.html#extra-problem-2",
    "title": "Chapter 8: Hypothesis Tests under Misspecification and Relaxed Assumptions",
    "section": "Extra Problem 2",
    "text": "Extra Problem 2\nDerive the generalized score test for the two independent samples of clustered binary data as described in Example 8.5.\nSolution:\n\\[\\begin{aligned}\n\\boldsymbol \\psi(\\mathbf x_i,Y_i, \\boldsymbol \\beta) &= (Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta) \\mathbf x_i = \\mathbf 0\n\\end{aligned}\\]\n\\(\\mathbf x_i^T = (1,0)\\) for first sample, \\((1,-1)\\) for second sample. \\(\\boldsymbol \\beta^T = (\\beta_1, \\beta_2)=(p_1,p_1-p_2)\\)\n\\(\\overset \\sim p = \\sum_{i=1}^n Y_i/\\sum_{i=1}^n m_i, \\hat p_1 = \\sum_{i=1}^{n_1} Y_i/m_1, , \\hat p_2 = \\sum_{i=1+n_1}^{n} Y_i/m_1\\)\nwhere \\(n = n_1+n_2, m_1 = \\sum_{i=1}^{n_1} m_i, m_2 = \\sum_{i=1+n_1}^{n} m_i\\)\n\\[\\begin{aligned}\n\\boldsymbol {\\psi \\psi}^T  &=(Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta) \\mathbf x_i \\mathbf x_i^T(Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta)^T \\\\\n&=(Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta) (Y_i - m_i \\boldsymbol \\beta^T\\mathbf x_i)\\mathbf x_i \\mathbf x_i^T & (Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta) \\text{ is scalar}\\\\\n&=(Y_i^2 -Y_i m_i  \\boldsymbol \\beta^T\\mathbf x_i- Y_im_i \\mathbf x_i^T \\boldsymbol \\beta + m_i^2 \\mathbf x_i^T \\boldsymbol \\beta \\boldsymbol \\beta^T\\mathbf x_i)\\mathbf x_i \\mathbf x_i^T \\\\\n&=(Y_i^2 -2Y_i m_i  \\boldsymbol \\beta^T\\mathbf x_i + m_i^2 \\mathbf x_i^T \\boldsymbol \\beta \\boldsymbol \\beta^T\\mathbf x_i)\\mathbf x_i \\mathbf x_i^T & \\beta^T\\mathbf x_i \\text{ is scalar} \\\\\n\\boldsymbol \\psi'  &= - m_i \\mathbf x_i^T \\mathbf x_i & \\text{product rule}\n\\end{aligned}\\]\nUsing Microsoft Mathematics for matrix multiplication,\n\\[\\begin{aligned}\n\\beta^T \\mathbf x_i &= \\begin{cases} \\beta_1  \\\\\n\\beta_1 - \\beta_2 \\end{cases}\n= \\begin{cases} p_1 \\text{ if first sample} \\\\\np_2 \\text{ if second sample}\\end{cases} \\\\ \\\\\n\\mathbf x_i^T \\boldsymbol \\beta \\boldsymbol \\beta^T\\mathbf x_i &= \\begin{cases}\n\\beta_1^2  \\\\\n(\\beta_1 - \\beta_2)^2 \\end{cases}\n=\\begin{cases}\np_1^2 \\text{ if first sample} \\\\\np_2^2 \\text{ if second sample}\\end{cases} \\\\\n\\mathbf x_i \\mathbf x_i^T &= \\begin{cases}\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\text{ if first sample} \\\\\n\\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} \\text{ if second sample}\n\\end{cases}\n\\end{aligned}\\]\nAlso note that \\(E(Y_i) =m_ip_j\\) and \\(E(Y_i^2) = m_ip_j(1-p_j) + m_i^2p_j^2\\) for sample \\(j\\).\nTherefore,\n\\[\\begin{aligned}\n\\mathbf A &= - \\frac 1 n \\sum_{i=1}^n \\boldsymbol \\psi' &\\text{p. 301}\\\\\n&= \\frac 1 n \\sum_{i=1}^n m_i \\mathbf x_i^T \\mathbf x_i \\\\\n&= \\frac 1 n \\sum_{i=1}^{n_1} m_i \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac 1 n \\sum_{i=1+n_1}^{n} m_i \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} \\\\\n&= \\frac 1 n \\begin{pmatrix} m_1+m_2 & -m_2 \\\\ -m_2 & m_2 \\end{pmatrix}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\mathbf B &= \\frac 1 n \\sum_{i=1}^n \\boldsymbol \\psi \\boldsymbol \\psi^T \\\\\n&= \\frac 1 n \\sum_{i=1}^n (Y_i^2 -2Y_i m_i  \\boldsymbol \\beta^T\\mathbf x_i + m_i^2 \\mathbf x_i^T \\boldsymbol \\beta \\boldsymbol \\beta^T\\mathbf x_i)\\mathbf x_i \\mathbf x_i^T \\\\\n&= \\frac 1 n \\sum_{i=1}^n (Y_i^2 -2Y_i m_i p_{j[i]} + m_i^2 p_{j[i]}^2)\\mathbf x_i \\mathbf x_i^T ~~~ j \\text{ indicates sample 1 or 2}\\\\\n&= \\frac 1 n \\sum_{i=1}^{n+1} (Y_i - m_i p_1)^2 \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\\\\\n&= \\frac 1 n \\begin{pmatrix} \\sum_{i=1}^{n+1} (Y_i - m_i p_1)^2+  \\sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2&\n- \\sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 \\\\ - \\sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 &  \\sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 \\end{pmatrix}\n\\end{aligned}\\]\nUnder \\(\\beta_2 = p_1- p_2 =0\\), we get \\(p_1 = p_2 = \\overset \\sim p= \\beta_1\\), thus,\n\\[\\overset \\sim {\\mathbf B} = \\frac 1 n \\begin{pmatrix} \\sum_{i=1}^{n+1} (Y_i - m_i \\overset \\sim p )^2+  \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2&\n- \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 \\\\ - \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 &  \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 \\end{pmatrix}\\]\n\\[\\begin{aligned}\nT_{GS} &= n^{-1} \\left[ \\sum \\boldsymbol \\psi_2(Y_i, \\overset \\sim {\\boldsymbol \\beta} ) \\right] \\overset \\sim V_{\\psi_2}^{-1} \\left[\\sum \\boldsymbol \\psi_2(Y_i, \\overset \\sim {\\boldsymbol \\beta}) \\right]  ~~~~~~~~~~~ \\text{pg. 347 } \\\\\n\\mathbf V_{\\psi_1} &= \\mathbf{B_{11} - A_{12} A_{22}^{-1} B_{21}- B_{12}\\{A_{22}^{-1}\\}^T A_{12}^T+A_{12}A_{22}^{-1}B_{22}{\\{A_{22}}^{-1}\\}^T A_{12}^T}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\mathbf V_{\\psi_2} &= \\mathbf{B_{22} - A_{21}A_{11}^{-1}B_{12}- B_{21} \\{A_{11}^{-1}\\} ^T A_{21}^T + A_{21} A_{11} ^{-1} B_{11} \\{{A_{11}}^{-1}\\}^T A_{21}^T} \\\\\n\\overset \\sim {\\mathbf V}_{\\psi_2} &= \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 - \\left(\\frac {-m_2}{n} \\right)\\left(\\frac {n}{m_1+m_2} \\right)\\left(-\\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 \\right) \\\\\n&~~~~~ - \\left(-\\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 \\right)\\left(\\frac {n}{m_1+m_2} \\right)\\left(\\frac {-m_2}{n} \\right) \\\\\n&~~~~~ + \\left( \\frac {-m_2}{n} \\right)\\left( \\frac {n}{m_1+m_2}\\right)\\left( \\frac 1 n\\sum_{i=1}^{n+1} (Y_i - m_i p_1)^2+ \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2\\right) \\left(\\frac {n}{m_1+m_2} \\right) \\left(\\frac{-m_2}{n} \\right) \\\\\n&= \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 - \\left(\\frac {2m_2}{m_1+m_2} \\right)\\left(\\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2 \\right) \\\\\n&~~~~~ + \\left( \\frac {m_2^2}{(m_1+m_2)^2}\\right)\\left( \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2\\right)+ \\left( \\frac {m_2^2}{(m_1+m_2)^2}\\right)\\left( \\frac 1 n \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2\\right)\n\\end{aligned}\\]\n\\[\\begin{aligned}\n&=\\left( 1-\\frac {m_2}{m_1+m_2}\\right)^2\\left( \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2\\right)+ \\left( \\frac {m_2}{(m_1+m_2)}\\right)^2\\left( \\frac 1 n \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2\\right)\\\\\n&=\\left( \\frac {m_1}{m_1+m_2}\\right)^2\\left( \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2\\right)+ \\left( \\frac {m_2}{m_1+m_2}\\right)^2\\left( \\frac 1 n \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2\\right)\n\\end{aligned}\\]\nAgain, using Microsoft mathematics:\nIn the first sample, \\[\\boldsymbol \\psi =(Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta) \\mathbf x_i = \\begin{pmatrix} Y_i -m_i \\beta_1 \\\\0 \\end{pmatrix}\\]\nIn the second sample, \\[\\boldsymbol \\psi =(Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta) \\mathbf x_i = \\begin{pmatrix} Y_i -m_i (\\beta_1- \\beta_2) \\\\ -Y_i +m_i (\\beta_1- \\beta_2)\\end{pmatrix}\\]\nCombining those samples, \\[\\boldsymbol \\psi =(Y_i - m_i \\mathbf x_i^T \\boldsymbol \\beta) \\mathbf x_i =\n\\begin{pmatrix} \\sum_{i=1}^{n_1} Y_i -m_i \\beta_1 + \\sum_{i=1+ n_1}^{n} Y_i -m_i (\\beta_1- \\beta_2) \\\\ \\sum_{i=1+ n_1}^{n} -Y_i +m_i (\\beta_1- \\beta_2)\\end{pmatrix}\\]\nThus, \\[\\begin{aligned}\nT_{GS} &= n^{-1} \\left[\\sum \\boldsymbol \\psi_2(Y_i, \\overset \\sim {\\boldsymbol \\beta}) \\right] \\overset \\sim V_{\\psi_2}^{-1} \\left[\\sum \\boldsymbol \\psi_2(Y_i, \\overset \\sim {\\boldsymbol \\beta}) \\right] \\\\\n&=  \\frac{n^{-1}\\left[\\sum_{i= 1+n_1}^{n_2}-Y_i +m_i (\\overset \\sim \\beta_1- \\overset \\sim \\beta_2) \\right]^2}{\\left( \\frac {m_1}{m_1+m_2}\\right)^2\\left( \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2\\right)+ \\left( \\frac {m_2}{m_1+m_2}\\right)^2\\left( \\frac 1 n \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2\\right)} \\\\\n&=  \\frac{n^{-1}\\left[\\sum_{i= 1+n_1}^{n_2}-Y_i +m_i \\overset \\sim p \\right]^2}{\\left( \\frac {m_1}{m_1+m_2}\\right)^2\\left( \\frac 1 n \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2\\right)+ \\left( \\frac {m_2}{m_1+m_2}\\right)^2\\left( \\frac 1 n \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2\\right)} \\\\\n&=  \\frac{(m_1+ m_2)^2\\left[m_2 \\overset \\sim p -\\sum_{i= 1+n_1}^{n_2}Y_i \\right]^2}{m_1^2 \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2+ m_2^2  \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2} \\\\\n&=  \\frac{(m_1+ m_2)^2\\left[m_2 \\frac{\\sum_{i=1}^{n_1} Y_i+\\sum_{i=1+n_1}^{n} Y_i}{m_1 + m_2} -\\frac{(m_1+m_2)\\sum_{i= 1+n_1}^{n_2}Y_i}{m_1+m_2} \\right]^2}{m_1^2 \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2+ m_2^2  \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2} \\\\\n&=  \\frac{\\left[m_2 (\\sum_{i=1}^{n_1} Y_i+\\sum_{i=1+n_1}^{n} Y_i) -(m_1+m_2)\\sum_{i= 1+n_1}^{n_2}Y_i \\right]^2}{m_1^2 \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2+ m_2^2  \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2} \\\\\n&=  \\frac{\\left[m_2 \\sum_{i=1}^{n_1} Y_i -m_1\\sum_{i= 1+n_1}^{n_2}Y_i \\right]^2}{m_1^2 \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2+ m_2^2  \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2} \\\\\n&=  \\frac{\\left[m_1m_2 \\hat p_1 -m_1 m_2 \\hat p_2 \\right]^2}{m_1^2 \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2+ m_2^2  \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2} \\\\\n&=  \\frac{m_1^2m_2^2\\left[\\hat p_1 -\\hat p_2 \\right]^2}{m_1^2 \\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2+ m_2^2  \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2} \\\\\n&=  \\frac{\\left[\\hat p_1 -\\hat p_2 \\right]^2}{\\sum_{i=1+n_1}^{n} (Y_i - m_i \\overset \\sim p )^2/m_2^2+  \\sum_{i=1}^{n_1} (Y_i - m_i \\overset \\sim p )^2/m_1^2} \\\\\n\\end{aligned}\\]\nWhich matches the book. (8.25 pg. 350)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Boos, D. D., and L. A. Stefanski. 2013. Essential Statistical\nInference: Theory and Methods. Springer Texts in Statistics.\nSpringer New York. https://link.springer.com/book/10.1007/978-1-4614-4818-1."
  }
]