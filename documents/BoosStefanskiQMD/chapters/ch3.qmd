---
title: "Selected Solutions to Boos and Stefanski"
author: "Michael Throolin"
subtitle: "Chapter 3: Likelihood-Based Tests and Confidence Regions"
format: pdf
geometry:
      - top=30mm
      - left=20mm
      - heightrounded
---

## 3.6

Assume that $Y_1$ and $Y_2$ are independent with respective geometric probability mass functions, $$f(y;p_i) = p_i(1-p_i)^{y-1}~~~~y=0,1,\dots~~~~ 0 \leq p_i \leq 1, ~~i=1,2.$$ Recall that the mean and variance of a geometric random variable with parameter $p$ are $1/p$ and $(1-p)/p^2$, respectively. For $H_0: p_1=p_2$ versus $H_a: p_1 \neq p_2$ show that the score statistic is $$T_S=\frac{\overset \sim p ^2}{2(1- \overset \sim p)}(Y_1-Y_2)^2, ~~~where~\overset \sim p = \frac 2 {Y_1+Y_2}$$

Solution:

$$\begin{aligned}
\boldsymbol \theta &= \begin{pmatrix} p_1 \\p_2 \end{pmatrix}, ~~~ h(\boldsymbol \theta) =p_1-p_2 = 0 \\
\mathcal L(\boldsymbol \theta) &= p_1(1-p_1)^{y_1-1}p_2(1-p_2)^{y_2-1},~~~~ H(\boldsymbol \theta) = \begin{bmatrix} 1 & -1 \end{bmatrix}
\\
\mathcal \ell(\boldsymbol \theta) &= \log p_1 + (y_1-1)\log(1-p_1) +\log p_2+ (y_2-1)\log(1-p_2)
\end{aligned}
$$
$$\begin{aligned}
S(\boldsymbol \theta) &= \begin{pmatrix} \frac 1 {p_1}- \frac{y_1-1}{1-p_1} \\ \frac 1 {p_2}- \frac{y_2-1}{1-p_2} \end{pmatrix} \\
I_T(\boldsymbol \theta) &= -E\begin{pmatrix} \frac {-1} {p_1^2}- \frac{y_1-1}{(1-p_1)^2} & 0 \\ 0& \frac {-1} {p_2^2}- \frac{y_2-1}{(1-p_2)^2} \end{pmatrix} =-\begin{pmatrix} \frac {-1} {p_1^2}- \frac{\frac{1}{p_1}-1}{(1-p_1)^2} & 0 \\ 0& \frac {-1} {p_2^2}- \frac{\frac{1}{p_2}-1}{(1-p_2)^2} \end{pmatrix} \\
&=-\begin{pmatrix} \frac {-(1-p_1)^2-p_1+p_1^2}{p_1^2(1-p_1)^2}& 0 \\ 0& \frac {-(1-p_2)^2-p_2+p_2^2}{p_2^2(1-p_2)^2} \end{pmatrix}=-\begin{pmatrix} \frac {-1+2p_1-p_1^2-p_1+p_1^2}{p_1^2(1-p_1)^2}& 0 \\ 0& \frac {-1+2p_2 - p_2^2-p_2+p_2^2}{p_2^2(1-p_2)^2} \end{pmatrix} \\
&=-\begin{pmatrix} \frac {p_1-1}{p_1^2(1-p_1)^2}& 0 \\ 0& \frac {p_2-1}{p_2^2(1-p_2)^2} \end{pmatrix}
=\begin{pmatrix} \frac {1}{p_1^2(1-p_1)}& 0 \\ 0& \frac {1}{p_2^2(1-p_2)} \end{pmatrix} \\
I_T(\boldsymbol \theta)^{-1} &=  \begin{pmatrix} p_1^2(1-p_1)& 0 \\ 0& p_2^2(1-p_2) \end{pmatrix}
\end{aligned}$$

Using the Lagrange multiplier method,
$$\begin{aligned}
maximize \{\ell(p)  -\lambda h(\theta)\}
& = \log p_1 + (y_1-1)\log(1-p_1) +\log p_2+ (y_2-1)\log(1-p_2) + \lambda p_1 - \lambda p_2\\
p_1 &= p_2  = p\\
0 &= \frac{1} {p} -\frac{y_1-1}{1-p} + \lambda \implies \lambda = -\frac{1} {p} +\frac{y_1-1}{1-p}\\
0 &= \frac{1} {p} -\frac{y_2-1}{1-p} -\lambda \implies \lambda = \frac{1} {p} -\frac{y_2-1}{1-p} \\
\implies 0&= \frac{2} {p} -\frac{y_2-1+y_1-1}{1-p}\\
&= 2-2p +2p -p(y_1+y_2) \\
\implies \overset \sim p &= \frac 2 {y_1+y_2},~~~ \hat \lambda_{RMLE} = \frac{1} {\overset \sim p} -\frac{y_2-1}{1-\overset \sim p}\\
& \hat \lambda_{RMLE} =  \frac{1- \overset \sim p - \overset \sim p y_2+ \overset \sim p}{\overset \sim p (1-\overset \sim p)}=  \frac{1 - \frac {2y_2} {y_1+y_2}}{\overset \sim p (1-\overset \sim p)} \\
&\hat \lambda_{RMLE}=  \frac{\frac {y_1-y_2} {y_1+y_2}}{\overset \sim p (1-\overset \sim p)}=\frac {(y_1-y_2) \overset \sim p /2}{\overset \sim p (1-\overset \sim p)}\\
&\hat \lambda_{RMLE}=  \frac{y_1-y_2}{2 (1-\overset \sim p)}
\end{aligned}$$

$$\begin{aligned}
T_S &= \hat \lambda_{RMLE}^T H(\hat \theta_{RMLE})I_T^{-1}(\hat \theta_{RMLE})  H(\hat \theta_{RMLE})^T\hat \lambda_{RMLE}^T \\
&= \left(\frac{y_1-y_2}{2 (1-\overset \sim p)}\right)^2  \begin{pmatrix} 1 & -1 \end{pmatrix} \begin{pmatrix} \overset \sim p^2(1-\overset \sim p)& 0 \\ 0&\overset \sim p^2(1-\overset \sim p) \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} \\
&= \left(\frac{y_1-y_2}{2 (1-\overset \sim p)}\right)^2  2 (\overset \sim p^2(1-\overset \sim p)) \\
&= \frac{\overset \sim p^2}{2 (1-\overset \sim p)}(y_1-y_2)^2
\end{aligned}$$


\newpage
## 3.8

Suppose that $Y_1,\dots, Y_{n_1}$ are iid from a $N(\mu_1,\sigma^2)$ distribution, $X_1,\dots,X_{n_2}$ are iid from a $N(\mu_2,\sigma^2)$ distribution, the samples are independent of each other, and we desire to test $H_0:\mu_1=\mu_2$.

### a.

Derive the Wald and score tests and express them as a function of the square of the usual two-sample pooled t . Also, show that the likelihood ratio statistic is $T_{LR} = (n_1+n_2)\log[1+t^2/(n_1+n_2-2)]$.

Solution:

$$\begin{aligned}
\ell (\mu_1,\mu_2,\sigma) &= c-n_1\log \sigma - \frac 1 {2\sigma^2} \sum_{i=1}^{n_1}(Y_i-\mu_1)^2 +c-n_2\log \sigma - \frac 1 {2\sigma^2} \sum_{i=1}^{n_2}(X_i-\mu_2)^2\\
&= 2c-(n_1+n_2)\log \sigma - \frac 1 {2\sigma^2} \sum_{i=1}^{n_1}(Y_i-\mu_1)^2 - \frac 1 {2\sigma^2} \sum_{i=1}^{n_2}(X_i-\mu_2)^2 \\
S\begin{pmatrix} \mu_1 \\ \mu_2 \\\sigma \end{pmatrix} 
&=\begin{pmatrix} \frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu_1)\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu_2)\\
\frac{-n_1-n_2}{\sigma} +\frac 1 {\sigma^3} \sum_{i=1}^{n_1} (Y_i-\mu_1)^2 +\frac 1 {\sigma^3} \sum_{i=1}^{n_2} (X_i-\mu_2)^2 \end{pmatrix}\\
&=\begin{pmatrix} \frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu_1)\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu_2)\\
\frac 1 {\sigma^3} \sum_{i=1}^{n_1} [(Y_i-\mu_1)^2-\sigma^2] +\frac 1 {\sigma^3} \sum_{i=1}^{n_2} [(X_i-\mu_2)^2 - \sigma^2] \end{pmatrix}\\
I_T \begin{pmatrix} \mu_1 \\ \mu_2 \\\sigma \end{pmatrix}
&= \begin{pmatrix} \frac {n_1} {\sigma^2} &0&0\\
0&\frac {n_2} {\sigma^2} & 0\\
0&0& \frac{n_1+n_2}{\sigma^2}\end{pmatrix},
I_T^{-1} \begin{pmatrix} \mu_1 \\ \mu_2 \\\sigma \end{pmatrix}
= \sigma^2 \begin{pmatrix} \frac {1} {n_1} &0&0\\
0&\frac {1} {n_2} & 0\\
0&0& \frac 1 {n_1+n_2}\end{pmatrix} \\
\mathbf h( \boldsymbol {\theta}) &= \mu_1-\mu_2 ,
\mathbf H(\boldsymbol \theta) = \begin{pmatrix} 1 & -1 &0\end{pmatrix}, \mathbf H(\boldsymbol \theta) \mathbf {I_T^{-1}}(\boldsymbol {\theta}) \mathbf H(\boldsymbol \theta)^T= \sigma^2 \left(\frac 1 {n_1} +\frac 1 {n_2}\right)
\end{aligned}$$


$$\begin{aligned}
\begin{pmatrix} \frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu_1)\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu_2)\\
\frac 1 {\sigma^3} \sum_{i=1}^{n_1} [(Y_i-\mu_1)^2-\sigma^2] +\frac 1 {\sigma^3} \sum_{i=1}^{n_2} [(X_i-\mu_2)^2 - \sigma^2] \end{pmatrix} &= \mathbf 0 \\
\implies \hat \mu_1 = \frac 1 {n_1} \sum_{i=1}^{n_1} Y_i = \bar Y ~~,~~\hat \mu_2 = \frac 1 {n_2} \sum_{i=1}^{n_2} X_i = \bar X \\
\sum_{i=1}^{n_1} [(Y_i-\hat \mu_1)^2-\sigma^2] + \sum_{i=1}^{n_2} [(X_i-\hat \mu_2)^2 - \hat \sigma^2] = 0 \\
\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2  =  (n_1+ n_2) \hat \sigma^2 \\
\implies \hat \sigma_{MLE} ^2 = \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 }{n_1+n_2}
\end{aligned}
$$

Note that $$\begin{aligned}
S_p^2 &= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 }{n_1+n_2-2} \\
\implies \frac{(n_1+ n_2 -2)}{n_1+n_2}S_p^2 &= \hat \sigma^2_{MLE}
\end{aligned}$$
$$
\begin{aligned}
\implies  \sigma_{RMLE} ^2 
&= \frac{\sum_{i=1}^{n_1} (Y_i- \mu)^2 + \sum_{i=1}^{n_2} (X_i- \mu)^2 }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y + \bar Y- \mu)^2 + \sum_{i=1}^{n_2} (X_i- \bar X + \bar X- \mu)^2 }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + 2(Y_i-\bar Y)(\bar Y- \mu) +(\bar Y- \mu)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 + 2(X_i-\bar X)(\bar X- \mu) +(\bar X- \mu)^2  }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2  +(\bar Y- \mu)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 +(\bar X- \mu)^2  }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 +n_1(\bar Y- \mu)^2+n_2(\bar X- \mu)^2  }{n_1+n_2}\\
&= \sigma^2_{MLE}+\frac{n_1(\bar Y- \mu)^2+n_2(\bar X- \mu)^2  }{n_1+n_2}
\end{aligned}$$

\newpage
Thus, the Wald Statistic is:

$$\begin{aligned}
T_W &= \mathbf h( \boldsymbol {\hat \theta})^T [\mathbf H(\boldsymbol {\hat \theta}) \mathbf {I_T^{-1}}(\boldsymbol {\hat \theta}) \mathbf H(\boldsymbol {\hat \theta})^T]^{-1}\mathbf h( \boldsymbol {\hat \theta})\\
&= \frac{(\bar Y- \bar X)^2}{\hat \sigma^2_{MLE} \left(\frac 1 {n_1} +\frac 1 {n_2}\right)}
\end{aligned}$$


Now, for the score function, I need to maximize $\ell (\mu_1,\mu_2, \sigma) -\lambda(\mu_1-\mu_2)$.

$$\begin{aligned}
\mu_1- \mu_2 = 0 &\implies \mu_1 = \mu_2 = \mu\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu) -\lambda =0& \implies \lambda= \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu) \\\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu) + \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu)= 0 
&\implies \hat \mu_{RMLE} = \frac{\sum_{i=1}^{n_2} X_i + \sum_{i=1}^{n_1} Y_i}{n_1+n_2} = \frac{n_2 \bar X + n_1 \bar Y}{n_1+n_2}
\end{aligned}$$

Simplifying $\hat \lambda_{RMLE}$,
$$\begin{aligned}
\hat \lambda&= \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu) \\
&= \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i)-n_1 \left(\frac{\sum_{i=1}^{n_1} X_i + \sum_{i=1}^{n_2} Y_i}{n_1+n_2}\right) \\
&= \frac{1}{\sigma^2} \left(\frac{(n_1+n_2)\sum_{i=1}^{n_1} (Y_i)-n_1 \left(\sum_{i=1}^{n_1} X_i + \sum_{i=1}^{n_2} Y_i\right)}{n_1+n_2}\right) \\
&= \frac{1}{\sigma^2} \left(\frac{n_2\sum_{i=1}^{n_1} Y_i-n_1 \sum_{i=1}^{n_1} X_i}{n_1+n_2}\right) \\
&= \frac{1}{\sigma^2} \left(\frac{n_1n_2}{n_1+n_2} \right)(\bar Y - \bar X)
\end{aligned}$$


The score statistic is:
$$\begin{aligned}
T_S &= \hat \lambda_{RMLE}^T H(\hat \theta_{RMLE})I_T^{-1}(\hat \theta_{RMLE}) \hat \lambda_{RMLE}^T \\
&= \left[\frac{1}{\sigma^2} \left(\frac{n_1n_2}{n_1+n_2} \right)(\bar Y - \bar X)\right]^2\sigma^2 \left(\frac 1 {n_1} +\frac 1 {n_2}\right) \\
&= \frac{1}{\sigma^2} \left[\left(\frac{n_1n_2}{n_1+n_2} \right)(\bar Y - \bar X)\right]^2 \left(\frac {n_1+n_2} {n_1n_2}\right) \\
&= \frac{(\bar Y - \bar X)^2 }{\sigma^2\left(\frac 1 {n_1} +\frac 1{n_2}\right)}
\end{aligned}$$

\newpage

And the LRT is:

$$\begin{aligned}T_{LR} &= -2 [\ell (\hat \theta_{RMLE}) - \ell(\hat \theta_{RMLE})] \\
&=-2 [2c- \frac 1 2 (n_1+n_2)\log \sigma_{RMLE}^2 - \frac 1 {2\sigma_{RMLE}^2} \sum_{i=1}^{n_1}(Y_i-\mu)^2 - \frac 1 {2\sigma_{RMLE}^2} \sum_{i=1}^{n_2}(X_i-\mu)^2] \\
&~~~~~~+2 [2c-\frac 1 2 (n_1+n_2)\log \sigma_{MLE}^2 - \frac 1 {2\sigma_{MLE}^2} \sum_{i=1}^{n_1}(Y_i-\mu_1)^2 - \frac 1 {2\sigma_{MLE}^2} \sum_{i=1}^{n_2}(X_i-\mu_2)^2] \\
&=(n_1+n_2) \log \left(\frac{\sigma_{RMLE}^2}{\sigma_{MLE}^2} \right)\\
&= (n_1+n_2) \log \left(\frac{\sigma^2_{MLE}+\frac{n_1(\bar Y- \mu)^2+n_2(\bar X- \mu)^2  }{n_1+n_2}}{\sigma_{MLE}^2} \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac 1{n_1+n_2}\left(n_1(\bar Y- \frac{n_2 \bar X + n_1 \bar Y}{n_1+n_2})^2+n_2(\bar X- \frac{n_2 \bar X + n_1 \bar Y}{n_1+n_2})^2  \right)}
{\sigma^2_{MLE}} \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac 1{n_1+n_2}\left(n_1(\frac{n_2 \bar Y- n_2 \bar X}{n_1+n_2})^2+n_2(\frac{n_1 \bar X - n_1 \bar Y}{n_1+n_2})^2  \right)}{\sigma_{MLE}^2 } \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac 1{(n_1+n_2)^3}\left(n_1n_2^2( \bar Y- \bar X)^2+n_1^2n_2(\bar X - \bar Y)^2  \right)}{\sigma_{MLE}^2 } \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac{n_1n_2}{(n_1 + n_2)^2} \left(\bar Y - \bar X\right)^2}
{\sigma_{MLE}^2 } \right)\\
&= (n_1+n_2) \log \left(1+\frac{ \left(\bar Y - \bar X\right)^2}
{(n_1+n_2)\sigma_{MLE}^2 \left(\frac 1{n_1} + \frac 1 {n_2}\right)} \right)\\
&= (n_1+n_2) \log \left(1+\frac{ \left(\bar Y - \bar X\right)^2}
{(n_1+n_2-2)S_p^2 \left(\frac 1{n_1} + \frac 1 {n_2}\right)} \right)\\
&= (n_1+n_2) \log \left(1+\frac{t^2}{(n_1+n_2-2)} \right)
\end{aligned}
$$

\newpage
b.

Let $n_1= n_2=5$. These tests reject $H_0$ at approximate level .05 if they are larger than 3.84. Find exact expressions for $P(T_W \geq 3.84), P(T_S \geq 3.84),$ and $P(T_{LR} \geq 3.84)$ using the fact that $t^2$ has an $F(1,n_1+n_2-2)$ under $H_0$.

Solution:

$$\begin{aligned}
P(T_W \geq 3.84)= P(T_S \geq 3.84) &= P\left(\frac{(\bar Y- \bar X)^2}{\hat \sigma^2_{MLE} \left(\frac 1 {5} +\frac 1 {5}\right)} \geq 3.84\right) \\
&= 1-\chi^2_{(1)}(3.84) \\
P(T_{LR} \geq 3.84) &= P\left((5+5) \log \left(1+\frac{t^2}{(5+5-2)} \right) \geq 3.84\right) \\
&= P\left(1+t^2/8  \geq \exp(0.384)\right)\\
&= P\left(t^2  \geq 8(\exp(0.384)-1)\right)\\
&= 1-F_{8\exp(0.384)-8}(1,8)
\end{aligned}$$

\newpage
## 3.9

Suppose that $Y_1,\dots, Y_n$ are independently distributed as Poisson random variables with means $\lambda_1, \dots, \lambda_n$, respectively. Thus, $P(Y_i=y) = f(y;\lambda_i) =\frac{\lambda_i^y e^{-\lambda_i}}{y!}I(y \in \mathbb N)$. Show that the score statistic for testing $H_0: \lambda_1 = \lambda_2 = \dots = \lambda_n=\lambda$ (i.e., that the $Y_i$ â€™s all have the same distribution) is given by $T_S = \sum_{i=1}^n \frac{(Y_i-\bar Y)^2}{\bar Y}$.

Solution:

$$\begin{aligned}
\mathcal L(\boldsymbol \lambda) &=\prod_{i=1}^n \frac{\lambda_i^{y_i} e^{-\lambda_i}}{y_i!} \\
\ell (\boldsymbol \lambda) &=\sum_{i=1}^n y_i \log \lambda_i -\lambda_i -\log(y_i!) \\
S_i (\boldsymbol \lambda) &= \frac {y_i} {\lambda_i}-1 \\
I_T (\boldsymbol \lambda) &= diag\left(-E\left(\frac{-y_i}{\lambda_i^2}\right)\right) = diag(1/\lambda_i) \\
I_T^{-1} (\boldsymbol \lambda)&= diag(\lambda_i)
\end{aligned}$$

$$\begin{aligned}
0 &=S_i (\boldsymbol \lambda) = \frac {y_i} {\lambda_i}-1 \\
\implies \hat \lambda_{i_{MLE}} &=y_i~~~,~~~~ \hat \lambda_{RMLE} =\bar y
\end{aligned}$$

$$\begin{aligned}
T_S &= \mathbf {S(\boldsymbol{\hat \lambda}_{RMLE})^T I_T^{-1} (\boldsymbol {\hat \lambda_{RMLE}})S(\boldsymbol{\hat \lambda}_{RMLE})}\\
&= \left\{\frac {Y_i} {\bar Y}-1 \right\}diag(\bar Y)\left\{\frac {Y_i} {\bar Y}-1 \right\}\\
&= \sum_{i=1}^n \bar Y\left(\frac {Y_i} {\bar Y}-1 \right)^2=\sum_{i=1}^n \bar Y\left(\frac {Y_i - \bar Y} {\bar Y}\right)^2\\
&= \sum_{i=1}^n \frac {(Y_i-\bar Y)^2} {\bar Y}\\
\end{aligned}$$

\newpage
## 3.12

Show that the $T_W$, $T_S$ and $T_{LR}$ statistics defined in Example 3.2 (p. 129) are asymptotically equivalent under $H_0$ by showing their differences converge to 0 in probability.

Solution:

$$\begin{aligned}
\ell (\hat p) &= \ell(p_0) + \ell'(p_0)(\hat p - p_0) +\frac 1 2 \ell^{2}(p_0)(\hat p - p_0)^2+ \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \\
\implies T_{LR} &=-2\left[\ell (\hat p_0) - \ell (\hat p) \right] \\
&=-2\left[\ell (\hat p_0) - \ell(p_0) - \ell'(p_0)(\hat p - p_0) -\frac 1 2 \ell^{2}(p_0)(\hat p - p_0)^2- \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \right] \\
&=2\left[S(p_0)(\hat p - p_0) -\frac 1 2 I_T(p_0)(\hat p - p_0)^2+ \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \right] \\
&=2\left[\frac{n \hat p - n p_0}{p_0 (1-p_0)}(\hat p -p_0)-\frac 1 2 \left(\frac{n \hat p}{p_0^2}+ \frac{n-n\hat p}{(1-p_0)^2}\right)(\hat p - p_0)^2+ \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \right] \\
&=n (\hat p -p_0)^2\left[\frac{2}{p_0 (1-p_0)}-\left(\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}\right)+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_o) \right] \\
\implies T_W- T_{LR} &= \frac{n(\hat p- p_0)^2}{\hat p (1-\hat p)} -n (\hat p -p_0)^2\left[\frac{2}{p_0 (1-p_0)}-\left(\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}\right)+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_o) \right] \\
&= n(\hat p- p_0)^2\left[\frac 1 {\hat p (1-\hat p)}-\frac{2}{p_0 (1-p_0)}+\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_o) \right]
\end{aligned}$$

Note that $n(\hat p- p_0)^2 \overset d \to \chi^2_{something}$ and,

$$\begin{aligned}
&\lim_{n \to \infty}\left[\frac 1 {\hat p (1-\hat p)}-\frac{2}{p_0 (1-p_0)}+\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_0) \right] \\
&=\frac 1 {p_0 (1-p_0)}-\frac{2}{p_0 (1-p_0)}+\frac{p_0}{p_0^2}+ \frac{1-p_0}{(1-p_0)^2}+0\\
&=-\frac{1}{p_0 (1-p_0)}+\frac{1}{p_0}+ \frac{1}{1-p_0}\\
&= -\frac{1}{p_0 (1-p_0)}+\frac{1-p_0+p_0}{p_0(1-p_0)}\\
&= 0
\end{aligned}
$$

Therefore, by Slutsky's theorem $T_w - T_{LR} \overset P \to 0$.

$$\begin{aligned}
T_W - T_S &= \frac{n(\hat p- p_0)^2}{\hat p (1-\hat p)} - \frac{n(\hat p- p_0)^2}{p_0 (1-p_0)}\\
&= n \left(\frac{p_0 (1-p_0)(\hat p- p_0)^2-\hat p (1-\hat p)(\hat p- p_0)^2}{\hat p p_0(1-\hat p) (1-p_0)} \right)\\
&= n \left(\frac{[(p_0-p_0^2)-(\hat p-\hat p^2)](\hat p- p_0)^2}{\hat p p_0(1-\hat p) (1-p_0)} \right)\\
&= \left[\sqrt n (\hat p- p_0) \right]^2\left(\frac{p_0 -\hat p + \hat p^2-p_0^2}{\hat p p_0(1-\hat p) (1-p_0)} \right)\\
\text{Note that } &p_0 - \hat p \overset p \to 0 \text{ by WLLN, so}
\left(\frac{p_0 -\hat p + \hat p^2-p_0^2}{\hat p p_0(1-\hat p) (1-p_0)} \right) \overset p \to 0\\
\text{and, }&\left[\sqrt n (\hat p- p_0) \right]^2 \overset d \to \chi^2_{something} \text{ by CLT} \\
\implies T_W - T_S &\overset P \to 0 \text{ by Slutsky's Theorem}
\end{aligned}$$

\newpage
## 3.16
**Derive the score statistic $T_S$ in (3.21, p. 148)**

$$\begin{aligned}
logit(p_{1+2(j-1)}) = \beta_j + \beta_{k+1} ~~&,~~logit(p_{2+2(j-1)}) = \beta_j\\
p_{1+2(j-1)} =  \frac{\exp(\beta_j + \beta_{k+1})}{1+\exp(\beta_j + \beta_{k+1})}~~
&,~~p_{2+2(j-1)} =  \frac{\exp(\beta_j)}{1+\exp(\beta_j)} \\
H_0: \beta_{k+1} = 0
\end{aligned}$$

$$\begin{aligned}
\mathcal L(\boldsymbol{\beta}_j, \beta_{k+1}) &= \prod_{j=1}^k \left(p_{1+2(j-1)}\right)^{a_j}
\left(1-p_{1+2(j-1)}\right)^{c_j}
\left(p_{2+2(j-1)}\right)^{b_j}
\left(1-p_{2+2(j-1)}\right)^{d_j} \\
&= \prod_{j=1}^k \left(\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{a_j}
\left(1-\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{c_j}\\
&~~~~~~~~~~~~~~~ \times
\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)^{b_j} 
\left(1-\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)^{d_j} \\
&= \prod_{j=1}^k \left(\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{a_j}
\left(\frac{1}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{c_j}\\
&~~~~~~~~~~~~~~~ \times
\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)^{b_j}
\left(\frac{1}{1+\exp(\boldsymbol{\beta}_j)}\right)^{d_j} \\
\ell(\boldsymbol{\beta}_j, \beta_{k+1})&= \sum_{j=1}^k
a_j \log \left(\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
-c_j\log\left({1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)\\
&~~~~~~~~~~~~~~~ +
b_j\log\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)
-d_j\log\left({1+\exp(\boldsymbol{\beta}_j)}\right) \\
S(\boldsymbol{\beta}_j, \beta_{k+1})&= \begin{pmatrix}
\left[
a_j \left(1- \frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-c_j\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)+
b_j\left(1-\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)-
d_j\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)\right]_{\substack{j^{th}~ entry~ of \\ k x 1 ~vector}} \\
\sum_{j=1}^k
a_j \left(1- \frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-c_j\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[
a_j +b_j  -(a_j + c_j)\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-(b_j+d_j)\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)\right]_{\substack{j^{th}~ entry~ of \\ k x 1 ~vector}} \\
\sum_{j=1}^k
a_j-(a_j+c_j)\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right) \right]_{\substack{j^{th}~ entry~ of \\ k x 1 ~vector}} \\
\sum_{j=1}^k
a_j-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
\end{pmatrix}
\end{aligned}$$

$$\begin{aligned}
I_T (\boldsymbol{\beta}_j, \beta_{k+1} ) &= - \begin{pmatrix}
diag \left\{-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1}))^2}\right)-m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)\right\}_{k\times k} 
&  \left\{-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1}))^2}\right)\right\}_{k\times 1}\\
\left\{-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1}))^2}\right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \beta_{k+1}))^2}\right) \right\}_{1\times 1}
\end{pmatrix}
\end{aligned}$$

Note, under the restricted MLE, $\beta_{k+1} = 0$ implies
$$ \begin{aligned}
p_j&=\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j)}= \frac{b_{j}}{c_j}= \frac{n_{1j}}{t_j}\\
1-p_j&= \frac{n_{2j}}{t_j} = 1-\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j)}=\frac{1}{(1+\exp(\boldsymbol{\beta}_j)}\\
\implies p_j(1-p_j) &= \frac{n_{1j}n_{2j}}{t_j^2} 
= \frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}
\end{aligned}$$

Thus, 
$$\begin{aligned}
\overset \sim I_T (\boldsymbol{\beta}_j, \beta_{k+1} ) &= \begin{pmatrix}
diag \left\{m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)+m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)\right\}_{k\times k} 
&  \left\{m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)\right\}_{k\times 1}\\
\left\{m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j ))^2}\right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j))^2}\right) \right\}_{1\times 1}
\end{pmatrix} \\
&= \begin{pmatrix}
diag\left\{\frac{(m_{1j}+ m_{2j})n_{1j}n_{2j}}{t_j^2} \right\}_{k\times k} 
&  \left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{k\times 1}\\
\left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right) \right\}_{1\times 1}
\end{pmatrix} \\
&= \begin{pmatrix}
diag\left\{\frac{t_jn_{1j}n_{2j}}{t_j^2} \right\}_{k\times k} 
&  \left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{k\times 1}\\
\left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right) \right\}_{1\times 1}
\end{pmatrix} \\
\implies \overset \sim I_{T,22}^{-1} &= [\overset \sim I_{T,22} -\overset \sim I_{T,21} \overset \sim I_{T,11}^{-1} \overset \sim I_{T,12} ]^{-1} \\
&= \left(\sum_{j=1}^k
\frac{m_{1j}n_{1j}n_{2j}}{t_j^2} - \left(\frac{m_{1j}n_{1j}n_{2j}}{t_j^2}\right)^2\frac{t_j}{n_{1j}n_{2j}}\right)^{-1}\\
&= \left(\sum_{j=1}^k
\frac{m_{1j}t_jn_{1j}^2n_{2j}^2-m_{1j}^2n_{1j}^2n_{2j}^2}{t_j^3n_{1j}n_{2j}}\right)^{-1}\\
&= \left(\sum_{j=1}^k
\frac{m_{1j}(t_j-m_{1j})n_{1j}n_{2j}}{t_j^3}\right)^{-1}\\
&=\frac{1}{\sum_{j=1}^km_{1j}m_{2j}n_{1j}n_{2j}/t_j^3}\\
\end{aligned}
$$


Now, to compute the score under the null hypothesis,
$$\begin{aligned}
\overset \sim S(\beta_j, \beta_{k+1})&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)
-m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right) \right]_{k \times 1} \\
\sum_{j=1}^k
a_j-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{n_{1j}}{t_j}\right)
-m_{2j}\left(\frac{n_{1j}}{t_j}\right) \right]_{k \times 1} \\
\sum_{j=1}^k
a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{n_{1j}}{t_j}\right)
-m_{2j}\left(\frac{n_{1j}}{t_j}\right) \right]_{k \times 1} \\
\sum_{j=1}^k a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right)
\end{pmatrix} \\
&= \begin{pmatrix}0 \\
\sum_{j=1}^k a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right) \end{pmatrix} \\
\implies \overset \sim S(\beta_j, \beta_{k+1})^T \overset \sim S(\beta_j, \beta_{k+1})&=
\left[\sum_{j=1}^k a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right)\right]^2
\end{aligned}$$

And the score statistic follows (matching Wikipedia, not the book **OUR CLASS CONCLUDED TEXTBOOK HAS ERROR FROM A PAPER THAT HAD A TYPO**):

$$\begin{aligned}
T_S &= \overset \sim S(\beta_j, \beta_{k+1})^T [\overset \sim I_{T,22} -\overset \sim I_{T,21} \overset \sim I_{T,11}^{-1} \overset \sim I_{T,12} ]^{-1}\overset \sim S(\beta_j, \beta_{k+1})\\
&=\frac{\left[\sum_{j=1}^k a_j-m_{1j}n_{1j}/t_j\right]^2}{\sum_{j=1}^km_{1j}m_{2j}n_{1j}n_{2j}/t_j^3}
\end{aligned}$$


## 3.18

Consider having two independent iid samples, the first with a $normal(\mu_1,1)$ distribution and sample size $n_1$, the second with a $normal(\mu_2,1)$ distribution and sample size $n_2$. For $H_0: \mu_1= \mu_2$ versus $H_a: \mu_1 < \mu_2$, find $T_{LR}$ and the testing procedure at $\alpha = 0.05$.

Solution:

Note that under $H_a$ the maximum likelihood estimators are
the usual ones if $\bar Y_1 \leq \bar Y_2$, but $\hat \mu_1 = \hat \mu_2 = (n_1 \bar Y_1 +n_2 \bar Y _2)/(n_1+n_2)$ if $\bar Y_1 > \bar Y_2$. Also, note that $P(2,2) = 1/2$ in (3.23, p. 152) for this case since the probability is 1/2 that the restricted estimators are the usual sample means with $l=2$ distinct values.

$$\begin{aligned}
\mathcal L(\mu_1,\mu_2) &= \prod_{i=1}^{n_1}\frac{1}{\sqrt{2 \pi}} e ^{-\frac 1 2 (y_{i1}- \mu_1)^2}\prod_{j=1}^{n_2}\frac{1}{\sqrt{2 \pi}} e ^{-\frac 1 2 (y_{j2}- \mu_2)^2} \\
\ell(\mu_1,\mu_2) &= C -\frac 1 2\sum_{i=1}^{n_1} (y_{i1}- \mu_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \mu_2)^2\\
T_{LR} &= -2 (\ell (\hat \theta_{RMLE})-\ell (\hat \theta_{MLE})) \\
&=\begin{cases} -2 \bigg(\left[-\frac 1 2\sum_{i=1}^{n_1} (y_{i1}- \bar y_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \bar y_2)^2\right] \\
~~~~~~~~~~~~~~-\left[-\frac 1 2\sum_{i=1}^{n_1} (y_{i1}- \bar y_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \bar y_2)^2\right] \bigg), & \bar Y_1 \leq \bar Y_2\\
-2 \bigg(\left[-\frac 1 2\sum_{i=1}^{n_1} \left(y_{i1}-  \frac{(n_1 \bar Y_1 +n_2 \bar Y _2)}{(n_1+n_2)}\right)^2-\frac 1 2\sum_{j=1}^{n_2} \left(y_{j2}- \frac{(n_1 \bar Y_1 +n_2 \bar Y _2)}{(n_1+n_2)}\right)^2\right]\\
~~~~~~~~~~~~~~-\left[-\frac 1 2\sum_{i=1}^{n_1} y_{i1}- \bar y_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \bar y_2)^2\right] \bigg) , &\bar Y_1 > \bar Y_2 \end{cases} \\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
\sum_{i=1}^{n_1} \left(y_{i1}-  \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2+\sum_{j=1}^{n_2} \left(y_{j2}- \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2\\
~~~~~~~~~~~~~~-\sum_{i=1}^{n_1} (y_{i1}^2- 2 y_{i1}\bar y_1 + \bar y_1^2)-\sum_{j=1}^{n_2} (y_{j2}^2-2y_{j2}\bar Y_2 +\bar Y_2^2) , &\bar Y_1 > \bar Y_2 \end{cases}
\end{aligned}$$
$$\begin{aligned}
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
\sum_{i=1}^{n_1} y_{i1}^2-  2n_1\bar Y_1 \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}+n_1\left(\frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2+\sum_{j=1}^{n_2} y_{j2}^2-2n_2 \bar Y_2 \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2} + n_2 \left(\frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2\\
~~~~~~~~~~~~~~-\sum_{i=1}^{n_1} y_{i1}^2 + 2 n_1 \bar Y_1^2 - n_1\bar Y_1^2-\sum_{j=1}^{n_2} y_{j2}^2+2n_2\bar Y_2^2 -n_2\bar Y_2^2 , &\bar Y_1 > \bar Y_2 \end{cases} \\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
-  2\frac{(n_1 \bar Y_1 +n_2 \bar Y _2)^2}{n_1+n_2}+(n_1+n_2)\left(\frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2+  n_1 \bar Y_1^2 +n_2\bar Y_2^2 , &\bar Y_1 > \bar Y_2 \end{cases} \\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
n_1 \bar Y_1^2 +n_2\bar Y_2^2 -  \frac{(n_1 \bar Y_1 +n_2 \bar Y _2)^2}{n_1+n_2}, &\bar Y_1 > \bar Y_2
\end{cases}\\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \frac{(n_1+n_2)(n_1 \bar Y_1^2 +n_2\bar Y_2^2) -(n_1 \bar Y_1 +n_2 \bar Y _2)^2}{n_1+n_2}, &\bar Y_1 > \bar Y_2 \end{cases} \\
 &=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \frac{(n_1^2 \bar Y_1^2 + n_1n_2 \bar Y_1^2 +n_1n_2 \bar Y_2^2 +n_2^2\bar Y_2^2) -(n_1^2 \bar Y_1^2 +2n_1n_2 \bar Y_1 \bar Y_2 +n_2^2 \bar Y _2^2)}{n_1+n_2}, &\bar Y_1 > \bar Y_2 \end{cases} \\
  &=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \left(\frac{n_1n_2}{n_1+n_2}\right)(\bar Y_1^2 -2 \bar Y_1 \bar Y_2 + \bar Y_2^2), &\bar Y_1 > \bar Y_2 \end{cases} \\
  &=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \left(\frac{n_1n_2}{n_1+n_2}\right)(\bar Y_1-\bar Y_2)^2, &\bar Y_1 > \bar Y_2 \end{cases} 
\end{aligned}$$


From 3.6.1a (p. 151), the $T_{LR}$ follows a non-chi-square distribution, $\bar \chi^2_{k=2}$. Note, that the following page states $P(\bar \chi^2_{k=2} \geq C) = \frac 1 2 I(C=0) + \frac 1 2P(\chi^2_{k=2} \geq C)$

So I need to find the value $C$ such that:

$$.05 = \alpha =P(\bar \chi^2_{k=2} \geq C) = \frac 1 2 I(C=0) + \frac 1 2P(\chi^2_{2} \geq C)$$
Note that $C \neq 0$, because if it were, we would get $1=  P(\bar \chi^2_{k=2} \geq C)$.

Thus, I am left with, 
$$ \begin{aligned}
.05 &=  \frac 1 2P(\chi^2_{2} \geq C) \\
\implies .1 &= P(\chi^2_{2} \geq C) \\
\implies C &= 4.60517 &\text{by r command qchisq(.1, df = 2, lower.tail = F)}
\end{aligned}$$

Therefore the testing procedure is to reject $H_0$ if $\bar Y_1> Y_2$ AND $\left(\frac{n_1n_2}{n_1+n_2}\right)(\bar Y_1-\bar Y_2)^2 > 4.60517$.
