% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Selected Solutions to Boos and Stefanski},
  pdfauthor={Michael Throolin},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Selected Solutions to Boos and Stefanski}
\author{Michael Throolin}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, breakable, frame hidden, boxrule=0pt, interior hidden, enhanced, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

These are solutions of problems given from Essential Statistical
Inference: Theory and Methods (Boos and Stefanski 2013). The solutions
were written by me, with occasional help from fellow classmates during
our coursework at the University of Utah. We covered chapters 1 through
8, and these are the solutions to the problems that came from the text.

I am not affiliated with the authors of the book. The solutions are for
self-study purposes only. Be aware that some of these solutions may not
have been peer-reviewed or verified and as such may be incomplete or
incorrect.

\bookmarksetup{startatroot}

\hypertarget{chapter-1-roles-of-modeling-in-statistical-inference}{%
\chapter{Chapter 1: Roles of Modeling in Statistical
Inference}\label{chapter-1-roles-of-modeling-in-statistical-inference}}

\hypertarget{suppose-that-y_1-cdots-y_n-are-iid-poissonlambda-and-that-hatlambda-bary.-define-sigma2_hatlambda-varhatlambda.-consider-the-following-two-estimators-of-sigma2_hatlambda-hatsigma2_hatlambda-frachatlambdantildesigma2_hatlambda-fracs2n}{%
\section{\texorpdfstring{1.12. Suppose that \(Y_1, \cdots, Y_n\) are iid
\(Poisson(\lambda)\) and that \(\hat{\lambda} = \bar{Y}\). Define
\(\sigma^2_{\hat{\lambda}} = var(\hat{\lambda})\). Consider the
following two estimators of \(\sigma^2_{\hat{\lambda}}\) :
\[\hat{\sigma}^2_{\hat{\lambda}} = \frac{\hat{\lambda}}{n}~~,~~\tilde{\sigma}^2_{\hat{\lambda}} = \frac{s^2}{n}\]}{1.12. Suppose that Y\_1, \textbackslash cdots, Y\_n are iid Poisson(\textbackslash lambda) and that \textbackslash hat\{\textbackslash lambda\} = \textbackslash bar\{Y\}. Define \textbackslash sigma\^{}2\_\{\textbackslash hat\{\textbackslash lambda\}\} = var(\textbackslash hat\{\textbackslash lambda\}). Consider the following two estimators of \textbackslash sigma\^{}2\_\{\textbackslash hat\{\textbackslash lambda\}\} : \textbackslash hat\{\textbackslash sigma\}\^{}2\_\{\textbackslash hat\{\textbackslash lambda\}\} = \textbackslash frac\{\textbackslash hat\{\textbackslash lambda\}\}\{n\}\textasciitilde\textasciitilde,\textasciitilde\textasciitilde\textbackslash tilde\{\textbackslash sigma\}\^{}2\_\{\textbackslash hat\{\textbackslash lambda\}\} = \textbackslash frac\{s\^{}2\}\{n\}}}\label{suppose-that-y_1-cdots-y_n-are-iid-poissonlambda-and-that-hatlambda-bary.-define-sigma2_hatlambda-varhatlambda.-consider-the-following-two-estimators-of-sigma2_hatlambda-hatsigma2_hatlambda-frachatlambdantildesigma2_hatlambda-fracs2n}}

\hypertarget{a.-when-the-poisson-model-holds-are-both-estimators-consistent}{%
\subsection{a. When the Poisson model holds, are both estimators
consistent?}\label{a.-when-the-poisson-model-holds-are-both-estimators-consistent}}

\[
\begin{aligned}
n\hat{\sigma}^2_{\hat{\lambda}} &= n\left(\frac{\hat{\lambda}}{n} \right) = \bar{Y} \overset{P}{\to} \lambda &\text{ by weak law of large numbers} \\
n{\sigma}^2_{{\lambda}} &= n \, var(\hat{\lambda}) =n\,var(\bar{Y}) \overset{P}{\to} n\frac{\lambda}{n} = \lambda & \overset{\text{ by Central Limit Theorem}}{\text{and } Y \sim POIS(\lambda)} \\
\end{aligned}
\]

Thus \(n\hat{\sigma}^2_{\hat{\lambda}}\) is a consistent estimator of
\(n{\sigma}^2_{\hat{\lambda}}\).

Now for the other estimator,

Let \(\epsilon > 0\),

\[
\begin{aligned}
\lim_{n \to \infty}Pr(|n\tilde{\sigma}^2_{\hat{\lambda}} - n\sigma^2_{\hat{\lambda}}| \geq \epsilon)
&= \lim_{n \to \infty}Pr(|n \frac{s^2}n - n\sigma^2_{\hat{\lambda}}| \geq \epsilon) \\
&= \lim_{n \to \infty}Pr(|s^2 - n \, var(\hat{\lambda}) | \geq \epsilon) \\
&= \lim_{n \to \infty}Pr(|s^2 - n \, var(\bar{Y}) | \geq \epsilon) \\
&=\lim_{n \to \infty}Pr(|s^2 - n (\lambda/n)| \geq \epsilon) \\
&= \lim_{n \to \infty}Pr(|s^2 - \lambda | \geq \epsilon) \\
&\leq \frac{E[s^2-\lambda]^2}{\epsilon^2} & \overset{\text{by Chebychev's Inequality}}{\text{(see page 233 Cassella-Berger )}} \\ \\
&= \frac{Var(s^2)}{\epsilon^2}
\end{aligned}
\]

Note that \(lim_{n \to \infty}Var(s^2) = 0\) under the Poisson model
since the kurtosis is not a function of n.~Thus
\(\lim_{n \to \infty}Pr(|n\tilde{\sigma}^2_{\hat{\lambda}} - n\sigma^2_{\hat{\lambda}}| \geq \epsilon) = 0,\)
thus, by definition, \(n\tilde{\sigma}^2_{\hat{\lambda}}\) is a
consistent estimator of \(n\sigma^2_{\hat{\lambda}}\).

\hypertarget{b.-when-the-poisson-model-holds-give-the-asymptotic-relative-efficiency-of-the-two-estimators.-actually-here-you-can-compare-exact-variances-since-the-estimators-are-so-simple.}{%
\subsection{b. When the Poisson model holds, give the asymptotic
relative efficiency of the two estimators. (Actually here you can
compare exact variances since the estimators are so
simple.)}\label{b.-when-the-poisson-model-holds-give-the-asymptotic-relative-efficiency-of-the-two-estimators.-actually-here-you-can-compare-exact-variances-since-the-estimators-are-so-simple.}}

\[
\begin{aligned}
RE(\hat{\sigma}^2_{\hat{\lambda}},\tilde{\sigma}^2_{\hat{\lambda}}) &= \lim_{n \to \infty} VAR(\frac{\hat{\lambda}}{n})/VAR(\frac{s^2}{n}) \\
&= \lim_{n \to \infty} Var(\hat{\lambda})/Var({s^2}) \\
&= \lim_{n \to \infty}\frac{Var(\bar{Y})}{[Var(Y_1)]^2[\frac{2}{n-1}+\frac{Kurt -3}{n}]} \\
&= \lim_{n \to \infty}\frac{\lambda/n}{\lambda^2\left[\frac{2}{n-1}+\frac{1/\lambda}{n}\right]} \\
&= \lim_{n \to \infty}\frac{1}{n\lambda\left[\frac{2}{n-1}+\frac{1/\lambda}{n}\right]} \\
&= \lim_{n \to \infty}\frac{1}{\left[\frac{2n\lambda}{n-1}+1\right]} \\
&= \frac{1}{2\lambda + 1}
\end{aligned}
\]

As the support for \(\lambda\) is \(\mathbb N\), this indicates
\(\hat{\sigma}^2_{\hat{\lambda}}\) is more efficient.

\hypertarget{c.-when-the-poisson-model-does-not-hold-but-assuming-second-moments-exist-are-both-estimators-consistent}{%
\subsection{c.~When the Poisson model does not hold but assuming second
moments exist, are both estimators
consistent?}\label{c.-when-the-poisson-model-does-not-hold-but-assuming-second-moments-exist-are-both-estimators-consistent}}

\(n\tilde{\sigma}^2_{\hat{\lambda}}\) will always be consistent as long
as \(Var(s^2) \overset{P}{\to} 0\).

\(n\hat{\sigma}^2_{\hat{\lambda}}\) would no longer necessarily be a
consistent estimator of \(n{\sigma}^2_{\hat{\lambda}}\). This is because

\[\begin{aligned}
n\hat{\sigma}^2_{\hat{\lambda}} &= n\left(\frac{\hat{\lambda}}{n} \right) = \bar{Y} \overset{P}{\to} \mu &\text{ by weak law of large numbers} \\
n{\sigma}^2_{{\lambda}} &= n \, var(\hat{\lambda}) =n\,var(\bar{Y}) \overset{P}{\to} n\frac{\sigma^2}{n} = \sigma^2 & \text{ by Central Limit Theorem}
\end{aligned}\]

Therefore, for any distribution where the mean does not equal the
variance (such as a standard normal distribution),
\(n{\hat{\sigma}}^2_{\hat{\lambda}}\) will not be a consistent estimator
of \(n{\sigma}^2_{\hat{\lambda}}\).

\newpage

\hypertarget{define-full-model-as-nmu-c2mu2-where-c-is-known.-define-hatmu_wls-as-the-value-that-minimizes}{%
\section{\texorpdfstring{1.13 Define full model as \(N(\mu, c^2\mu^2)\),
where \(c\) is known. Define \(\hat{\mu}_{WLS}\) as the value that
minimizes}{1.13 Define full model as N(\textbackslash mu, c\^{}2\textbackslash mu\^{}2), where c is known. Define \textbackslash hat\{\textbackslash mu\}\_\{WLS\} as the value that minimizes}}\label{define-full-model-as-nmu-c2mu2-where-c-is-known.-define-hatmu_wls-as-the-value-that-minimizes}}

\[\rho(\mu) = \sum_{i=1}^n \frac{(Y_i- \mu)^2}{c^2\mu^2}\].

\hypertarget{a-show-that-mu_wls-converges-in-probability-but-that-its-limit-is-not-mu.}{%
\subsection{\texorpdfstring{a) Show that \(\mu_{WLS}\) converges in
probability, but that its limit is not
\(\mu\).}{a) Show that \textbackslash mu\_\{WLS\} converges in probability, but that its limit is not \textbackslash mu.}}\label{a-show-that-mu_wls-converges-in-probability-but-that-its-limit-is-not-mu.}}

\[
\begin{aligned}
0 &= \frac{\partial}{\partial \mu} \left\{\rho(\mu)\right\} 
= \frac{\partial }{\partial \mu} \left\{ \sum_{i=1}^n \frac{(Y_i- \mu)^2}{c^2\mu^2} \right\} = \frac{\partial }{\partial \mu} \left\{ \sum_{i=1}^n \frac{Y_i^2- 2Y_i\mu +\mu^2}{c^2\mu^2} \right\} \\
&= \frac{\partial }{\partial \mu} \left\{ \frac{nm_2'- 2n\bar{Y}\mu +n\mu^2}{c^2\mu^2} \right\} \\
&=\frac{c^2\mu^2(-2n\bar{Y}+2n\mu)-2c^2\mu(nm_2'- 2n\bar{Y}\mu +n\mu^2)}{c^4\mu^4} \\
&=\frac{\mu(-2n\bar{Y}+2n\mu)-2(nm_2'- 2n\bar{Y}\mu +n\mu^2)}{c^2\mu^3} \\
&=\frac{-2n\bar{Y}\mu+2n\mu^2-2nm_2'+ 4n\bar{Y}\mu -2n\mu^2}{c^2\mu^3} \\
&=\frac{2n\bar{Y}\mu -2nm_2'}{c^2\mu^3} = \frac{2n(\bar{Y}\mu -m_2')}{c^2\mu^3}\\
\implies \mu &= \frac{m_2'}{\bar{Y}}
\end{aligned}
\]

\[
\begin{aligned}
\frac{\partial^2}{\partial^2 \mu} \left\{\rho(\mu)\right\}_{\mu=\frac{m_2'}{\bar{Y}}} &= \frac{\partial^2 }{\partial^2 \mu} \left\{ \sum_{i=1}^n \frac{(Y_i- \mu)^2}{c^2\mu^2} \right\}_{\mu = \frac{m_2'}{\bar{Y}}} 
=\frac{\partial}{\partial \mu} \left\{ \frac{2n(\bar{Y}\mu -m_2')}{c^2\mu^3} \right\}_{\mu = \frac{m_2'}{\bar{Y}}} \\
&=\left\{ \frac{c^2\mu^3(2n\bar{Y})-3c^2\mu^2 2n(\bar{Y}\mu -m_2')}{c^4\mu^6} \right\}_{\mu = \frac{m_2'}{\bar{Y}}} \\
&=\left\{ \frac{2n\bar{Y}\mu-6n\bar{Y}\mu +6nm_2'}{c^2\mu^4} \right\}_{\mu = \frac{m_2'}{\bar{Y}}} 
=\left\{ \frac{-4n\bar{Y}\mu+6nm_2'}{c^2\mu^4} \right\}_{\mu = \frac{m_2'}{\bar{Y}}} \\
&=\frac{-4n\bar{Y}\frac{m_2'}{\bar{Y}}+6nm_2'}{c^2\mu^4} =\frac{2nm_2'}{c^2\mu^4}
> 0.
\end{aligned}
\]

Thus \(\hat{\mu}_{WLS} = \frac{m_2'}{\bar{Y}}\). By the Strong Law of
Large Numbers, \(m_2' \to E(m_2')\) and \(\bar{Y} \to \mu\). Note that,

\[
\begin{aligned}E(m_2') &= E\left(\frac{1}{n}\sum_{i=1}^n Y_i^2\right)=\frac{1}{n}\sum_{i=1}^nE\left( Y_i^2\right) \\ &=\frac{1}{n}\sum_{i=1}^n \left(Var(Y_i) +[E(Y_i)]^2 \right) =\frac{1}{n}\sum_{i=1}^n \left( c^2\mu^2+\mu^2 \right)\\
&=c^2\mu^2+\mu^2 = (c^2+1)\mu^2
\end{aligned}
\]

Thus, by the continuous mapping theorem,
\(\hat{\mu}_{WLS}= \frac{m_2'}{\bar{Y}} \overset{P}{\to} = \frac{(c^2+1)\mu^2}{\mu}=(c^2+1)\mu\)

\hypertarget{b-find-erho-mu-and-show-that-it-does-not-equal-zero.}{%
\subsection{\texorpdfstring{b) Find \(E[\rho ' (\mu)]\) and show that it
does not equal
zero.}{b) Find E{[}\textbackslash rho \textquotesingle{} (\textbackslash mu){]} and show that it does not equal zero.}}\label{b-find-erho-mu-and-show-that-it-does-not-equal-zero.}}

\[
\begin{aligned}
E[\rho ' (\mu)] &= E\left(\frac{2n(\bar{Y}\mu -m_2')}{c^2\mu^3} \right) \\
&= \frac{2n}{c^2\mu^3}\left[E(\bar{Y})\mu -E(m_2')\right] \\
&= \frac{2n}{c^2\mu^3}\left[\mu^2 -(c^2+1)\mu^2\right] \\
&= \frac{2n}{c^2\mu^3}\left[-c\mu^2\right] \\
&=\frac{-2n}{\mu} \neq 0.
\end{aligned}
\]

\newpage

\hypertarget{c-find-the-estimator-obtained-by-minimizing-rho_mu-sum_i1n-fracy_i--mu2c2mu2-2logmu.}{%
\subsection{\texorpdfstring{c) Find the estimator obtained by minimizing
\(\rho_*(\mu) = \sum_{i=1}^n \frac{(Y_i- \mu)^2}{c^2\mu^2} + 2log(\mu)\).}{c) Find the estimator obtained by minimizing \textbackslash rho\_*(\textbackslash mu) = \textbackslash sum\_\{i=1\}\^{}n \textbackslash frac\{(Y\_i- \textbackslash mu)\^{}2\}\{c\^{}2\textbackslash mu\^{}2\} + 2log(\textbackslash mu).}}\label{c-find-the-estimator-obtained-by-minimizing-rho_mu-sum_i1n-fracy_i--mu2c2mu2-2logmu.}}

\[\begin{aligned}
0 &= \frac{\partial}{\partial \mu} \left\{\rho_*(\mu)\right\} \\
&= \frac{\partial}{\partial \mu} \left\{\left[ \sum_{i=1}^n \frac{(Y_i- \mu)^2}{c^2\mu^2} \right] + 2n\log(\mu)\right\}\\
&= \frac{2n(\bar{Y}\mu -m_2')}{c^2\mu^3} +\frac{2n}{\mu} \\
&= \frac{2n(\bar{Y}\mu -m_2'+c^2\mu^2)}{c^2\mu^3}\\
\implies \mu &= \frac{-\bar{Y} + \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2} & \overset {\text{other solution cannot exist since }}{\mu > 0}
\end{aligned}
\]

\[
\begin{aligned}
\frac{\partial^2}{\partial^2 \mu} \left\{\rho_*(\mu)\right\}_{\mu = \hat{\mu}}
&=\frac{\partial}{\partial \mu} \left\{ \frac{2n(\bar{Y}\mu -m_2'+c^2\mu^2)}{c^2\mu^3} \right\}_{\mu = \hat{\mu}}
=\frac{\partial}{\partial \mu} 2n\left\{ \frac{\bar{Y}\mu -m_2'+c^2\mu^2}{c^2\mu^3} \right\}_{\mu = \hat{\mu}} \\
&=2n\left\{\frac{c^2\mu^3(\bar{Y}+2c^2\mu)-3c^2\mu^2(\bar{Y}\mu -m_2'+c^2\mu^2)}{c^4\mu^6} \right\}_{\mu = \hat{\mu}} \\
&=2n\left\{\frac{\mu(\bar{Y}+2c^2\mu)-3(\bar{Y}\mu -m_2'+c^2\mu^2)}{c^2\mu^4} \right\}_{\mu = \hat{\mu}} \\
&=2n\left\{\frac{-2\bar{Y}\mu - c^2\mu^2+3m_2'}{c^2\mu^4} \right\}_{\mu = \hat{\mu}} \\
\end{aligned}
\]

We only need to show the numerator to be positive.

\[
\begin{aligned}
&- c^2\mu^2-2\bar{Y}\mu +3m_2' \bigg|_{\mu = \left(\frac{-\bar{Y} + \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2}\right)}\\
&=-c^2\left(\frac{-\bar{Y} + \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2}\right)^2-2\bar{Y}\left(\frac{-\bar{Y} + \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2}\right)+3m_2' \\
&=-c^2\left(\frac{\bar{Y}^2 -2\bar{Y} \sqrt{\bar{Y}^2+4c^2m_2'}+\bar{Y}^2+4c^2m_2'}{4c^4}\right)-2\bar{Y}\left(\frac{-\bar{Y} + \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2}\right)+3m_2' \\
&=\frac{-\bar{Y}^2 +\bar{Y} \sqrt{\bar{Y}^2+4c^2m_2'}-2c^2m_2'}{2c^2}+\left(\frac{2\bar{Y}^2 -2\bar{Y} \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2}\right)+3m_2' \\
&=\frac{\bar{Y}^2+4c^2m_2' -\bar{Y} \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2} \\
&=\frac{\left(\sqrt{\bar{Y}^2+4c^2m_2'} -\bar{Y}\right)\sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2} \\
&>0~\text{because } \sqrt{x^2+a} > x~\forall a>0.
\end{aligned}
\]

Thus
\(\hat{\mu}_{^*{WLS}}=\frac{-\bar{Y} + \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2}\)

\newpage

\hypertarget{d-is-hatmu_wls-a-consistent-estimator-of-mu-if-so-find-its-asymptotic-variance-and-compare-to-hatmu_mle-and-hatmu_mom.}{%
\subsection{\texorpdfstring{d) Is \(\hat{\mu}_{^*{WLS}}\) a consistent
estimator of \(\mu\)? If so, find its asymptotic variance and compare to
\(\hat{\mu}_{MLE}\) and
\(\hat{\mu}_{MOM}\).}{d) Is \textbackslash hat\{\textbackslash mu\}\_\{\^{}*\{WLS\}\} a consistent estimator of \textbackslash mu? If so, find its asymptotic variance and compare to \textbackslash hat\{\textbackslash mu\}\_\{MLE\} and \textbackslash hat\{\textbackslash mu\}\_\{MOM\}.}}\label{d-is-hatmu_wls-a-consistent-estimator-of-mu-if-so-find-its-asymptotic-variance-and-compare-to-hatmu_mle-and-hatmu_mom.}}

By continuous mapping theorem,

\[
\begin{aligned}
\hat{\mu}_{^*{WLS}} =\frac{-\bar{Y} + \sqrt{\bar{Y}^2+4c^2m_2'}}{2c^2} &\overset{P}{\to} \frac{-\mu + \sqrt{\mu^2+4c^2(c^2+1)\mu^2}}{2c^2}\\
&=\frac{-\mu +\mu \sqrt{1+4c^2(c^2+1)}}{2c^2} \\
&=\frac{-\mu +\mu \sqrt{1+4c^2+ 4c^4}}{2c^2} \\
&=\frac{-\mu +\mu \sqrt{(1+2c^2)^2}}{2c^2} \\
&=\frac{-\mu +\mu (1+2c^2)}{2c^2} \\
&=\mu.
\end{aligned}
\]

Thus \(\hat{\mu}_{^*{WLS}}\) is a consistent estimator of \(\mu\).

Note that \(\hat{\mu}_{^*{WLS}} = \hat{\mu}_{MLE}\). Thus,
\(Var(\hat{\mu}_{^*{WLS}}) = Var(\hat{\mu}_{MLE}) = \frac{c^2\mu^2}{n(1+2c^2)}\),
and the relative efficiency between the two is 1. That is,

\(ARE(\hat{\mu}_{^*{WLS}}, \hat{\mu}_{MLE}) =1\) And, according to
(1.17, p.~18) in the book,
\(ARE(\hat{\mu}_{MOM}, \hat{\mu}_{^*WLS}) =1+2c^2\).

Thus \(\hat{\mu}_{MLE}\) and \(\hat{\mu}_{^*{WLS}}\) are equally more
efficient than \(\hat{\mu}_{MOM}\)

\hypertarget{e-find-erho_-mu-and-show-that-it-does-not-equal-zero.}{%
\subsection{\texorpdfstring{e) Find \(E[\rho_* ' (\mu)]\) and show that
it does not equal
zero.}{e) Find E{[}\textbackslash rho\_* \textquotesingle{} (\textbackslash mu){]} and show that it does not equal zero.}}\label{e-find-erho_-mu-and-show-that-it-does-not-equal-zero.}}

\[
\begin{aligned}
E[\rho_* ' (\mu)] &= E\left(\frac{2n(\bar{Y}\mu -m_2'+c^2\mu^2)}{c^2\mu^3}\right) \\
&= \frac{2n}{c^2\mu^3}\left[E(\bar{Y})\mu -E(m_2') +E(c^2\mu^2)\right] \\
&= \frac{2n}{c^2\mu^3}\left[\mu^2 -(c^2+1)\mu^2 +c^2\mu^2\right]\\
&= \frac{2n}{c^2\mu^3}(0) \\
&=0.
\end{aligned}
\]

\newpage

\hypertarget{to-check-on-the-asymptotic-variance-expression-in-1.20-p.-18-generate-1000-samples-of-size-n20-from-the-exponential-density-12exp-y2-that-has-mean-mu2-and-variance4-so-that-in-terms-of-the-model-sigma21.-for-each-sample-calculate-hatmu_mle-in-1.16-p.-17.-then-compute-1.20-p.-18-for-this-exponential-density-and-compare-it-to-n20-times-the-sample-variance-of-the-1000-values-of-hatmu_mle.-repeat-for-n50.}{%
\subsection{\texorpdfstring{1.17. To check on the asymptotic variance
expression in (1.20, p.~18), generate 1000 samples of size \(n=20\) from
the exponential density \((1/2)exp(-y/2)\) that has mean \(\mu=2\) and
variance=4 so that in terms of the model \(\sigma^2=1\). For each
sample, calculate \(\hat{\mu}_{MLE}\) in (1.16, p.~17). Then compute
(1.20, p.~18) for this exponential density and compare it to \(n=20\)
times the sample variance of the 1000 values of \(\hat{\mu}_{MLE}\).
Repeat for
\(n=50\).}{1.17. To check on the asymptotic variance expression in (1.20, p.~18), generate 1000 samples of size n=20 from the exponential density (1/2)exp(-y/2) that has mean \textbackslash mu=2 and variance=4 so that in terms of the model \textbackslash sigma\^{}2=1. For each sample, calculate \textbackslash hat\{\textbackslash mu\}\_\{MLE\} in (1.16, p.~17). Then compute (1.20, p.~18) for this exponential density and compare it to n=20 times the sample variance of the 1000 values of \textbackslash hat\{\textbackslash mu\}\_\{MLE\}. Repeat for n=50.}}\label{to-check-on-the-asymptotic-variance-expression-in-1.20-p.-18-generate-1000-samples-of-size-n20-from-the-exponential-density-12exp-y2-that-has-mean-mu2-and-variance4-so-that-in-terms-of-the-model-sigma21.-for-each-sample-calculate-hatmu_mle-in-1.16-p.-17.-then-compute-1.20-p.-18-for-this-exponential-density-and-compare-it-to-n20-times-the-sample-variance-of-the-1000-values-of-hatmu_mle.-repeat-for-n50.}}

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
& Sample Variance & Asymptotic Variance \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
sample\_size\_20 & 0.2465302 & 0.2888889 \\
sample\_size\_50 & 0.1007942 & 0.1155556 \\
\end{longtable}

The sample variance approaches the asymptotic variance as sample size
increases.

Check my r code below,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#Set constants}
\NormalTok{mu }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{pop\_var }\OtherTok{\textless{}{-}} \DecValTok{4}
\NormalTok{c}\OtherTok{\textless{}{-}} \DecValTok{1}


\NormalTok{skew }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{kurt }\OtherTok{\textless{}{-}} \DecValTok{9}


\CommentTok{\#For sample = 20}
\NormalTok{sample\_size}\OtherTok{\textless{}{-}} \DecValTok{20}

\CommentTok{\#Compute asymptotic variance as defined in book (1.21)}
\NormalTok{a\_var\_20}\OtherTok{\textless{}{-}}\NormalTok{ (c}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{*}\NormalTok{mu}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(sample\_size}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{*}\NormalTok{skew}\SpecialCharTok{+}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{*}\NormalTok{(kurt}\DecValTok{{-}3}\NormalTok{))}

\CommentTok{\#Simulate mu\_mle}
\NormalTok{sample }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{mu\_mle}\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1000}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{)\{}
\NormalTok{    sample[[i]]}\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(}\AttributeTok{n=}\NormalTok{sample\_size,}\AttributeTok{rate =}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    first\_moment }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(sample[[i]])}
\NormalTok{    second\_moment }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample[[i]]}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{sample\_size}
    
\NormalTok{    mu\_mle[i] }\OtherTok{\textless{}{-}}
\NormalTok{      (}\FunctionTok{sqrt}\NormalTok{((first\_moment}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}
              \DecValTok{4}\SpecialCharTok{*}\NormalTok{(c}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\NormalTok{second\_moment)}\SpecialCharTok{{-}}\NormalTok{ first\_moment)}\SpecialCharTok{/}
\NormalTok{      (}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\NormalTok{sample\_var\_20 }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(mu\_mle)}

\NormalTok{sample\_size}\OtherTok{\textless{}{-}} \DecValTok{50}

\CommentTok{\#Compute asymptotic variance as defined in book (1.21)}
\NormalTok{a\_var\_50}\OtherTok{\textless{}{-}}\NormalTok{ (c}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{*}\NormalTok{mu}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(sample\_size}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{*}\NormalTok{skew}\SpecialCharTok{+}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{*}\NormalTok{(kurt}\DecValTok{{-}3}\NormalTok{))}

\CommentTok{\#Simulate mu\_mle}
\NormalTok{sample }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{mu\_mle}\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1000}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{)\{}
\NormalTok{    sample[[i]]}\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(}\AttributeTok{n=}\NormalTok{sample\_size,}\AttributeTok{rate =}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    first\_moment }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(sample[[i]])}
\NormalTok{    second\_moment }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample[[i]]}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{sample\_size}
    
\NormalTok{    mu\_mle[i] }\OtherTok{\textless{}{-}}
\NormalTok{      (}\FunctionTok{sqrt}\NormalTok{((first\_moment}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}
              \DecValTok{4}\SpecialCharTok{*}\NormalTok{(c}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\NormalTok{second\_moment)}\SpecialCharTok{{-}}\NormalTok{ first\_moment)}\SpecialCharTok{/}
\NormalTok{      (}\DecValTok{2}\SpecialCharTok{*}\NormalTok{c}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\NormalTok{sample\_var\_50 }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(mu\_mle)}

\NormalTok{sample\_size\_20 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(sample\_var\_20, a\_var\_20)}
\NormalTok{sample\_size\_50 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(sample\_var\_50, a\_var\_50)}

\FunctionTok{kable}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(sample\_size\_20,sample\_size\_50),}
      \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\FunctionTok{expression}\NormalTok{(}\StringTok{"Sample Variance"}\NormalTok{), }\FunctionTok{expression}\NormalTok{(}\StringTok{"Asymptotic Variance"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\hypertarget{chapter-2-likelihood-construction-and-estimation}{%
\chapter{Chapter 2: Likelihood Construction and
Estimation}\label{chapter-2-likelihood-construction-and-estimation}}

\hypertarget{section}{%
\section{2.1}\label{section}}

Let \(Y_1,\dots, Y_n\) be iid positive random variables such that
\(Y^{(\lambda)}\) is assumed to have a normal\((\mu,\sigma^2)\)
distribution, where

\[ Y^{(\lambda)} =
\begin{cases}
\frac{Y^\lambda-1}{\lambda} &when~ \lambda \neq 0 \\
\log(Y)& when~ \lambda =0.
\end{cases}
\] Derive the log likelihood
\(\ell_n(\mu,\sigma, \lambda | \mathbf{Y})\) of the observed data
\(Y_1,\dots, Y_n\).

Solution:

\[ \begin{aligned}
P\left(\frac{Y^{(\lambda)}-1} \lambda \leq \frac{y^{(\lambda)}-1} \lambda  \right) &= P\left(Y^{(\lambda)}\leq \frac{y^{(\lambda)}-1} \lambda \right)  & \text{for } \lambda \neq0\\
&=  \Phi \left(\frac{\frac{y^{(\lambda)}-1} \lambda  - \mu}{\sigma} \right) \\
\implies f_{\frac{Y^{(\lambda)}-1} \lambda} &= \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\frac{y^{(\lambda)}-1} \lambda  - \mu}{\sigma}\right)^2\right\}\left(\frac{1}{\lambda\sigma}\right)
\end{aligned}
\]

\[ \begin{aligned}
P\left(\log(Y) \leq \log(y\right) &= P\left( Y^{(\lambda)} <\log(y) \right) \text{ for }\lambda =0\\
&= \Phi \left( \frac{\log (y)- \mu}{\sigma} \right)\\
\implies f_{\log(Y)} &= \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\log (y)- \mu}{\sigma}\right)^2\right\}\left(\frac{1}{y\sigma}\right)
\end{aligned}
\] \[
\begin{aligned}
\mathcal{L} (\mu,\sigma |\mathbf{Y}) &= \prod_{i=1}^n \left[\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\frac{y_i^{(\lambda)}-1} \lambda  - \mu}{\sigma}\right)^2\right\}\left(\frac{1}{\lambda\sigma}\right)\right]^{I(\lambda \neq0)} \left[\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\log (y_i)- \mu}{\sigma}\right)^2\right\}\left(\frac{1}{y_i\sigma}\right)\right]^{I(\lambda =0)} \\
&= \left[\frac{1}{\sigma^2\sqrt{2\pi }}\right]^n\prod_{i=1}^n \left[\exp\left\{-\frac{1}{2}\left(\frac{\frac{y_i^{(\lambda)}-1} \lambda  - \mu}{\sigma}\right)^2\right\}\left(\frac{1}{\lambda}\right)\right]^{I(\lambda \neq0)} \left[\exp\left\{-\frac{1}{2}\left(\frac{\log (y_i)- \mu}{\sigma}\right)^2\right\}\left(\frac{1}{y_i}\right)\right]^{I(\lambda =0)} \\
\end{aligned}
\]

\[
\begin{aligned}
\implies \ell (\mu,\sigma |\mathbf{Y}) &=\sum_{i=1}^n\left[ \left( -\frac{1}{2}\left(\frac{\frac{y_i^{(\lambda)-\mu}} \lambda- \mu}{\sigma}\right)^2 - \log(\lambda)\right){I(\lambda \neq0)} +
\left(-\frac{1}{2}\left(\frac{\log (y_i)- \mu}{\sigma}\right)^2-\log(y_i)\right){I(\lambda =0)} \right] \\
&~~~~~~~~~~~~~~~~~~~~~~~+n \log\left(\frac{1}{\sigma^2\sqrt{2\pi}}\right)
\end{aligned}\]

\newpage

\hypertarget{section-1}{%
\section{2.3}\label{section-1}}

Recall the ZIP model

\[
\begin{aligned}
P(Y = 0) &= p+(1-p)e^{-\lambda} \\
P(Y=y) &= (1-p)\frac{\lambda^y e^{-\lambda}}{y!} & y=1,2,...
\end{aligned}
\]

\hypertarget{a}{%
\subsection{a)}\label{a}}

Reparametrize the model by defining
\(\pi \equiv P(Y =0) = p+(1-p)e^{-\lambda}\). Solve for \(p\) in terms
of \(\pi\) and \(\lambda\), then substitute so that the density only
depends on them.

Solution:

\[
\begin{aligned}
\pi &= p+(1-p)e^{-\lambda} \\
&=p (1-e^{-\lambda}) + e^{-\lambda} \\
\implies p &= \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}}
\end{aligned}
\]

\[ \begin{aligned}
P(Y = y) &= \left(p+(1-p)e^{-\lambda}\right)^{I(y=0)}\left((1-p)\frac{\lambda^y e^{-\lambda}}{y!}\right)^{I(y \in \mathbb{N}^+)} \\
&= \left[\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)+ \left(1-\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)\right)e^{-\lambda}\right]^{I(y=0)}
\left[\left(1-\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)} \\
&= \left[\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)+ \left( \frac{1-e^{-\lambda}-\pi+e^{-\lambda}}{1-e^{-\lambda}}\right)e^{-\lambda}\right]^{I(y=0)}
\left[\left( \frac{1-e^{-\lambda}-\pi+e^{-\lambda}}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)}\\
&= \left[\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)+ \left( \frac{1-\pi}{1-e^{-\lambda}}\right)e^{-\lambda}\right]^{I(y=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)} \\
&= \left[\left( \frac{\pi-\pi e^{-\lambda}}{1-e^{-\lambda}}\right)\right]^{I(y=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)} \\
&= \pi^{I(y=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)}
\end{aligned}
\] \newpage

\hypertarget{b}{%
\subsection{b)}\label{b}}

Let \(n_0\) represent the number of samples in an iid sample of size
\(n\). Assuming the complete data is available, show that the likelihood
factors into two pieces and that \(\hat{\pi} = n_0/n\). Also show that
the MLE for \(\lambda\) is the solution to a simple nonlinear equation
involving \(\bar{Y}_+\).

Solution:

\[ \begin{aligned}
\mathcal{L}(\lambda, p | \mathbf{Y}) &= \prod_{i=1}^n\pi^{I(y_i=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y_i \in \mathbb{N}^+)} \\
&= \pi^{n_0}(1-\pi)^{n-n_0}\prod_{Y_i\in \mathbb{N}^+}\frac{\lambda^{y_i} e^{-\lambda}}{y_i! ( 1-e^{-\lambda})}
\end{aligned}
\]

To find \(\hat{\pi}\), we only need to maximize
\(g(\pi) = \pi^{n_0}(1-\pi)^{n-n_0}\).

\[\begin{aligned}
\log g(\pi) &= n_0 \log (\pi) +(n-n_0)\log(1-\pi) \\
\frac{d \log g}{d\pi} &= \frac{n_0}{\pi } -\frac{n-n_0}{1-\pi } \\
\frac{d^2 \log g}{d\pi^2} &=-\frac{n_0}{\pi^2 } - \frac{n-n_0}{(1-\pi)^2}  > 0
\end{aligned}
\]

\[\begin{aligned}
0 =\frac{d \log g}{d\pi} &= \frac{n_0}{\pi } -\frac{n-n_0}{1-\pi }  \\
\implies \frac{\pi}{1-\pi} &= \frac{n_0}{n-n_0} \\
\implies \pi\left(1+ \frac{n_0}{n-n_0}\right) &= \frac{n_0}{n-n_0} \\
\implies \pi &= \frac{n_0}{(n-n_0)\left(1+ \frac{n_0}{n-n_0}\right)} \\
&= \frac{n_0}{n}
\end{aligned}
\]

Thus, \(\hat{\pi} = n_0/n\).

To find \(\hat{\lambda}\), we need to maximize
\(h(\lambda) =\prod_{Y_i\in \mathbb{N}^+}\frac{\lambda^{y_i} e^{-\lambda}}{y_i! ( 1-e^{-\lambda})}\)

\[
\begin{aligned}
\log h(\lambda) &= \sum_{Y_i \in \mathbb{N}^+} y_i\log(\lambda) -\lambda - \log(y_i!) -\log(1-e^{-\lambda}) \\
\implies \frac{d \log h}{d \lambda} &=  (n-n_0) \left(\frac{\bar{y}_+}{\lambda}-1-\frac{ e^{-\lambda}}{1-e^{-\lambda}} \right) \\
0= \frac{d \log h}{d \lambda} &=  (n-n_0) \left(\frac{\bar{y}_+}{\lambda}-1-\frac{ e^{-\lambda}}{1-e^{-\lambda}} \right) \\
&=  \bar{y}_+-\lambda -\frac{ \lambda e^{-\lambda}}{1-e^{-\lambda}}
\end{aligned}
\]

Thus, \(\hat{\lambda}_{MLE}\) will be a solution to the above simple
non-linear equation, which involves \(\bar{y}_+\).

\hypertarget{c}{%
\subsection{c)}\label{c}}

Now consider the truncated or conditional sample consisting of the
\(n- n_0\) nonzero values. Write down the conditional likelihood for
these values and obtain the same equation for \(\hat{\lambda}_{MLE}\) as
in a)

Solution:

\[
\begin{aligned}
P(Y=y |Y>0) &= \frac{P(Y>0 \cap Y=y)}{P(Y>0)} \\
&= \frac{
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)}}{1-\pi}
\\
&= \left[\frac{\lambda^y e^{-\lambda}}{y!(1-e^{-\lambda})}\right]^{I(y \in \mathbb N^+)}
\end{aligned}
\]

Thus, we need to maximize the same function we found in part b), which
will be the solution to a non-simple linear equation involving
\(\bar{y}\).

\newpage

\hypertarget{section-2}{%
\section{2.4}\label{section-2}}

In sampling land areas for counts of an animal species, we obtain an iid
sample of counts \(Y_1, \dots, ,Y_n\), where each \(Y_i\) has a Poisson
distribution with parameter \(\lambda\)

\hypertarget{a-derive-the-maximum-likelihood-estimator-of-lambda.-call-it-hatlambda_mle_1}{%
\subsection{\texorpdfstring{a) Derive the maximum likelihood estimator
of \(\lambda\). Call it
\(\hat{\lambda}_{MLE_1}\)}{a) Derive the maximum likelihood estimator of \textbackslash lambda. Call it \textbackslash hat\{\textbackslash lambda\}\_\{MLE\_1\}}}\label{a-derive-the-maximum-likelihood-estimator-of-lambda.-call-it-hatlambda_mle_1}}

Solution:

\[ \begin{aligned}
\mathcal{L} (\lambda, \mathbf{Y}) &= \prod_{i=1}^n \frac{e^{-\lambda} \lambda^{y_i}}{y_i!} \\
\implies \ell (\lambda, \mathbf{Y}) &= \sum_{i=1}^n -\lambda + y_i\log(\lambda)- \log(y_i!) \\
&=  -n\lambda + n\bar{y}\log(\lambda)- \sum_{i=1}^n \log(y_i!) \\
\implies \frac{d \ell}{d \lambda} &= -n+ \frac{n\bar{y}}{\lambda} \\
\implies \frac{d^2 \ell}{d \lambda^2} &= -\frac{n\bar{y}}{\lambda^2}
 < 0 \text{ because } \bar{y}, \lambda, n >0.
 \end{aligned}
\]

\[ \begin{aligned}
0=\frac{d \ell}{d \lambda} &= -n+ \frac{n\bar{y}}{\hat{\lambda}_{MLE_1}}\\
\implies \hat{\lambda}_{MLE_1}= \bar{y}
\end{aligned}
\]

\hypertarget{b-1}{%
\subsection{b)}\label{b-1}}

For simplicity in quadrat sampling, sometimes only the presence or
absence of a species is recorded. Let \(n_0\) be the number of \(Y_i\)'s
that are zero. Write down the binomial likelihood based only on \(n_0\).
Show that the maximum likelihood estimator of \(\lambda\) based only on
\(n_0\) is \(\hat{\lambda}_{MLE_2}= -\log(n_0/n)\).

Solution:

\[
\begin{aligned}
P(Y=0)&= e^{-\lambda} \\
\implies \mathcal L (\lambda | \mathbf Y) &= {n \choose n_0} (e^{-\lambda})^{n_0}(1-e^{-\lambda})^{n-n_o} \\
\implies \ell (\lambda | \mathbf Y) &=\log{n \choose n_0}- n_0\lambda+(n-n_0)\log(1-e^{-\lambda}) \\
\implies \frac{ d\ell (\lambda | \mathbf Y)}{d \lambda} &= - n_0+\frac{(n-n_0)e^{-\lambda}}{1-e^{-\lambda}} \\
\implies \frac{ d^2 \ell (\lambda | \mathbf Y)}{d \lambda^2} &=  (n-n_0) \frac{-(1-e^{-\lambda})e^{-\lambda} -e^{-\lambda}(e^{-\lambda})}{(1-e^{-\lambda})^2} \\
&=  \frac{-e^{-\lambda}(n-n_0)}{(1-e^{-\lambda})^2} < 0 ~\text{since}~ n_0<n
\end{aligned}
\] \[
\begin{aligned}
0= \frac{ d\ell (\lambda | \mathbf Y)}{d \lambda} &= - n_0+\frac{(n-n_0)e^{-\lambda}}{1-e^{-\lambda}} \\
\implies e^{-\lambda} &= \frac{n_0}{n-n_0}(1-e^{-\lambda}) \\
\implies e^{-\lambda} \left(1+ \frac{n_0}{n-n_0} \right) &= \frac{n_0}{n-n_0} \\
\implies e^{-\lambda}  &= \frac{n_0}{n-n_0}\left(\frac{n-n_0}{n} \right) \\
\implies \hat{\lambda}_{MLE_2} &= -\log(n_0/n)
\end{aligned}
\]

\hypertarget{c-1}{%
\subsection{c)}\label{c-1}}

Use the delta theorem from Ch. 1 to show that the asymptotic relative
efficiency of \(\hat{\lambda}_{MLE_1}\) to \(\hat{\lambda}_{MLE_2}\) is
\(\frac{e^{\lambda}-1}{\lambda}\).

Solution:

\[
\begin{aligned}
\sqrt n [\bar{y} - \lambda] &\overset {d}{\to} N(0,\lambda) &\text{CLT, poisson distribution}\\
\sqrt n [n_0/n - e^{-\lambda}] &\overset {d}{\to} N(0,e^{-\lambda}(1-e^{-\lambda})) &\text{CLT, bernoulli distribution} \\
\sqrt n [-\log(n_0/n) - \log(e^{-\lambda}/n)] &\overset {d}{\to} N\left(0,ne^{-\lambda}(1-e^{-\lambda})\left[-\frac{1}{e^{-\lambda}/n}\frac{1}{n}\right]^2 \right) & \text{by Delta Method} \\
&\overset {d}{\to} N\left(0,\frac{(1-e^{-\lambda})}{e^{-\lambda}} \right) \\
&\overset {d}{\to} N\left(0,e^{\lambda}-1 \right) \\
\end{aligned}
\]

Thus,

\[ARE (\hat{\lambda}_{MLE_1},\hat{\lambda}_{MLE_2}) = \frac{AVAR(\hat{\lambda}_{MLE_2})}{AVAR(\hat{\lambda}_{MLE_1})}= \frac{e^\lambda-1}{\lambda}\]

\hypertarget{d}{%
\subsection{d)}\label{d}}

The overall goal of the sampling is to estimate the mean number of the
species per unit land area. Comment on the use of
\(\hat{\lambda}_{MLE_2}\) in place of \(\hat{\lambda}_{MLE_1}\). That
is, explain to a researcher for what values and under what
distributional assumptions is it reasonable?

Solution:

Since \(e^{\lambda}-1 > \lambda\), \(\hat{\lambda}_{MLE_2}\) will always
be more efficient. However, the distributional assumptions behind when
to use which estimator are inherently different.
\(\hat{\lambda}_{MLE_2}\) presumes that we have presence-absence data,
or zero inflated poisson data at the worst, and is looking to predict
the number of land units with the absence of a species. However, if we
know the data absolutely comes from a poisson distribution,
\(\hat{\lambda}_{MLE_1}\) will fit the distributional assumptions
better.

\newpage

\hypertarget{section-3}{%
\section{2.9}\label{section-3}}

The sample \(Y_1,\dots, Y_n\) is iid with distribution
\(F_Y(y;p_o,p_1,\alpha, \beta)= p_0I(0 \leq y) + (1-p_0-p_1)F(y;\alpha,\beta) + p_1I(y\geq 1)\),
where \(F(y;\alpha,\beta)\) is the beta distribution. Use the \(2h\)
method to show that the likelihood is
\(p_0^{n_0}p_1^{n_1}(1-p_0-p_1)^{n-n_0-n_1} \prod_{0<Y_i<1} f(Y_i;\alpha,\beta)\).

Solution:

\[
\begin{aligned}
\mathcal L(p_0,p_1,\alpha, \beta | \mathbf{Y}) &= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n P_{p_0,p_1,p_2}\left(Y_i^* \in (Y_i-h,Y_i+h] |Y_i \right) \\
&= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n F_i(Y_i+h;p_o,p_1,\alpha, \beta)-F_i(Y_i-h;p_o,p_1,\alpha, \beta) \\
&= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n \big\{ p_0I(0 \leq y_i+h) + (1-p_0-p_1)F(y_i+h;\alpha,\beta) + p_1I(y_i+h\geq 1) \\
&~~~~~~~~~-[p_0I(0 \leq y_i-h) + (1-p_0-p_1)F(y_i-h;\alpha,\beta) + p_1I(y_i-h\geq 1)] \big\} \\
&= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n \big\{ p_0[I(0 \leq y_i+h)-I(0 \leq y_i-h)] \\
&~~~~~~~~~+ (1-p_0-p_1)[F(y_i+h;\alpha,\beta)-F(y_i-h;\alpha,\beta)] \\
&~~~~~~~~~+ p_1[I(y_i+h\geq 1)-I(y_i-h\geq 1)] \big\} \\
&=\prod_{i=1}^n \big\{ p_0\lim_{h \to 0^+} (2h)^{-n}[I(0 \leq y_i+h)-I(0 \leq y_i-h)]\\
&~~~~~~~~~+ p_1\lim_{h \to 0^+} (2h)^{-n}[I(y_i+h\geq 1)-I(y_i-h\geq 1)] \big\} \\
&~~~~~~~~~+ (1-p_0-p_1)\lim_{h \to 0^+} (2h)^{-n}[F(y_i+h;\alpha,\beta)-F(y_i-h;\alpha,\beta)]\big\}  \\
&=\prod_{i=1}^n \big\{ p_0 I(y_i=0) + p_1I(y_i=1) + (1-p_0-p_1)f(y_i;\alpha,\beta)I(0<y_i<1) \\
&=p_0^{n_0}p_1^{n_1}(1-p_0-p_1)\prod_{0<Y_i<1} f(y_i;\alpha,\beta) \\
\end{aligned}
\]

\newpage

\hypertarget{section-4}{%
\section{2.12}\label{section-4}}

For an iid sample \(Y_1,\dots, Y_n\), Type II censoring occurs when we
observe only the smallest \(r\) values. For example, in a study of light
bulb lifetimes, we might stop the study after the first \(r=10\) bulbs
have failed. Assuming a continuous distribution with density
\(f(y;\mathbf{\theta})\), the likelihood is just the joint density of
the smallest \(r\) order statistics evaluated at those order statistics:
\[\mathcal L\left(\mathbf \theta; Y_{(1)}, \dots, Y_{(r)}\right) = \frac {n!}{(n-r)!}\left[\prod_{i=1}^r f\left(Y_{(i)}; \mathbf \theta \right) \right]\left[1-F\left(Y_{(r)}; \mathbf \theta \right)\right]^{n-r}\]
For this situation, let \(f(y;\sigma) = e^{-y/\sigma}/\sigma\) and find
the MLE of \(\sigma\).

Solution:

\[ \begin{aligned}
\mathcal L\left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right) &= \frac {n!}{(n-10)!}\left[\prod_{i=1}^{10} \frac {e^{-y_{(i)}/\sigma}} \sigma \right]\left[1-\left(1- e^{-y_{(10)}/\sigma}\right) \right]^{n-10} \\
&= \frac {n!}{(n-10)!\sigma^{10}}\left[\prod_{i=1}^{10} {e^{-y_{(i)}/\sigma}} \right]\left[e^{-y_{(10)}/\sigma} \right]^{n-10} \\
\implies \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right) &= \log \left[\frac {n!}{(n-10)!} \right] - 10 \log(\sigma) -  \frac{\sum_{i=1}^{10}y_{(i)}} {\sigma} - \frac{(n-10)y_{(10)}}{\sigma}\\
\implies \frac {\partial \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial \sigma} &= - \frac{10}{\sigma} +  \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\sigma^2} \\
\implies \frac {\partial^2 \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial \sigma^2} &= \frac{10}{\sigma^2} -  2\frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\sigma^3}
\end{aligned}\] \[ \begin{aligned}
0= \frac {\partial \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial {\sigma}} &= - \frac{10}{\hat{\sigma}} +  \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\hat{\sigma}^2} \\
\implies  \hat{\sigma} &= \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10}
\end{aligned}
\] Verifying this is a maximum, \[\begin{aligned}
\frac {\partial^2 \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial \sigma^2} \bigg|_{\sigma = \hat \sigma}&= \frac{10}{\left(\frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10}\right)^2} - 2 \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\left( \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10} \right)^3} \\
&=  \frac{-10^3}{\left({(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}\right)^2}\\
&< 0. ~~~~~~~~~~~\text{(denominator is clearly positive)}
\end{aligned}\] Conclusively,

\[\hat{\sigma}_{MLE} = \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10}\]

More generally, I can follow the same steps outside this specific
situation to find

\[\hat{\sigma}_{MLE} = \frac{(n-r)y_{(r)}+ \sum_{i=1}^{r}y_{(i)}}{r}\]

\newpage

\hypertarget{section-5}{%
\section{2.16}\label{section-5}}

The standard Box-Cox regression model (Box and Cox 1964) assumes that
after transformation of the observed \(Y_i\) to \(Y_i^{(\lambda)}\). we
have the linear model
\[ Y_i^{(\lambda)} = \mathbf{x_i^T \beta} + e_i,~~~~~ i =1,\dots,n \]
where \(Y_i\) is assumed positive and the \(x_i\) \(i= 1, \dots, n\). In
addition assume that \(e_1,\dots,e_n\) are iid normal\((0, \sigma^2)\)
errors. Recall that the Box-Cox transformation is defined in Problem 2.1
(p.~107) and is strictly increasing for all \(\lambda\) Show that the
likelihood is
\[\mathcal L \left(\beta,\sigma,\lambda| \{Y_i, \mathbf x_i\}_{i=1}^n \right) = \left(\sqrt{2\pi}\sigma\right)^n \exp\left[-\sum_{i=1}^n \frac{ \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \prod_{i=1}^n \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|\]

Solution:

We know from the Jacobian method of transformations that
\[f_Y(y;\theta) = f_{Y^{(\lambda)}} (y^{(\lambda)};\theta)\left|J\right|=f_{Y^{(\lambda)}} (y;\theta)\left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y}\right|\].

Thus, since
\(Y_i^{(\lambda)} \overset {iid}\sim N(\mathbf{x^T \beta}, \sigma^2)\),

\[f\left(Y_i, \mathbf x_i|\beta,\sigma,\lambda \right) = \left(\sqrt{2\pi}\sigma\right) \exp\left[\frac{- \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|\]
which implies,
\[\begin{aligned}\mathcal L \left(\beta,\sigma,\lambda| \{Y_i, \mathbf x_i\}_{i=1}^n \right)
&= \prod_{i=1}^n \left(\sqrt{2\pi}\sigma\right) \exp\left[\frac{- \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|\\
&= \left(\sqrt{2\pi}\sigma\right)^n \exp\left[-\sum_{i=1}^n \frac{ \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \prod_{i=1}^n \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|
\end{aligned}\]

\newpage

\hypertarget{section-6}{%
\section{2.20}\label{section-6}}

One version of the negative binomial probability mass function is given
by
\[f(y;\mu,k) = \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)} \left( \frac{k}{\mu+k}\right)^k \left( 1- \frac{k}{\mu+k}\right)^y I(y \in \mathbb N)\]
where \(\mu\) and \(k\) are parameters. Assume that \(k\) is known and
put \(f(y; \mu,k)\) in the GLM form (2.14, p.~53), identifying
\(b(\theta)\), etc., and derive the mean and variance of
\(Y, E(Y) = \mu, Var(Y)= \mu + \mu^2/k\).

Solution:

\[ \begin{aligned}
f(y;\mu,k) &= \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)} \left( \frac{k}{\mu+k}\right)^k \left( 1- \frac{k}{\mu+k}\right)^y  \\
&= \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)} \left( \frac{k}{\mu+k}\right)^k \left( \frac{\mu}{\mu+k}\right)^y\\
\log f(y;\mu,k) &= \log \left({\Gamma(y+k)} \right)- \log \left( \Gamma(k)\right)- \log \left({\Gamma(y+1)} \right) + k \log \left({k}\right) -k \log \left({\mu+k} \right)+ y \log \left( \frac{\mu}{\mu+k}\right) \\
&= \{y \log \left( \frac{\mu}{\mu+k}\right)-k \log \left({\mu+k}\right)\}+\log \left({\Gamma(y+k)} \right)- \log \left( \Gamma(k)\right)- \log \left({\Gamma(y+1)} \right) + k \log \left({k}\right)
\end{aligned}\]

Let \(\theta = \log \left( \frac{\mu}{\mu+k}\right)\). Then, setting
\(a(\phi) = 1\),

\[\begin{aligned}
c(y)&=\log \left({\Gamma(y+k)} \right)- \log \left( \Gamma(k)\right)- \log \left({\Gamma(y+1)} \right) + k \log \left({k}\right) \\
b(\theta) &= k \log \left({\mu+k}\right) \\
&= k \log \left((\mu + k)\frac{k}{k}\right) \\
&= k \log \left(\frac{k}{\frac{k}{(\mu + k)}}\right) \\
&= k \log \left(\frac{k}{\frac{\mu+k - \mu}{(\mu + k)}}\right) \\
&= k \log \left(\frac{k}{1-\frac{\mu}{(\mu + k)}}\right) \\
&= k \log \left(\frac{k}{1-e^{\theta}}\right)
\end{aligned}\]

Because \(k\) is known, I have shown this parametrization of the
negative binomial distribution can be manipulated to be in Generalized
Linear Model form.

Thus,
\(E(y) = b'(\theta) = \frac{ke^{\theta}}{1-e^\theta} = \frac{\frac{k\mu}{\mu+k}}{1- \frac{\mu}{\mu+k}} = \frac{\frac{k\mu}{\mu+k}}{1- \frac{\mu}{\mu+k}}= \frac{\frac{k\mu}{\mu+k}}{\frac{k}{\mu+k}}= \mu\).

And,
\(Var(Y)= b''(\theta)= \frac{(1-e^\theta)ke^\theta - ke^\theta(-e^\theta)}{(1-e^\theta)^2}= \frac{\left(\frac{k}{\mu+k}\right)k\left(\frac{\mu}{\mu+k}\right)+ k\left(\frac{\mu}{\mu+k}\right)^2}{\left(\frac{k}{\mu+k}\right)^2}= \mu+ \frac{\mu^2}{k}\).

\newpage

\hypertarget{section-7}{%
\section{2.21}\label{section-7}}

The usual gamma density is given by
\[f(y;\alpha,\beta) = \frac{1}{\Gamma (\alpha)\beta^\alpha}y^{\alpha-1}e^{-y/\beta} I(0 \leq y < \infty) ~~~ \alpha,\beta>0\]
and has mean \(\alpha \beta\) and variance \(\alpha \beta^2\). First
reparameterize by letting \(\mu= \alpha \beta\) so that the parameter
vector is now \((\mu, \alpha)\). Now put this gamma family in the form
of a generalized linear model, identifying
\(\theta, b(\theta), a_i(\phi)\), and \(c(y,\phi)\). Note that
\(\alpha\) is unknown here and should be related to \(\phi\) Make sure
that \(b(\theta)\) is actually a function of \(\theta\) and take the
first derivative to verify that \(b(\theta)\) is correct.

Solution:

\[\begin{aligned}
f(y;\alpha,\beta) &= \frac{1}{\Gamma (\alpha)\left(\frac{\mu}{\alpha}\right)^\alpha}y^{\alpha-1}e^{-y (\alpha/\mu)} \\
\log f(y;\alpha,\beta) &= -\log \left(\Gamma (\alpha) \right) -\alpha \log (\mu) +\alpha \log(\alpha)+ (\alpha-1) \log(y) -y (\alpha/\mu) \\
&= \left[y \left(\frac{-\alpha}{\mu} \right) -\alpha \log (\mu) \right]  +(\alpha-1)\log(y)+\alpha \log(\alpha)-\log \left(\Gamma (\alpha) \right)  \\
&= \left[ \frac{y(-1/\mu)- \log (\mu)}{1/\alpha} \right]  +(\alpha-1)\log(y)+\alpha \log(\alpha)-\log \left(\Gamma (\alpha) \right)  \\
\end{aligned}\]

Let \(\theta = -1/\mu\), \(\phi = 1/\alpha\).Then,

\[
\log f(y;\alpha,\beta) = \left[ \frac{y\theta- \log \left(-1/\theta\right)}{\phi} \right]  +\left(\frac{1}{\phi}-1\right)\log(y)+\left(\frac{1}{\phi}\right) \log\left(\frac{1}{\phi}\right)-\log \left(\Gamma \left(\frac{1}{\phi}\right) \right)
\]

Thus, \[ \begin{aligned}
b(\theta) &= \log(-1/\theta) \\
a(\phi) &= \phi \\
c(y, \phi) &= \left(\frac{1}{\phi}-1\right)\log(y)+\left(\frac{1}{\phi}\right) \log\left(\frac{1}{\phi}\right)-\log \left(\Gamma \left(\frac{1}{\phi}\right) \right)
\end{aligned}
\]

And,
\(b'(\theta) = \left(\frac{1}{-1/\theta}\right)\left(\frac{-1}{\theta^2}\right) = 1/\theta = \mu\).

\newpage

\hypertarget{section-8}{%
\section{2.22}\label{section-8}}

Consider the standard one-way ANOVA situation with \(Y_{ij}\)
distributed as \(N(\mu_i,\sigma^2)\), \(i=1,\dots,k\), \(j=1,\dots,n_i\)
and all the random variables are independent.

\hypertarget{a.}{%
\subsection{a.}\label{a.}}

Form the log likelihood, take derivatives, and show that the MLEs are
\(\hat \mu_i = \bar{Y_i}\), \(i=1,\dots,k\), \(\hat \sigma^2 = SSE/N\),
where \(SSE= \sum_{i=1}^k \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y_i})^2\) and
\(N=\sum_{i=1}^k n_i\).

Solution:

\[
\begin{aligned}
\mathcal L (\mu_i, \sigma^2) &= \prod_{i=1}^k \prod_{j=1}^{n_i} \frac{1}{\sqrt{2\pi\sigma^2}} {\exp\left(\frac{-(y_{ij}-\mu_i)^2}{2\sigma^2}\right)} \\
\ell(\mu_i, \sigma^2) &= \sum_{i=1}^k \sum_{j=1}^{n_i} \frac{1}{\sqrt{2\pi\sigma^2}} {\exp\left(\frac{-(y_{ij}-\mu_i)^2}{2\sigma^2}\right)} \\
&= -\frac{N}{2}\log(2\pi)-\frac{N}{2}\log(\sigma^2)-\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{(y_{ij}-\mu_i)^2}{2\sigma^2} \\
\end{aligned}
\] Finding \(\hat \mu_i\), \[
\begin{aligned}
0 =\frac{\partial \ell}{\partial \mu_i}&=\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{2(y_{ij}-\mu_i)}{2\sigma^2} \\
&=\sum_{i=1}^k \sum_{j=1}^{n_i}  y_{ij}-\mu_i \\
&= \sum_{i=1}^N \bar y_i- n_i\mu_i \\
\implies \mu_i &= \bar y_i \\
\frac{\partial^2 \ell}{\partial \mu_i^2} \bigg|_{\mu_i= \bar{Y_i}}&=\sum_{i=1}^k \sum_{j=1}^{n_i}  -2 <0 \\
\implies \hat \mu_i &= \bar y_i 
\end{aligned}
\]

Finding \(\hat \sigma^2\), \[
\begin{aligned}
0 =\frac{\partial \ell}{\partial \sigma^2} \bigg|_{\mu_i = \bar y_i}&=-\frac{N}{2\sigma^2}+\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{(y_{ij}-\bar y_i)^2}{2(\sigma^2)^2} \\
&=-N\sigma^2+\sum_{i=1}^k \sum_{j=1}^{n_i}(y_{ij}-\bar y_i)^2 \\
\implies \sigma^2&=\frac{\sum_{i=1}^k \sum_{j=1}^{n_i}(y_{ij}-\bar y_i)^2}{N} \\
&= SSE/N \\
\frac{\partial^2 \ell}{\partial \left(\sigma^2 \right)^2} \bigg|_{\mu_i = \bar y_i, \sigma^2 = SSE/N}&=\frac{N}{2(\sigma^2)^2}-2\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{(y_{ij}-\bar y_i)^2}{2(\sigma^2)^3}  \bigg|_{\sigma^2 = SSE/N} \\
&=\frac{N}{2(SSE/N)^2}-\frac{SSE}{(SSE/N)^3} \\
&=\frac{N}{2(SSE/N)^2}-\frac{N}{(SSE/N)^2} <0. \\
\implies \hat \sigma^2 &= SSE/N
\end{aligned}
\] \newpage 

\hypertarget{b.}{%
\subsection{b.}\label{b.}}

Now define
\(\mathbf V_i^T = (Y_{i1} -\bar {Y_i}, \dots, Y_{i,n_i-1}- \bar{Y_i})\).
Using standard matrix manipulations with the multivariate normal
distribution, the density of \(\mathbf V_i\) is given by
\[(2 \pi)^{-(n_i-1)/2}n_i^{1/2}\sigma^{-(n_i-1)} \exp \left(- \frac{1}{2 \sigma ^2} \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \right) \]
where \(I_{n_i-1}\) is the \(n_i-1\) by \(n_i-1\) identity matrix and
\(J_{n_i-1}\) is an \(n_i-1\) by \(n_i-1\) matrix of 1's. Now form the
(marginal) likelihood based on \(\mathbf V_1, \dots, \mathbf V_k\) and
show that the MLE for \(\sigma^2\) is now \(\sigma^2 = SSE/(N-k)\).

Solution:

\[
\begin{aligned}
\mathcal L (\sigma^2; \mathbf V_1,\dots \mathbf V_k) &= \prod_{i=1}^k(2 \pi)^{-(n_i-1)/2}n_i^{1/2}(\sigma^2)^{-(n_i-1)/2} \exp \left(- \frac{1}{2 \sigma ^2} \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \right) \\
\ell (\sigma^2; \mathbf V_1,\dots \mathbf V_k) &= \sum_{i=1}^k\frac{-(n_i-1)\log(2 \pi)+n_i}{2}-\frac{(n_i-1)}{2}\log(\sigma^2) - \frac{1}{2 \sigma ^2} \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
0=\frac{d \ell}{d \sigma^2}&=\sum_{i=1}^k \frac{-(n_i-1)}{\sigma^2}+\frac{1}{2 (\sigma^2)^2}  \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
&=\sum_{i=1}^k -2\sigma^2(n_i-1)+ \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
&=-2\sigma^2(N-k)+\sum_{i=1}^k\sum_{j=1}^{n_i} 2(y_{ij} - \bar y_i)^2\\
\implies  \sigma^2 &= SSE/(N-k)
\end{aligned}
\]

\[
\begin{aligned}
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2 = SSE/(N-k)}&=\sum_{i=1}^k \frac{(n_i-1)}{(\sigma^2)^2}-\frac{1}{(\sigma^2)^3}  \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
&=\sum_{i=1}^k \frac{(n_i-1)}{(\sigma^2)^2}-\frac{1}{(\sigma^2)^3} \sum_{i=1}^k\sum_{j=1}^{n_i} 2(y_{ij} - \bar y_i)^2 \\
&=\frac{N-k}{(SSE/(N-k))^2}-\frac{1}{(SSE/(N-k))^3}\sum_{i=1}^k\sum_{j=1}^{n_i} 2(y_{ij} - \bar y_i)^2\\
&=\frac{N-k}{(SSE/(N-k))^2}-2\frac{(N-k)}{(SSE/(N-k))^2}\\ 
&=-1 \\
\implies  \hat \sigma^2 &= SSE/(N-k)
\end{aligned}
\]

\hypertarget{c.}{%
\subsection{c.}\label{c.}}

Finally, let us take a more general approach and assume that \(Y\) has
an \(N\) dimensional multivariate normal distribution with mean
\(X \mathbf{\beta}\) and covariance matrix
\(\Sigma = \sigma^2 Q(\boldsymbol{\theta})\), where X is an
\(N\times p\) full rank matrix of known constants, \(\boldsymbol \beta\)
is a \(p\)-vector of regression parameters, and
\(Q(\boldsymbol{\theta})\) is an \(N\times N\) standardized covariance
matrix depending on the unknown parameter \(\boldsymbol{\theta}\).
Typically \(\boldsymbol{\theta}\) would consist of variance component
and/or spatial correlation parameters. We can concentrate the likelihood
by noting that if \(Q(\boldsymbol{\theta})\) were known, then the
generalized least squares estimator would be
\(\boldsymbol{\hat \beta(\theta)} =(X^T Q(\boldsymbol{\theta})^{-1}X)^{-1}X^TQ(\boldsymbol{\theta})^{-1} \mathbf Y\).
Substituting for \(\boldsymbol \beta\) yields the profile log likelihood
\[-\frac{N}{2} \log(2\pi) - N \log \sigma- \frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2\sigma^2},\]
where
\(GSSE(\boldsymbol{\theta}) =(\mathbf Y - X \boldsymbol{\hat{\beta}}(\boldsymbol{\theta}))^T Q(\boldsymbol{\theta})^{-1}(\mathbf Y -X\boldsymbol{\hat \beta}(\boldsymbol{\theta}))\).
To connect with part a), let \(Q(\boldsymbol{\theta})\) be the identity
matrix (so that \(GSSE(\boldsymbol{\theta})\) is just \(SSE\)) and find
the maximum likelihood estimator of \(\sigma^2\).

Solution:

\[
\begin{aligned}
\ell(\sigma^2 | Y) &= -\frac{N}{2} \log(2\pi) - \frac{N}{2} \log \sigma^2- \frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2\sigma^2} \\
0=\frac{\partial \ell}{\partial \sigma^2} &= - \frac{N}{2 \sigma^2}+ \frac{GSSE(\boldsymbol{\theta})}{2(\sigma^2)^2}\\
&=-N\sigma^2 +GSSE(\boldsymbol \theta) \\
\implies \sigma^2 &= GSSE(\boldsymbol \theta)/N \\
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/N}&= \frac{N}{2 (\sigma^2)^2}-\frac{GSSE(\boldsymbol{\theta})}{(\sigma^2)^3}\bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/N}\\
&= \frac{N}{2 (GSSE(\boldsymbol \theta)/N)^2}-\frac{GSSE(\boldsymbol{\theta})}{(GSSE(\boldsymbol \theta)/N)^3} \\
&= -1/2 \\
\implies \hat \sigma^2 &= GSSE(\boldsymbol \theta)/N \\
\end{aligned}
\] \newpage 

\hypertarget{d.}{%
\subsection{d.}\label{d.}}

Continuing part c), the REML approach is to transform to
\(\mathbf V = A^T \mathbf Y\), where the \(N \times p\) columns of \(A\)
are linearly independent and \(A^T X =0\) so that \(\mathbf V\) is
\(MN(0;A^T \Sigma A)\) A special choice of \(A\) leads to the REML log
likelihood
\[-\frac{N-p}{2}\log(2\pi) -(N-p)\log \sigma - \frac{1}{2} \log |X^T Q(\boldsymbol{\theta} )^{-1}X|-\frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2 \sigma^2}.\]
To connect with part b), let \(Q(\boldsymbol{\theta})\) be the identity
matrix and find the maximum likelihood estimator of \(\sigma^2\).

Solution:

\[\ell(\sigma^2 | Y) = -\frac{N-p}{2}\log(2\pi) -\frac{(N-p)}{2}\log (\sigma^2) - \frac{1}{2} \log |X^T Q(\boldsymbol{\theta} )^{-1}X|-\frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2 \sigma^2}\]
\[
\begin{aligned}
0=\frac{\partial \ell}{\partial \sigma^2} &= - \frac{N-p}{2 \sigma^2}+ \frac{GSSE(\boldsymbol{\theta})}{2(\sigma^2)^2}\\
&=-(N-p)\sigma^2 +GSSE(\boldsymbol \theta) \\
\implies \sigma^2 &= GSSE(\boldsymbol \theta)/(N-p) \\
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/(N-p)}&= \frac{N}{2 (\sigma^2)^2}-\frac{GSSE(\boldsymbol{\theta})}{(\sigma^2)^3}\bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/(N-p)}\\
&= \frac{N-p}{2 (GSSE(\boldsymbol \theta)/(N-p))^2}-\frac{GSSE(\boldsymbol{\theta})}{(GSSE(\boldsymbol \theta)/(N-p))^3} \\
&= -1/2 \\
\implies \hat \sigma^2 &= GSSE(\boldsymbol \theta)/(N-p) \\
\end{aligned}
\]

\newpage

\hypertarget{section-9}{%
\section{2.25}\label{section-9}}

If \(\mathbf Y\) is from an exponential family where
\((\mathbf W, \mathbf V)\) are jointly sufficient for
\((\boldsymbol \theta_1, \boldsymbol \theta_2)\), then the conditional
density of \(\mathbf W | \mathbf V\) is free of the nuisance parameter
\(\boldsymbol \theta_2\) and can be used as a conditional likelihood for
estimating \(\boldsymbol \theta_1\). In some cases it may be difficult
to find the conditional density. However, from (2.19, p.~57) we have
\[\frac{f_{\mathbf Y}(\mathbf y; \boldsymbol \theta_1, \boldsymbol \theta_2)}{f_{\mathbf V}(\mathbf y; \boldsymbol \theta_1, \boldsymbol \theta_2)}= f_{\mathbf{W|V}}(\mathbf{w|v}; \boldsymbol \theta_1).\]
Thus, if you know the density of \(\mathbf Y\) and of \(\mathbf V\),
then you can get a conditional likelihood equation.

\hypertarget{a.-1}{%
\subsection{a.}\label{a.-1}}

Now let \(\mathbf Y_1, \dots, \mathbf Y_n\) be iid \(N(\mu,\sigma^2)\),
\(V=\bar Y\), and \(\boldsymbol \theta = (\sigma, \mu)^T\). Form the
ratio above and note that it is free of \(\mu\). (It helps to remember
that \(\sum (Y_i-\mu)^2= \sum (Y_i- \bar Y)^2 + n(\bar Y - \mu)^2\).)

Solution:

By Central Limit Theorem,
\(\mathbf V = \bar Y \sim N(\mu, \sigma^2/n)\). Thus, \[ \begin{aligned}
\frac{f_{\mathbf Y}(\mathbf y; \boldsymbol \sigma, \boldsymbol \mu)}{f_{\mathbf V}(\mathbf y; \boldsymbol \sigma, \boldsymbol \mu)} &= \frac{\prod_{i=1}^n(2\pi)^{-1/2}\sigma^{-1}\exp(-(y_i- \mu)^2/2\sigma^2)}{(2\pi)^{-1/2}\sigma^{-1}n^{1/2}\exp(-n(\bar y- \mu)^2/2\sigma^2)} \\
&= \frac{(2\pi)^{-n/2}\sigma^{-n}\exp(-\sum_{i=1}^n(y_i- \mu)^2/2\sigma^2)}{(2\pi)^{-1/2}\sigma^{-1}n^{1/2}\exp(-n(\bar y- \mu)^2/2\sigma^2)} \\
&= \frac{(2\pi)^{-n/2}\sigma^{-n}\exp(-\sum_{i=1}^n[(y_i- \bar y)^2 + n(\bar y - \mu)^2]/2\sigma^2)}{(2\pi)^{-1/2}\sigma^{-1}n^{1/2}\exp(-n(\bar y- \mu)^2/2\sigma^2)} \\
&=(2\pi)^{-(n-1)/2}\sigma^{-(n-1)}n^{-1/2}\exp\left({-\sum_{i=1}^n (Y_i- \bar Y)^2}/{2\sigma^2}\right) &\text{This is free of}~\mu.
\end{aligned}\]

\hypertarget{b.-1}{%
\subsection{b.}\label{b.-1}}

Find the conditional maximum likelihood estimator of \(\sigma^2\).

Solution:

\[\begin{aligned}
\mathcal L (\sigma^2| \mathbf Y)&=(2\pi)^{-(n-1)/2}(\sigma^2)^{-(n-1)/2}n^{-1/2}\exp\left({-\sum_{i=1}^n (Y_i- \bar Y)^2}/{2\sigma^2}\right) \\
\ell (\sigma^2| \mathbf Y)&=\frac{-(n-1)}{2}\log(2\pi)-\frac{n-1}{2}\log(\sigma^2) -\frac{\log n}{2} -\frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{2\sigma^2} \\
0 = \frac{\partial \ell}{\partial \sigma^2}&=-\frac{n-1}{2(\sigma^2)} +\frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{2(\sigma^2)^2}\\
\implies \sigma^2 &= \frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{n-1}= SSE/(n-1) \\
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2 = SSE/(n-1)}&=\frac{n-1}{2(\sigma^2)^2} -\frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{(\sigma^2)^3} \bigg|_{\sigma^2 = SSE/(n-1)} \\
&=\frac{n-1}{2(SSE/(n-1))^2} -\frac{SSE}{(SSE/(n-1))^3}\\
&= -1/2 <0. \\
\implies \hat \sigma^2 &= \frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{n-1}= SSE/(n-1) 
\end{aligned}\]

\newpage

\hypertarget{section-10}{%
\section{2.26}\label{section-10}}

Consider the normal theory linear measurement error model
\[\begin{aligned} Y_i = \alpha + \beta U_i +\sigma_e e_i, & X_i =U_i+ \sigma Z_i, & i = 1,\dots,n \end{aligned}\]
where \(e_1, \dots, e_n\), \(Z_1,\dots, Z_n\) are iid \(N(0,1)\) random
variables, \(\sigma^2\) is known, and \(\alpha,\beta, \sigma_e\), and
\(U_1, \dots, U_n\) are unknown parameters.

\hypertarget{a.-2}{%
\subsection{a.}\label{a.-2}}

Let \(s_X^2\) denote the sample variance of \(X_1,\dots, X_n\). Show
that \(E(s_X^2)=s_U^2+\sigma^2\) where \(s_U^2\) is the sample variance
of \(\{U_i\}_1^n\).

Solution:

\[
\bar X = \sum_{i=1}^n (U_i+\sigma Z_i)/n = \bar U + \sigma \bar Z, \bar Z \sim N(0,1/n) 
\] \[\begin{aligned}
E(s_X^2) &= E \left(\frac{\sum_{i=1}^n (X_i -\bar X)^2}{n-1} \right).
= E \left(\frac{\sum_{i=1}^n X_i^2 -2X_i\bar X + \bar X^2}{n-1} \right)\\ 
&= E \left(\frac{\sum_{i=1}^n (U_i + \sigma Z_i)^2 -2(U_i + \sigma Z_i)(\bar U + \sigma \bar Z) +(\bar U + \sigma \bar Z)^2}{n-1} \right)\\
&= E \left(\frac{\sum_{i=1}^n U_i^2 + 2\sigma  U_i Z_i+ \sigma^2 Z_i^2 -2(U_i \bar U + \sigma U_i \bar Z +\sigma Z_i \bar U + \sigma^2Z_i \bar Z) +\bar U^2 + 2\sigma \bar U \bar Z + \sigma^2\bar Z^2}{n-1} \right)\\
&= E \left(\frac{\sum_{i=1}^n U_i^2-2U_i \bar U+\bar U^2 + \sigma^2 Z_i^2  -2 \sigma^2Z_i \bar Z  + \sigma^2\bar Z^2}{n-1} \right)\\
&= E \left(\frac{\sum_{i=1}^n(U_i- \bar U)^2}{n-1} \right) + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\\
&= s_U^2 + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\\
&= s_U^2 + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\\
\end{aligned}
\] Note that, \[ \begin{aligned}
(n-1) \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\sim \chi^2(n-1) \\
\implies E \left[(n-1) \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\right] &= n-1 \\
\implies E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right) &= 1
\end{aligned}
\] Therefore,
\(E(s_X^2)= s_U^2 + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right) =s_U^2 + \sigma^2\).

\textbf{For the remainder of this problem assume that
\(s_U^2 \to \sigma_U^2\) as \(n \to \infty\) and that \(s_X^2\)
converges in probability to \(\sigma_U^2\). (Note that even though
\(\{U_i\}_1^n\) are parameters it still makes sense to talk about their
sample variance, and denoting the limit of \(s_U^2\) as \(n \to \infty\)
by \(\sigma_U^2\) is simply a matter of convenience).}

\hypertarget{b.-2}{%
\subsection{b.}\label{b.-2}}

Show that the estimate of slope from the least squares regression of
\(\{Y_i\}_1^n\) on \(\{X_i\}_1^n\) (call it \(\hat{\beta}_{Y|X}\) is not
consistent for \(\beta\) as \(n \to \infty\)). This shows that it is not
OK to simply ignore the measurement error in the predictor variable.

Solution:

It is a well-known result that in SLR
\(\hat{\beta}_{Y|X}= \frac{\widehat{Cov(X,Y)}}{s^2_X}\). Thus,
\[ \begin{aligned}
\lim_{n \to \infty}\hat{\beta}_{Y|X} &= \lim_{n \to \infty} \frac{Cov(X,Y)}{s^2_X} \\
&= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \frac{\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{n-1} \\
&= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \frac{\sum_{i=1}^nX_iY_i-X_i\bar Y-\bar XY_i+\bar X\bar Y}{n-1} \\
X_iY_i &= (U_i + \sigma Z_i)(\alpha + \beta U_i +\sigma_e e_i) \\
&= \alpha U_i + \beta U_i^2 +\sigma_e U_ie_i +\alpha \sigma Z_i+ \beta \sigma U_i Z_i +\sigma_e\sigma e_i Z_i \\
X_i\bar Y &= (U_i + \sigma Z_i)(\alpha + \beta \bar U +\sigma_e \bar e) \\
&= \alpha U_i + \beta U_i \bar U +\sigma_e U_i \bar e +\alpha \sigma Z_i+ \beta \sigma \bar U Z_i +\sigma_e\sigma \bar e Z_i \\
\bar XY_i &= (\bar U + \sigma \bar Z)(\alpha + \beta U_i +\sigma_e e_i) \\
&= \alpha \bar U + \beta U_i \bar U +\sigma_e \bar Ue_i +\alpha \sigma \bar Z+ \beta \sigma U_i \bar Z +\sigma_e\sigma e_i \bar Z \\
\bar X \bar Y &= (\bar U + \sigma \bar Z)(\alpha + \beta \bar U +\sigma_e \bar e) \\
&= \alpha \bar U + \beta \bar U^2 +\sigma_e \bar U \bar e +\alpha \sigma \bar Z+ \beta \sigma \bar U \bar Z +\sigma_e\sigma \bar e \bar Z \\
\implies \lim_{n \to \infty}\hat{\beta}_{Y|X} &= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \frac{\sum_{i=1}^n \beta (U_i^2-2 U_i\bar U +\bar U^2)+\sigma_e(U_i e_i -U_i \bar e -\bar U e_i + \bar U \bar e)}{n-1} \\
&~~~~~+\frac{\sum_{i=1}^n \beta \sigma (U_iZ_i- \bar U Z_i - U_i \bar Z +\bar U \bar Z)+\sigma_e \sigma(e_i Z_i -\bar e Z_i - e_i\bar Z + \bar e\bar Z )}{n-1}  \\
&= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \left[ \beta s_U^2+\frac{\sum_{i=1}^n \sigma_e \sigma(e_i Z_i -\bar e Z_i - e_i\bar Z + \bar e\bar Z )}{n-1} \right] ~~~~~~ \left( \overset{\text{Central Limit Theorem,}}{e_i,Z_i, \sqrt n \bar e, \sqrt n \bar Z \sim N(0,1)} \right) \\
&= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \left[ \beta s_U^2+\frac{\sigma_e \sigma \left[\sum_{i=1}^n (e_i - \bar e) (Z_i- \bar Z)\right]}{n-1} \right]\\
&= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \left[ \beta s_U^2+\sigma_e \sigma Cov(e_i,Z_i) \right] \\
&= \frac{1}{\sigma_U^2+ \sigma^2}(\beta s_U^2)~~~~~~~~~~~~~~~~~\bigg(e_i,Z_i \overset{iid}{\sim} N(0,1) \implies Cov(e_i,Z_i) =0 \bigg) \\
&= \frac{\beta s_U^2}{\sigma_U^2+ \sigma^2}
\end{aligned}
\] Therefore \(\lim_{n \to \infty}\hat{\beta}_{Y|X}\) is not a
consistent estimator of \(\beta\).

\hypertarget{c.-1}{%
\subsection{c.~}\label{c.-1}}

Now construct the full likelihood for
\(\alpha,\beta,\sigma_{\epsilon}^2, U_1,\dots,U_n\) and show that it has
no sensible maximum. Do this by showing that the full likelihood
diverges to \(\infty\) when \(U_i =(Y_i-\alpha)/\beta\) for all \(i\)
and \(\sigma_{\epsilon}^2 \to 0\). This is another well-known example of
the failure of maximum likelihood to produce meaningful estimates.

Solution:

\[ \begin{aligned}
Y_i &= \alpha + \beta U_i +\sigma_e e_i \sim N(\alpha + \beta U_i, \sigma_{e}^2) \\
\implies \mathcal L(\alpha, \beta, \mathbf U, \sigma_e | \mathbf Y) &= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_{e}^2}}\exp \left(\frac{-(y_i -(\alpha +\beta U_i))^2}{2\sigma_{e}^2}\right) \\
\lim_{\sigma^2_{e} \to 0}L(\alpha, \beta, \mathbf U, \sigma_e | \mathbf Y)|_{U_i =(Y_i-\alpha)/\beta} &=\lim_{\sigma^2_{e} \to 0} \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_{e}^2}}\exp \left(\frac{-(y_i -(\alpha +\beta ((Y_i-\alpha)/\beta)))^2}{2\sigma_{e}^2}\right) \\
&=\lim_{\sigma^2_{e} \to 0}\prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_{e}^2}} = \lim_{\sigma^2_{e} \to 0} (2 \pi \sigma_{e}^2)^{-n/2} \\
&= \infty
\end{aligned} \]

\hypertarget{d.-1}{%
\subsection{d.}\label{d.-1}}

Consider the simple estimator (of \(\beta\))
\[\hat \beta_{MOM} = \frac{s_X^2}{s_X^2-\sigma^2} \hat \beta _{Y|X},\]
and show that it is a consistent estimator of \(\beta\). This shows that
consistent estimators exist, and thus the problem with maximum
likelihood is not intrinsic to the model.

Solution:

\[
\begin{aligned}
\lim_{n \to \infty}\hat \beta_{MOM} &= \lim_{n \to \infty}\frac{s_X^2}{s_X^2-\sigma^2} \hat \beta _{Y|X} \\
&=\frac{\sigma_U^2 + \sigma^2}{\sigma_U^2} \left(\frac{\beta \sigma_U^2}{\sigma_U^2+ \sigma^2}\right) \\
&=\beta
\end{aligned}
\] \newpage

\hypertarget{e.}{%
\subsection{e.}\label{e.}}

Assuming that all other parameters are known, show that
\(T_i = Y_i \beta/\sigma_{e}^2+X_i/\sigma^2\) is a sufficient statistic
for \(U_i\), \(i=1,\dots,n\).

Solution:

\[
\begin{aligned}
X_i  &\sim N(U_i, \sigma^2)\\
Y_i &  \sim N(\alpha + \beta U_i, \sigma^2_e) \\
\end{aligned}
\] \[
\begin{aligned}
f_{X_i,Y_i|U_i}(x,y)&=f_{Y_i|U_i}(y_i|u_i) f_{X_i|U_i}(x_i) \\
&=\frac{1}{\sqrt{2 \pi \sigma_{e}^2}}\exp \left(\frac{-(y_i -(\alpha +\beta u_i))^2}{2 \sigma_{e}^2}\right) \frac{1}{\sqrt{2 \pi \sigma^2}}\exp \left(\frac{-(x_i- u_i)^2}{2 \sigma^2}\right)   \\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left(\frac{-(y_i^2 -2y_i\alpha - 2y_i \beta u_i +\alpha^2 +2\alpha \beta u_i + \beta^2u_i^2)}{2 \sigma_{e}^2}-\frac{x_i^2-2x_iu_i+ u_i^2}{2 \sigma^2}\right)   \\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left(\frac{-\sigma^2((y_i-\alpha)^2 - 2y_i \beta u_i +2\alpha \beta u_i + \beta^2u_i^2)-\sigma_e^2(x_i^2-2x_iu_i+ u_i^2)}{2 \sigma^2\sigma_e^2}\right)   \\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left(\frac{(2\sigma^2 y_i\beta +2\sigma_e^2x_i)u_i}{2\sigma^2\sigma^2_e}\right)\exp \left(\frac{-\sigma^2((y_i-\alpha)^2 +2\alpha \beta u_i + \beta^2u_i^2)-\sigma_e^2(x_i^2+ u_i^2)}{2 \sigma^2\sigma_e^2}\right)   \\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}\right)\right]\exp \left(\frac{-(\sigma^2(y_i-\alpha)^2 +2\sigma^2\alpha \beta u_i + \sigma^2\beta^2u_i^2+ \sigma_e^2x_i^2+\sigma_e^2u_i^2)}{2 \sigma^2\sigma_e^2}\right)   \\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}\right)\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2x_i^2)}{2 \sigma^2\sigma_e^2} -\left(\frac{\alpha \beta}{\sigma_e^2}u_i + \frac{\beta^2}{2\sigma_e^2}u_i^2+\frac{1}{2\sigma^2}u_i^2\right)\right]   \\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2x_i^2)}{2 \sigma^2\sigma_e^2}\right]\\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(t_i-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2x_i^2)}{2 \sigma^2\sigma_e^2}\right]\\
\end{aligned}
\]

Thus, \(\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}\right)\)
is sufficient for \(U_i\).

\newpage

\hypertarget{f.}{%
\subsection{f.}\label{f.}}

Find the conditional distribution of \(Y_i|T_i\) and use it to construct
a conditional likelihood for \(\alpha, \beta\), and
\(\sigma_{\epsilon}^2\) in a manner similar to that for the logistic
regression measurement error model.

Solution:

\[ \begin{aligned}
T_i &= Y_i \beta/\sigma_{e}^2+X_i/\sigma^2 \implies X_i= (T_i-Y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2} \right)
\end{aligned}
\] By the Jacobian method of transformations: \[ \begin{aligned}
f_{Y_i,T_i} (y_i,t_i) &= f_{X_i,Y_i}\left(y_i,(t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)\right)\left| \begin{matrix} 0&1/\sigma^2 \\ 1 &  \beta/\sigma_e^2\end{matrix} \right| \\
&=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{(t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)}{\sigma^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right] \\
&~~~~~~\times\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2\left((t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)\right)^2)}{2 \sigma^2\sigma_e^2}\right] \left|\frac{-1}{\sigma^2}\right|\\
\implies f_{Y_i|T_i=t_i}(y_i,t_i) &= \frac{f_{Y_i, T_i}(y_i,t_i)}{f_{T_i}(t_i)}
\end{aligned}
\] \[
\begin{aligned}
&=\frac{\frac{1}{2 \pi \sigma_e \sigma^3}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{(t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)}{\sigma^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2\left((t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)\right)^2)}{2 \sigma^2\sigma_e^2}\right] }
{\frac{1}{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}} \exp \left[\frac{-(t_i-(\sigma_e^{-2}\beta \alpha+ \sigma_e^{-2}\beta^2 u_i+\sigma^{-2}u_i))^2}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{t_i-y_i\beta}{\sigma_e^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2-\frac{(y_i-\alpha)^2+ (t_i-y_i\beta)^2\left(\frac{\sigma^2}{\sigma_e^2}\right)}{2 \sigma_e^2}\right] }
{2 \pi \sigma_e \sigma^3 \exp \left[\frac{-(t_i-(\sigma_e^{-2}\beta \alpha+ \sigma_e^{-2}\beta^2 u_i+\sigma^{-2}u_i))^2}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\exp \left[ u_i\left(\frac{t_i-\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2-\frac{(y_i-\alpha)^2+ (t_i-y_i\beta)^2\left(\frac{\sigma^2}{\sigma_e^2}\right)}{2 \sigma_e^2}\right] }
{2 \pi \sigma_e \sigma^3 \exp \left[\frac{-(t_i^2-2t_i\sigma_e^{-2}\beta \alpha-2t_i \sigma_e^{-2}\beta^2 u_i-2t_i\sigma^{-2}u_i+\sigma_e^{-4}\beta^2\alpha^2 +2\sigma_e^{-4}\beta^3\alpha u_i +2 \sigma_e^{-2}\sigma^{-2}\alpha\beta u_i+\sigma_e^{-4}\beta^4u_i^2 + 2\sigma_e^{-2}\sigma^{-2}\beta^2u_i^2 + \sigma^{-4}u_i^2)}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\exp \left[ \frac{2u_i (\beta^2 \sigma_e^{-2}+ \sigma^{-2})(t_i-\alpha\beta)\sigma_e^{-2} -(\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left(\beta^2\sigma_e^{-2}+\sigma^{-2}\right)u_i^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2} }{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] }
{2 \pi \sigma_e \sigma^3 \exp \left[\frac{-\left(t_i^2-2t_i\sigma_e^{-2}\beta \alpha-2t_i \sigma_e^{-2}\beta^2 u_i-2t_i\sigma^{-2}u_i+\sigma_e^{-4}\beta^2\alpha^2 +2\sigma_e^{-4}\beta^3\alpha u_i +2 \sigma_e^{-2}\sigma^{-2}\alpha\beta u_i+\sigma_e^{-4}\beta^4u_i^2 + 2\sigma_e^{-2}\sigma^{-2}\beta^2u_i^2 + \sigma^{-4}u_i^2\right)}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\\
&~~~~~~~\times\exp \left[ \frac{2(\beta^2 \sigma_e^{-2}+ \sigma^{-2})(t_i-\alpha\beta)\sigma_e^{-2} u_i-2t_i \sigma_e^{-2}\beta^2 u_i-2t_i\sigma^{-2}u_i+2\sigma_e^{-4}\beta^3\alpha u_i +2 \sigma_e^{-2}\sigma^{-2}\alpha\beta u_i}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
&~~~~~~~\times \exp \left[ \frac{-(\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left(\beta^2\sigma_e^{-2}+\sigma^{-2}\right)u_i^2+\sigma_e^{-4}\beta^4u_i^2 + 2\sigma_e^{-2}\sigma^{-2}\beta^2u_i^2 + \sigma^{-4}u_i^2}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
&~~~~~~\times\exp \left[\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
&=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\exp \left[\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
\end{aligned}
\] Therefore, the conditional likelihood is:
\[\mathcal L =\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\exp \left[\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]\]

And the MLEs based on this are: \[ \begin{aligned}
\ell (\alpha, \beta, \sigma_e^2, \sigma^2 ; Y_i, T_i) &= \log \left(\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\right)+\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})} \\
0 = \frac{\partial \ell}{\partial \alpha} &= \frac{2t_i\sigma_e^{-2}\beta +2\sigma_e^{-4}\beta^2\alpha-2(\beta^2 \sigma_e^{-2}+ \sigma^{-2})(y_i-\alpha)}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\\
&= t_i\sigma_e^{-2}\beta -\beta^2 \sigma_e^{-2}y_i- \sigma^{-2}y_i +\beta^2 \sigma_e^{-2}\alpha+\sigma^{-2}\alpha\\
\implies \alpha &= \frac{t_i\sigma_e^{-2}\beta-\beta^2 \sigma_e^{-2}y_i- \sigma^{-2}y_i}{\sigma_e^{-4}\beta^2-\beta^2 \sigma_e^{-2}-\sigma^{-2}}\\
0 = \frac{\partial \ell}{\partial \beta} &=  \left(\frac{1}{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}\right)\left(\frac{{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}[-2t_i\sigma_2^{-2}\alpha+2\sigma_e^{-4}\alpha^2\beta-2\beta\sigma_e^{-4}[(y_i-\alpha)^2-(t_i-y_i\beta)^2\sigma^2\sigma_e^2]+2y_i(t_i-y_i\beta)\sigma^2\sigma_e^{-2}(\beta^2\sigma_e^{-2}+\sigma^{-2})]}{4(\beta^2\sigma_e^{-2}+\sigma^{-2})^2}\right)\\
&=  -2t_i\sigma_2^{-2}\alpha+2\sigma_e^{-4}\alpha^2\beta-\beta\sigma_e^{-4}[(y_i-\alpha)^2-(t_i-y_i\beta)^2\sigma^2\sigma_e^2]+y_i(t_i-y_i\beta)\sigma^2\sigma_e^{-2}(\beta^2\sigma_e^{-2}+\sigma^{-2})\\
\end{aligned}\]

Ran out of time to finish problem. but just taking derivatives to find
MLEs of conditional likelihood from here.

\newpage

\hypertarget{section-11}{%
\section{2.35.}\label{section-11}}

Derive the Fisher information matrix \(\mathbf I(\boldsymbol \theta)\)
for the reparameterized ZIP
model:\[\begin{aligned} P(Y =0) = \pi\\ P(Y=y) = \left(\frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^ye^{-y}}{y!} \end{aligned}\]

Solution:

\[\begin{aligned}
P(Y =0) &= \pi\\
P(Y=y) &= \left(\frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^ye^{-y}}{y!} \\
\implies \mathcal L(\pi, \lambda |y) &= \pi^{I(y=0)} \left[\left(\frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^ye^{-y}}{y!}\right]^{I(y \in \mathbb N^+)} \\
\implies \ell(\pi, \lambda |y) &= \log (\pi) I(y=0) + \left[\log (1-\pi) - \log(1-e^{-\lambda})+y \log (\lambda) -y -\log (y!)\right]I(y \in \mathbb N^+) \\
S(\pi,\lambda) = \frac{\partial \ell}{\partial (\pi,\lambda)^T} &= \begin{pmatrix}\frac{1}{\pi}I(y= 0)-\frac{1}{1-\pi} I(y \in \mathbb N^+) \\ \left[-\frac{e^{-\lambda}}{1-e^{-\lambda}} + \frac{y}{\lambda} \right]I(y \in \mathbb N^+)\end{pmatrix} \\
I(\theta) = - E \left[\frac{\partial S}{\partial (\pi,\lambda)} \right] &= -E\begin{pmatrix}\frac{-1}{\pi^2}I(y= 0)+\frac{1}{(1-\pi)^2} I(y \in \mathbb N^+) & 0 \\ 0& \left[-\frac{-e^{-\lambda}(1-e^{-\lambda})-e^{-\lambda}(e^{-\lambda})}{1-e^{-\lambda}} - \frac{y}{\lambda^2} \right]I(y \in \mathbb N^+)\end{pmatrix} \\
&= -\begin{pmatrix}\frac{-1}{\pi^2}\pi-\frac{1}{(1-\pi)^2} (1-\pi) & 0 \\ 0& \left[-\frac{-e^{-\lambda}(1-e^{-\lambda})-e^{-\lambda}(e^{-\lambda})}{1-e^{-\lambda}} - \frac{\left(\frac{1}{1-e^{-\lambda}}\right)\lambda}{\lambda^2} \right](1-\pi))\end{pmatrix} \\
&=\begin{pmatrix}\frac{1}{\pi}+\frac{1}{1-\pi} & 0 \\ 0& \left[\frac{-e^{-\lambda}}{1-e^{-\lambda}} + \frac{1}{(1-e^{-\lambda})\lambda} \right](1-\pi)\end{pmatrix}
\end{aligned}\]

\newpage

\hypertarget{section-12}{%
\section{2.37}\label{section-12}}

Suppose that \(Y_1, \dots, Y_n\) are iid from the one parameter
exponential family \(f(y;\theta)= \exp[yg(\theta)-b(\theta)+c(y)]\).

\hypertarget{a.-3}{%
\subsection{a.}\label{a.-3}}

For \(g(\theta) = \theta\), find \(\bar{\mathbf{I}}(Y,\theta)\) (sample
version) and explain why it is the same as \(I(\theta)\) (Fisher
information).

Solution:

\[
\begin{aligned}
\mathcal L(\theta|\mathbf Y) &= \prod_{i=1}^n \exp[y_ig(\theta)-b(\theta)+c(y_i)] \\
\implies \ell (\theta | \mathbf Y) &= \sum_{i=1}^n y_i \theta-b(\theta)+c(y_i) \\
\implies S (\theta | \mathbf Y) &= \sum_{i=1}^n y_i -b'(\theta)\\
\implies \frac{d S (\theta | \mathbf Y)}{d \theta} &= -b''(\theta) = -nb''(\theta) \\
\implies \bar{\mathbf{I}}(Y,\theta) &= \frac{1}{n} \sum_{i=1}^n \left[-\frac{d S (\theta | Y_i)}{d \theta}\right] \\
&= b''(\theta)
\end{aligned}
\]

The fisher information is the same as the sample fisher information as
the derivative of the score function is constant with respect to the
sample data. The fisher information and sample information should be
equal when variables are iid.

\hypertarget{b.-3}{%
\subsection{b.}\label{b.-3}}

Now for general differentiable \(g(\theta)\), find \(I(\theta)\)

Solution:

\[
\begin{aligned}
\mathcal L(\theta|\mathbf Y) &= \prod_{i=1}^n \exp[y_ig(\theta)-b(\theta)+c(y_i)] \\
\implies \ell (\theta | \mathbf Y) &= \sum_{i=1}^n y_i g(\theta)-b(\theta)+c(y_i) \\
\implies S (\theta | \mathbf Y) &= \sum_{i=1}^n y_ig'(\theta) -b'(\theta)\\
\implies \frac{d S (\theta | \mathbf Y)}{d \theta} &= \sum_{i=1}^n y_ig''(\theta) -b''(\theta) \\
&= n \bar y g''(\theta) -nb''(\theta) \\
\implies I(Y,\theta) &= - E \left[\frac{d S (\theta | Y_i)}{d \theta}\right] \\
&= -E\left[y_ig''(\theta) -b''(\theta)\right] \\
&= - \left[b'(\theta) g''(\theta) -b''(\theta)\right] & E(y_i) = b'(\theta) \\
&= b''(\theta) - b'(\theta) g'' (\theta) 
\end{aligned}
\]

\newpage

\hypertarget{section-13}{%
\section{2.41}\label{section-13}}

Use simulation to verify that the information matrix for Example 2.21
(p.~78) is correct when \(\mu =1\) and \(\sigma =1\). One approach is to
generate samples of size \(n=100\) (or larger) from a normal(1,1)
distribution and exponentiate to get lognormal data. Then form the log
likelihood and use a numerical derivative routine to find the second
derivative matrix for each sample. Then average over 1000 replications
and compare to the given information matrix.

Solution:

\begin{verbatim}
              [,1]          [,2]      [,3]
[1,]  1.0000000000 -0.0007235042 -0.998414
[2,] -0.0007235042  1.9926544888 -2.001938
[3,] -0.9984139964 -2.0019375153  4.503790
\end{verbatim}

\begin{verbatim}
     [,1] [,2] [,3]
[1,]    1    0 -1.0
[2,]    0    2 -2.0
[3,]   -1   -2  4.5
\end{verbatim}

The mean of the fisher information matrices that I simulated is printed
above, with the fisher info the book describes right below it. These two
matrices are very close together, which means the simulation supports
the book's example.

\newpage

\hypertarget{section-14}{%
\section{2.43}\label{section-14}}

For the generalized linear model with link function \(g\), not
necessarily the canonical link, write down the likelihood function and
show how to obtain the likelihood score equation,
\[S(\boldsymbol \beta, \phi) = \sum_{i=1}^n \mathbf D_i \frac{(Y_i-\mu_i)}{Var(Y_i)}=0, \text{ where } \mathbf D_i =\mathbf D_i(\boldsymbol \beta) = \frac{\partial \mu_i(\boldsymbol \beta)}{\partial \boldsymbol \beta^T}\]
(In the above expression we have suppressed the dependence of \(D_i\)
and \(\mu_i\) on \(\boldsymbol \beta\).) The key idea used is the chain
rule and the fact that the derivative of \(\theta_i= b'^{-1}(\mu_i)\)
with respect to \(\mu_i\) is \(1/b''(\theta_i)\).

Solution:

\[
\begin{aligned}
\ell (y_i;\theta_i,\phi) &= \frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i,\phi) \\
&= \frac{y_ib'^{-1}(\mu_i) - b(b'^{-1}(\mu_i))}{a_i(\phi)} + c(y_i,\phi) \\
S(\beta, \phi) &= \frac{y_ib''^{-1}(\mu_i)\mathbf D_i - b'(b'^{-1}(\mu_i))b''^{-1}(\mu_i) \mathbf D_i}{a_i(\phi)} \\
&= \frac{y_i\mathbf D_i - b'(b'^{-1}(\mu_i)) \mathbf D_i}{b''(\theta_i)a_i(\phi)} & \left[b''^{-1}(\mu_i) = \frac{1}{b''(\theta_i)}\right] \\
&= \frac{y_i\mathbf D_i - \mu_i \mathbf D_i}{b''(\theta_i)a_i(\phi)} \\
&= \mathbf D_i\frac{Y_i - \mu_i}{Var(Y_i)} & \left[Var(Y_i) = b''(\theta_i)a_i(\phi)\right]\\
\end{aligned}
\]

\newpage

\hypertarget{section-15}{%
\section{2.44}\label{section-15}}

Continuing the last problem, show that the Fisher information matrix for
the \(\boldsymbol \beta\) part of the generalized linear model is given
by
\[\bar{\mathbf I} ( \boldsymbol  \beta ) = \frac{1}{n} \sum_{i=1}^n \frac{\mathbf{D_i D_i}^T}{Var(Y_i)}\]
Here you can use either of two methods: a) take the expectation of the
negative of the derivative of \(S(\boldsymbol \beta, \phi)\), and noting
that all the ugly derivatives drop out because \(E(Y_i-\mu_i)=0\); or b)
the individual summed components of
\(\mathbf{\bar I}(\boldsymbol \beta)\) can also be found using the
cross-product definition of information in (2.39, p.~67).

Solution:

\[
\begin{aligned}
\bar{\mathbf I}(\boldsymbol \beta) 
&= \frac{1}{n}\sum_{i=1}^n E\left[\mathbf s_i(\boldsymbol \beta) \mathbf s_i(\boldsymbol \beta)^T\right] \\
&= \frac{1}{n}\sum_{i=1}^n E\left[\mathbf D_i\frac{Y_i - \mu_i}{Var(Y_i)}\mathbf D_i^T\frac{Y_i - \mu_i}{Var(Y_i)}\right] \\
&= \frac{1}{n}\sum_{i=1}^n \mathbf D_i \mathbf D_i^T\frac{E[(Y_i - \mu_i)^2]}{[Var(Y_i)]^2} \\
&= \frac{1}{n}\sum_{i=1}^n \mathbf D_i \mathbf D_i^T\frac{Var(Y_i)}{[Var(Y_i)]^2} \\
&= \frac{1}{n}\sum_{i=1}^n \frac{\mathbf D_i \mathbf D_i^T}{Var(Y_i)} \\
\end{aligned}
\]

\newpage

\hypertarget{section-16}{%
\section{2.45}\label{section-16}}

Suppose that \(X_1\) and \(X_2\) are independent and continuous random
variables with densities \(f_1\) and \(f_2\), respectively. \(Z\) is a
Bernoulli(\(p\)) random variable and independent of \(X_1\) and \(X_2\).
Define \(Y= ZX_1 +(1-ZX_2)\).

\hypertarget{a-1}{%
\subsection{a)}\label{a-1}}

Use the \(2h\) method to show that the joint density of \((Y,Z)\) is
given by \[f_{Y,Z}(y,z) = [pf_1(y)]^z[(1-p)f_2(y)]^{1-z}\]

Solution:

\[\begin{aligned}
f_{Y,Z} (y,z) &= lim_{h \to 0^+} (2h)^{-2} P\left(Y \in(y-h, y+h], Z \in (z-h,z+h]\right) \\
&= lim_{h \to 0^+} (2h)^{-1} P\left(Y \in(y-h, y+h], Z = z\right) ~~~~~~~~~\text{(for small }h) \\
&= lim_{h \to 0^+} (2h)^{-1} \big[P(z\,x_1+ (1-z)\,x_2 \leq y+h], z= 0) -P(z\,x_1+ (1-z)\,x_2 < y-h], z= 0) \\ &~~~~~~~~~~~~~~~~~~+
P(z\,x_1+ (1-z)\,x_2 \leq y+h], z= 1) -P(z\,x_1+ (1-z)\,x_2 < y-h], z= 1)\big] \\
&= lim_{h \to 0^+} (2h)^{-1} \big[P(z=0)[P(x_2 \leq y+h]) -P(\,x_2 < y-h])] \\&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+P(z=1)[P(x_1\leq y+h]) -P(x_1 < y-h])\big] \\
&= lim_{h \to 0^+} (2h)^{-1} \big\{p[P((x_2 \leq y+h]) -P(\,x_2 < y-h])]^{1-z} \\
&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\times\left[(1-p)[P(z\,x_1 \leq y+h]) -P(x_1 < y-h]) \right]^{z} \big\} \\
&= [pf_1(y)]^z[(1-p)f_2(y)]^{1-z}
\end{aligned}
\]

\hypertarget{b-2}{%
\subsection{b)}\label{b-2}}

Use the \(2h\) method to show that
\[P(Z=1|Y=y) = \frac{pf_1(y)}{pf_1(y) + (1-p)f_2(y)}\]

Solution:

I have already used the \(2h\) method in the problem above to find the
pdf \(f_{Y,Z}(y,z)\), now we can use simple probability rules for the
rest of the work:

\[\begin{aligned}
P(Z=1|Y=y) &= \frac{P(Z=1,Y=y)}{P(Y=y)}\\
&= \frac{P(Z=1,Y=y)}{\sum_{Z}P(Y=y,Z=z)}\\
&= \frac{P(Z=1,Y=y)}{P(Y=y,Z=0) + P(Y=y,z=1)}\\
&= \frac{f_{Y,Z} (y,z=1)}{f_{Y,Z} (y,z=0) + f_{Y,Z} (y,z=1)} \\
&= \frac{pf_1(y)}{[(1-p)f_2(y)] + [pf_1(y)]}
\end{aligned}
\]

\newpage

\hypertarget{section-17}{%
\section{2.47}\label{section-17}}

A mixture of three component densities has the form
\[f(y;\boldsymbol \theta, \mathbf p) = p_1f_1(y;\boldsymbol \theta) + p_2f_2(y;\boldsymbol \theta) + p_3f_3(y;\boldsymbol \theta),\]
where \(p_1= p_2 = p_3 = 1\). We observe an iid sample
\(Y_1,\dots, Y_n\) from \(f(y;\boldsymbol \theta, \mathbf p )\).

\hypertarget{a.-4}{%
\subsection{a.}\label{a.-4}}

Show how to define multinomial(\(1;p_1,p_2,p_3\)) vectors
\((Z_{i1},Z_{i2},Z_{i3})\) to get a representation for the \(Y_i\) from
\(f(y;\boldsymbol \theta, \mathbf p )\) based on independent random
variables \((X_{i1},X_{i2},X_{i3})\) from the individual components.

Solution:

Let
\(Z_{i1} \sim Bernoulli(p_1),Z_{i2} \sim Bernoulli(p_2),Z_{i3} \sim Bernoulli(p_3)\)
such that \((Z_{i1},Z_{i2},Z_{i3}) \sim multinomial(1;p_1,p_2,p_3)\)
Also, let the pdfs of \(X_{i1},X_{i2},X_{i3}\) be
\(f_1(y;\boldsymbol \theta), f_1(y;\boldsymbol \theta), f_1(y;\boldsymbol \theta)\)
respectively.

Define \(Y_i = Z_{i1}X_{i1}+ Z_{i2}X_{i2} + Z_{i3}X_{i3}\).

Thus,
\(f_{Y_i}(y) = p_1f_1(y;\boldsymbol \theta) + p_2f_2(y;\boldsymbol \theta) + p_3f_3(y;\boldsymbol \theta)\)

\hypertarget{b.-4}{%
\subsection{b.}\label{b.-4}}

Give the complete data log likelihood and the function \(Q\) to be
maximized at the M step.

Solution:

The joint distribution \(f(Y_i,Z_i)\) can be found as follows:

\[
\begin{aligned}
f(Y_i,Z_i) &= f(Y_i|Z_i)f(Z_i) \\
&= f(Z_{i1}X_{i1}+ Z_{i2}X_{i2} + Z_{i3}X_{i3}|Z_i)f(Z_i) \\
&= f_{X_{i1}}(y_i|Z_{i1})f_{X_{i2}}(y_i|Z_{i2})f_{X_{i3}}(y_i|Z_{i3})f(Z_i)\\
&= [f_{X_{i1}}(y_i;\boldsymbol \theta)]^{z_{i1}}[f_{X_{i2}}(y_i;\boldsymbol \theta)]^{Z_{i2}}[f_{X_{i3}}(y_i;\boldsymbol \theta)]^{Z_{i3}}p_1^{z_{i1}}p_2^{z_{i2}}p_3^{z_i3}
\end{aligned}
\] Thus, the complete log likelihood is
\[\begin{aligned}\ell_C(\boldsymbol \theta,p_1,p_2,p_3|\mathbf Y,\mathbf Z) &= \sum_{i=1}^n z_{i1}\log(f_{X_{i1}}(\boldsymbol \theta;y_i))+z_{i2}\log (f_{X_{i2}}(\boldsymbol \theta;y_i))+z_{i3}\log(f_{X_{i3}}(\boldsymbol \theta;y_i))\\
&~~~~~~~~~~~~~~~~~~~~~+z_{i1}\log p_1+z_{i2}\log p_2+z_{i3} \log p_3 \end{aligned}\]

and,
\[\begin{aligned}Q(\boldsymbol \theta,p_1,p_2,p_3,\boldsymbol \theta^v,p_1^v,p_2^v,p_3^v,\mathbf Y) &= E_{(\boldsymbol \theta^v,p_1^v,p_2^v,p_3^v)} [\ell_C(\boldsymbol \theta,p_1,p_2,p_3|\mathbf Y,\mathbf Z)]\\
&=\sum_{i=1}^n \bigg\{w_{i1}^v\log(f_{X_{i1}}(y_i;\boldsymbol \theta))+w_{i2}^v\log (f_{X_{i2}}(y_i;\boldsymbol \theta))+w^v_{i3}\log(f_{X_{i3}}(y_i;\boldsymbol \theta))\\
&~~~~~~~~~~~~~~~~~~~~~+w^v_{i1}\log p_1+w^v_{i2}\log p_2+w^v_{i3} \log p_3\bigg\} \end{aligned}\]

Where
\(\begin{aligned} w_{ij}^v= E_{(\boldsymbol \theta^v,p_1^v,p_2^v,p_3^v)}[Z_{ij}|Y_i]&= \frac{p_j^{v}f_j(Y_i,\boldsymbol \theta^v)}{\sum_{j=1}^n p_j^vf_j(Y_i,\boldsymbol \theta^v)}\end{aligned}\)

\newpage

\hypertarget{section-18}{%
\section{2.48}\label{section-18}}

Suppose that the data \(Y_1, \dots, Y_n\) are assumed to come from a
mixture of two binomial distributions. Thus
\[f(y;p,\theta_1,\theta_2) = p {n \choose y} \theta_1^y(1-\theta_1)^{n-y} +(1-p) {n \choose y} \theta_2^y(1-\theta_2)^{n-y}.\]
Find \(Q(p,\theta_1,\theta_2,p^v,\theta_1^v,\theta_2^v)\) and the
updating formulas.

Solution:

Define \(Z_i \sim Bernoulli(p)\), \(Y_i = ZX_{1i}+ (1-Z)X_{2i}\), where
\(X_{1i} \sim Binom(n,\theta_1)\), \(X_{2i} \sim Binom(n,\theta_2)\) and
\(X_{1i},X_{2i}\) are independent of \(Z\). Thus, \(Y_i\) will have a
mixture density equal to \(f(y;p,\theta_1,\theta_2)\).

Therefore,
\[\mathcal L_C(p,\theta_1,\theta_2;y_i,Z) = \left[p f_{X_1}(y_i;n,\theta_1)\right]^{z_i} \left[(1-p) f_{X_2}(y_i;n,\theta_2)\right]^{1-{z_i}}\]
And, following steps outlined on page 85 of Boos, \[\begin{aligned}
Q(p,\theta_1,\theta_2,p^v,\theta_1^v,\theta_2^v) &= \sum_{i=1}^n  w_i^v \log f_{X_1}(y_i;n,\theta_1)+(1-w^v_i) \log f_{X_2}(y_i;n,\theta_2) +w^v_i \log p + (1-w^v_i)\log(1-p)
\end{aligned}
\]

Where
\(w^v_i = \frac{p^vf_{X_1}(y_i;n,\theta_1)}{p^vf_{X_1}(y_i;n,\theta_1)+(1-p^v)f_{X_2}(y_i;n,\theta_2)}\).

Substituting in the distributions,

\[
\begin{aligned}
Q(p,\theta_1,\theta_2,p^v,\theta_1^v,\theta_2^v) &= \sum_{i=1}^n \bigg\{ w_i^v \log \left[{n \choose y_i} \theta_1^{y_i}(1-\theta_1)^{n-y_i}\right] +(1-w_i^v) \log \left[{n \choose y_i} \theta_2^{y_i}(1-\theta_2)^{n-y_i}\right]\\
&~~~~~~~~~ +w_i^v \log p + (1-w_i^v)\log(1-p)\bigg\} \\
&= \sum_{i=1}^n \bigg\{ w_i^v \left[ \log {n \choose y_i} + y_i \log \theta_1 +\log y_i +(n-y_i)\log(1-\theta_1)\right]\\
&~~~~~~~~~  +(1-w_i^v) \left[ \log {n \choose y_i} + y_i \log \theta_2 +\log y_i +(n-y_i)\log(1-\theta_2)\right]\\
&~~~~~~~~~ +w_i^v \log p + (1-w_i^v)\log(1-p)\bigg\}
\end{aligned}
\]

Now to maximize \(Q\), \[ \begin{aligned}
0= \frac{\partial Q}{\partial \theta_1} &=\sum_{i=1}^n \frac{w_i^vy_i}{\theta_1}-\frac{w_i^v(n-y_i)}{1-\theta_1} \\
\implies 0&= \sum_{i=1}^n w_i^v(y_i-y_i\theta_1-(n-y_i)\theta_1)\\
\implies 0&= \sum_{i=1}^n w_i^v(y_i-n\theta_1)\\
\implies \theta_1^{v+1} &= \frac{\sum_{i=1}^n w_i^vy_i}{\sum_{i=1}^n w_i^vn}
\end{aligned}
\] Similarly, \[ \begin{aligned}
0= \frac{\partial Q}{\partial \theta_1} &=\sum_{i=1}^n \frac{(1-w_i^v)y_i}{\theta_2}-\frac{(1-w_i^v)(n-y_i)}{1-\theta_2} \\
\implies 0&= \sum_{i=1}^n (1-w_i^v)(y_i-y_i\theta_2-(n-y_i)\theta_2)\\
\implies \theta_2^{v+1} &= \frac{\sum_{i=1}^n (1-w_i^v)y_i}{\sum_{i=1}^n (1-w_i^v)n}
\end{aligned}
\]

and, \[\begin{aligned}
0=\frac{\partial Q}{\partial p} &=\frac{w_i^v}{p}-\frac{1-w_i^v}{1-p} \\
\implies 0&= w_i^v-pw_i^v-p+pw_i^v \\
\implies p^{v+1} &= w_i^v
\end{aligned}\]

\newpage

\hypertarget{section-19}{%
\section{2.49}\label{section-19}}

Recall that the ZIP model is just a mixture of densities
\(f(y;\lambda, p) =pf_1(y)+(1-p)f_2(y;\lambda)\) where
\[\begin{aligned}f_1(y)=I(y=0)~&~~~~~~~~~f_2(y;\lambda)= \frac{\lambda^ye^{-\lambda}}{y!}~&~~y=0,1,2,\dots\end{aligned}\]
Lambert (1992) used it to model product defects as a function of
covariates. In the ``perfect'' state, no defects occur
(\(P(Y_i= 0)= 0\)), whereas in the ``imperfect'' state, the number of
defects \(Y_i\) follows a Poisson\((\lambda)\) distribution. The author
used the EM Algorithm as follows (except we won't do the more
complicated modeling with covariates.) Let \(Z_i = 1\) if the product is
in the perfect state and \(Z_i = 0\) for the imperfect state. Recall
that the contribution to the complete data likelihood for a pair
\((Y_i,Z_i)\) is \([pf_1(Y_i)]^{Z_i}[(1-p)f_2(Y_i;\lambda)]^{1-Z_i}\)
(and here note that the first part reduces to \(p^{Z_i}\) because
\(f_1\) is a point mass at 0).

\hypertarget{a.-e-step.}{%
\subsection{a. E step.}\label{a.-e-step.}}

Write down the complete data log likelihood and find
\(Q(\lambda,p,\lambda^v,p^v)\) in terms of
\(w_i^v = E(Z_i|Y_i,\lambda^v,p^v)\). (You do not need to give an
expression for \(w_i^v\).)

Solution:

\[
\begin{aligned}
\mathcal L_C (\lambda, p, \lambda^v, p^v; Y_i,Z_i) &= \prod_{i=1}^n[pf_1(Y_i)]^{Z_i}[(1-p)f_2(Y_i;\lambda)]^{1-Z_i} \\
\implies  \ell_C(\lambda, p, \lambda^v, p^v; Y_i,Z_i) &= \sum_{i=1}^n z_i \log(f_1(y_i))+(1-z_i)\log(f_2(y)) +z_i \log p +(1-z_i)\log(1-p) \\
\implies  Q(\lambda, p, \lambda^v, p^v; Y_i) &=\sum_{i=1}^nE_{(\lambda^v,p^v)}
\left[z_i \log(f_1(y_i))+(1-z_i)\log(f_2(y,\lambda)) +z_i \log p +(1-z_i)\log(1-p)\right] \\
&=\sum_{i=1}^nw_i^v \log(f_1(y_i))+(1-w^v_i)\log(f_2(y,\lambda)) +w^v_i \log p +(1-w^v_i)\log(1-p) \\
&=\sum_{i=1}^nw_i^v I(y_i=0)+(1-w^v_i)\log\left(\frac{\lambda^{y_i}e^{-\lambda}}{y_i!}\right)I(y_i \in \mathbb N^+) \\&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+w^v_i \log p +(1-w^v_i)\log(1-p) \\
\end{aligned}
\]

\hypertarget{b.-m-step.}{%
\subsection{b. M step.}\label{b.-m-step.}}

Find expressions for \(\lambda^{v+1}\) and \(p^{v+1}\) by maximizing
\(Q\) from the E step.

Solution:

\[
\begin{aligned}
0 =\frac{\partial Q}{\partial p} &= \sum_{i=1}^n\frac{w_i^v}{p}-\frac{1-w_i^v}{1-p}\\
\implies 0 &= \sum_{i=1}^n w_i^v-pw_i^v-p+pw_i^v \\
\implies p^{v+1} &= \frac{\sum_{i=1}^n w_i^v}{n}
\end{aligned}
\]

\[
\begin{aligned}
0 =\frac{\partial Q}{\partial \lambda} &= (1-w_i^v)\left[\frac{y_i}{\lambda}-1\right]I(y_i \in \mathbb N^+)\\
\implies 0 &= (1-w_i^v)(y_i-\lambda)I(y_i \in \mathbb N^+) \\
\implies \lambda^{v+1} &=\frac{(1-w_i^v)y_i}{1-w_i^v}
\end{aligned}
\]

\bookmarksetup{startatroot}

\hypertarget{chapter-3-likelihood-based-tests-and-confidence-regions}{%
\chapter{Chapter 3: Likelihood-Based Tests and Confidence
Regions}\label{chapter-3-likelihood-based-tests-and-confidence-regions}}

\hypertarget{section-20}{%
\section{3.6}\label{section-20}}

Assume that \(Y_1\) and \(Y_2\) are independent with respective
geometric probability mass functions,
\[f(y;p_i) = p_i(1-p_i)^{y-1}~~~~y=0,1,\dots~~~~ 0 \leq p_i \leq 1, ~~i=1,2.\]
Recall that the mean and variance of a geometric random variable with
parameter \(p\) are \(1/p\) and \((1-p)/p^2\), respectively. For
\(H_0: p_1=p_2\) versus \(H_a: p_1 \neq p_2\) show that the score
statistic is
\[T_S=\frac{\overset \sim p ^2}{2(1- \overset \sim p)}(Y_1-Y_2)^2, ~~~where~\overset \sim p = \frac 2 {Y_1+Y_2}\]

Solution:

\[\begin{aligned}
\boldsymbol \theta &= \begin{pmatrix} p_1 \\p_2 \end{pmatrix}, ~~~ h(\boldsymbol \theta) =p_1-p_2 = 0 \\
\mathcal L(\boldsymbol \theta) &= p_1(1-p_1)^{y_1-1}p_2(1-p_2)^{y_2-1},~~~~ H(\boldsymbol \theta) = \begin{bmatrix} 1 & -1 \end{bmatrix}
\\
\mathcal \ell(\boldsymbol \theta) &= \log p_1 + (y_1-1)\log(1-p_1) +\log p_2+ (y_2-1)\log(1-p_2)
\end{aligned}
\] \[\begin{aligned}
S(\boldsymbol \theta) &= \begin{pmatrix} \frac 1 {p_1}- \frac{y_1-1}{1-p_1} \\ \frac 1 {p_2}- \frac{y_2-1}{1-p_2} \end{pmatrix} \\
I_T(\boldsymbol \theta) &= -E\begin{pmatrix} \frac {-1} {p_1^2}- \frac{y_1-1}{(1-p_1)^2} & 0 \\ 0& \frac {-1} {p_2^2}- \frac{y_2-1}{(1-p_2)^2} \end{pmatrix} =-\begin{pmatrix} \frac {-1} {p_1^2}- \frac{\frac{1}{p_1}-1}{(1-p_1)^2} & 0 \\ 0& \frac {-1} {p_2^2}- \frac{\frac{1}{p_2}-1}{(1-p_2)^2} \end{pmatrix} \\
&=-\begin{pmatrix} \frac {-(1-p_1)^2-p_1+p_1^2}{p_1^2(1-p_1)^2}& 0 \\ 0& \frac {-(1-p_2)^2-p_2+p_2^2}{p_2^2(1-p_2)^2} \end{pmatrix}=-\begin{pmatrix} \frac {-1+2p_1-p_1^2-p_1+p_1^2}{p_1^2(1-p_1)^2}& 0 \\ 0& \frac {-1+2p_2 - p_2^2-p_2+p_2^2}{p_2^2(1-p_2)^2} \end{pmatrix} \\
&=-\begin{pmatrix} \frac {p_1-1}{p_1^2(1-p_1)^2}& 0 \\ 0& \frac {p_2-1}{p_2^2(1-p_2)^2} \end{pmatrix}
=\begin{pmatrix} \frac {1}{p_1^2(1-p_1)}& 0 \\ 0& \frac {1}{p_2^2(1-p_2)} \end{pmatrix} \\
I_T(\boldsymbol \theta)^{-1} &=  \begin{pmatrix} p_1^2(1-p_1)& 0 \\ 0& p_2^2(1-p_2) \end{pmatrix}
\end{aligned}\]

Using the Lagrange multiplier method, \[\begin{aligned}
maximize \{\ell(p)  -\lambda h(\theta)\}
& = \log p_1 + (y_1-1)\log(1-p_1) +\log p_2+ (y_2-1)\log(1-p_2) + \lambda p_1 - \lambda p_2\\
p_1 &= p_2  = p\\
0 &= \frac{1} {p} -\frac{y_1-1}{1-p} + \lambda \implies \lambda = -\frac{1} {p} +\frac{y_1-1}{1-p}\\
0 &= \frac{1} {p} -\frac{y_2-1}{1-p} -\lambda \implies \lambda = \frac{1} {p} -\frac{y_2-1}{1-p} \\
\implies 0&= \frac{2} {p} -\frac{y_2-1+y_1-1}{1-p}\\
&= 2-2p +2p -p(y_1+y_2) \\
\implies \overset \sim p &= \frac 2 {y_1+y_2},~~~ \hat \lambda_{RMLE} = \frac{1} {\overset \sim p} -\frac{y_2-1}{1-\overset \sim p}\\
& \hat \lambda_{RMLE} =  \frac{1- \overset \sim p - \overset \sim p y_2+ \overset \sim p}{\overset \sim p (1-\overset \sim p)}=  \frac{1 - \frac {2y_2} {y_1+y_2}}{\overset \sim p (1-\overset \sim p)} \\
&\hat \lambda_{RMLE}=  \frac{\frac {y_1-y_2} {y_1+y_2}}{\overset \sim p (1-\overset \sim p)}=\frac {(y_1-y_2) \overset \sim p /2}{\overset \sim p (1-\overset \sim p)}\\
&\hat \lambda_{RMLE}=  \frac{y_1-y_2}{2 (1-\overset \sim p)}
\end{aligned}\]

\[\begin{aligned}
T_S &= \hat \lambda_{RMLE}^T H(\hat \theta_{RMLE})I_T^{-1}(\hat \theta_{RMLE})  H(\hat \theta_{RMLE})^T\hat \lambda_{RMLE}^T \\
&= \left(\frac{y_1-y_2}{2 (1-\overset \sim p)}\right)^2  \begin{pmatrix} 1 & -1 \end{pmatrix} \begin{pmatrix} \overset \sim p^2(1-\overset \sim p)& 0 \\ 0&\overset \sim p^2(1-\overset \sim p) \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} \\
&= \left(\frac{y_1-y_2}{2 (1-\overset \sim p)}\right)^2  2 (\overset \sim p^2(1-\overset \sim p)) \\
&= \frac{\overset \sim p^2}{2 (1-\overset \sim p)}(y_1-y_2)^2
\end{aligned}\]

\newpage

\hypertarget{section-21}{%
\section{3.8}\label{section-21}}

Suppose that \(Y_1,\dots, Y_{n_1}\) are iid from a \(N(\mu_1,\sigma^2)\)
distribution, \(X_1,\dots,X_{n_2}\) are iid from a \(N(\mu_2,\sigma^2)\)
distribution, the samples are independent of each other, and we desire
to test \(H_0:\mu_1=\mu_2\).

\hypertarget{a.-5}{%
\subsection{a.}\label{a.-5}}

Derive the Wald and score tests and express them as a function of the
square of the usual two-sample pooled t . Also, show that the likelihood
ratio statistic is \(T_{LR} = (n_1+n_2)\log[1+t^2/(n_1+n_2-2)]\).

Solution:

\[\begin{aligned}
\ell (\mu_1,\mu_2,\sigma) &= c-n_1\log \sigma - \frac 1 {2\sigma^2} \sum_{i=1}^{n_1}(Y_i-\mu_1)^2 +c-n_2\log \sigma - \frac 1 {2\sigma^2} \sum_{i=1}^{n_2}(X_i-\mu_2)^2\\
&= 2c-(n_1+n_2)\log \sigma - \frac 1 {2\sigma^2} \sum_{i=1}^{n_1}(Y_i-\mu_1)^2 - \frac 1 {2\sigma^2} \sum_{i=1}^{n_2}(X_i-\mu_2)^2 \\
S\begin{pmatrix} \mu_1 \\ \mu_2 \\\sigma \end{pmatrix} 
&=\begin{pmatrix} \frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu_1)\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu_2)\\
\frac{-n_1-n_2}{\sigma} +\frac 1 {\sigma^3} \sum_{i=1}^{n_1} (Y_i-\mu_1)^2 +\frac 1 {\sigma^3} \sum_{i=1}^{n_2} (X_i-\mu_2)^2 \end{pmatrix}\\
&=\begin{pmatrix} \frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu_1)\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu_2)\\
\frac 1 {\sigma^3} \sum_{i=1}^{n_1} [(Y_i-\mu_1)^2-\sigma^2] +\frac 1 {\sigma^3} \sum_{i=1}^{n_2} [(X_i-\mu_2)^2 - \sigma^2] \end{pmatrix}\\
I_T \begin{pmatrix} \mu_1 \\ \mu_2 \\\sigma \end{pmatrix}
&= \begin{pmatrix} \frac {n_1} {\sigma^2} &0&0\\
0&\frac {n_2} {\sigma^2} & 0\\
0&0& \frac{n_1+n_2}{\sigma^2}\end{pmatrix},
I_T^{-1} \begin{pmatrix} \mu_1 \\ \mu_2 \\\sigma \end{pmatrix}
= \sigma^2 \begin{pmatrix} \frac {1} {n_1} &0&0\\
0&\frac {1} {n_2} & 0\\
0&0& \frac 1 {n_1+n_2}\end{pmatrix} \\
\mathbf h( \boldsymbol {\theta}) &= \mu_1-\mu_2 ,
\mathbf H(\boldsymbol \theta) = \begin{pmatrix} 1 & -1 &0\end{pmatrix}, \mathbf H(\boldsymbol \theta) \mathbf {I_T^{-1}}(\boldsymbol {\theta}) \mathbf H(\boldsymbol \theta)^T= \sigma^2 \left(\frac 1 {n_1} +\frac 1 {n_2}\right)
\end{aligned}\]

\[\begin{aligned}
\begin{pmatrix} \frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu_1)\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu_2)\\
\frac 1 {\sigma^3} \sum_{i=1}^{n_1} [(Y_i-\mu_1)^2-\sigma^2] +\frac 1 {\sigma^3} \sum_{i=1}^{n_2} [(X_i-\mu_2)^2 - \sigma^2] \end{pmatrix} &= \mathbf 0 \\
\implies \hat \mu_1 = \frac 1 {n_1} \sum_{i=1}^{n_1} Y_i = \bar Y ~~,~~\hat \mu_2 = \frac 1 {n_2} \sum_{i=1}^{n_2} X_i = \bar X \\
\sum_{i=1}^{n_1} [(Y_i-\hat \mu_1)^2-\sigma^2] + \sum_{i=1}^{n_2} [(X_i-\hat \mu_2)^2 - \hat \sigma^2] = 0 \\
\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2  =  (n_1+ n_2) \hat \sigma^2 \\
\implies \hat \sigma_{MLE} ^2 = \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 }{n_1+n_2}
\end{aligned}
\]

Note that \[\begin{aligned}
S_p^2 &= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 }{n_1+n_2-2} \\
\implies \frac{(n_1+ n_2 -2)}{n_1+n_2}S_p^2 &= \hat \sigma^2_{MLE}
\end{aligned}\] \[
\begin{aligned}
\implies  \sigma_{RMLE} ^2 
&= \frac{\sum_{i=1}^{n_1} (Y_i- \mu)^2 + \sum_{i=1}^{n_2} (X_i- \mu)^2 }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y + \bar Y- \mu)^2 + \sum_{i=1}^{n_2} (X_i- \bar X + \bar X- \mu)^2 }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + 2(Y_i-\bar Y)(\bar Y- \mu) +(\bar Y- \mu)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 + 2(X_i-\bar X)(\bar X- \mu) +(\bar X- \mu)^2  }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2  +(\bar Y- \mu)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 +(\bar X- \mu)^2  }{n_1+n_2}\\
&= \frac{\sum_{i=1}^{n_1} (Y_i-\bar Y)^2 + \sum_{i=1}^{n_2} (X_i-\bar X)^2 +n_1(\bar Y- \mu)^2+n_2(\bar X- \mu)^2  }{n_1+n_2}\\
&= \sigma^2_{MLE}+\frac{n_1(\bar Y- \mu)^2+n_2(\bar X- \mu)^2  }{n_1+n_2}
\end{aligned}\]

\newpage

Thus, the Wald Statistic is:

\[\begin{aligned}
T_W &= \mathbf h( \boldsymbol {\hat \theta})^T [\mathbf H(\boldsymbol {\hat \theta}) \mathbf {I_T^{-1}}(\boldsymbol {\hat \theta}) \mathbf H(\boldsymbol {\hat \theta})^T]^{-1}\mathbf h( \boldsymbol {\hat \theta})\\
&= \frac{(\bar Y- \bar X)^2}{\hat \sigma^2_{MLE} \left(\frac 1 {n_1} +\frac 1 {n_2}\right)}
\end{aligned}\]

Now, for the score function, I need to maximize
\(\ell (\mu_1,\mu_2, \sigma) -\lambda(\mu_1-\mu_2)\).

\[\begin{aligned}
\mu_1- \mu_2 = 0 &\implies \mu_1 = \mu_2 = \mu\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu) -\lambda =0& \implies \lambda= \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu) \\\\
\frac 1 {\sigma^2} \sum_{i=1}^{n_2} (X_i-\mu) + \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu)= 0 
&\implies \hat \mu_{RMLE} = \frac{\sum_{i=1}^{n_2} X_i + \sum_{i=1}^{n_1} Y_i}{n_1+n_2} = \frac{n_2 \bar X + n_1 \bar Y}{n_1+n_2}
\end{aligned}\]

Simplifying \(\hat \lambda_{RMLE}\), \[\begin{aligned}
\hat \lambda&= \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i-\mu) \\
&= \frac{1}{\sigma^2} \sum_{i=1}^{n_1} (Y_i)-n_1 \left(\frac{\sum_{i=1}^{n_1} X_i + \sum_{i=1}^{n_2} Y_i}{n_1+n_2}\right) \\
&= \frac{1}{\sigma^2} \left(\frac{(n_1+n_2)\sum_{i=1}^{n_1} (Y_i)-n_1 \left(\sum_{i=1}^{n_1} X_i + \sum_{i=1}^{n_2} Y_i\right)}{n_1+n_2}\right) \\
&= \frac{1}{\sigma^2} \left(\frac{n_2\sum_{i=1}^{n_1} Y_i-n_1 \sum_{i=1}^{n_1} X_i}{n_1+n_2}\right) \\
&= \frac{1}{\sigma^2} \left(\frac{n_1n_2}{n_1+n_2} \right)(\bar Y - \bar X)
\end{aligned}\]

The score statistic is: \[\begin{aligned}
T_S &= \hat \lambda_{RMLE}^T H(\hat \theta_{RMLE})I_T^{-1}(\hat \theta_{RMLE}) \hat \lambda_{RMLE}^T \\
&= \left[\frac{1}{\sigma^2} \left(\frac{n_1n_2}{n_1+n_2} \right)(\bar Y - \bar X)\right]^2\sigma^2 \left(\frac 1 {n_1} +\frac 1 {n_2}\right) \\
&= \frac{1}{\sigma^2} \left[\left(\frac{n_1n_2}{n_1+n_2} \right)(\bar Y - \bar X)\right]^2 \left(\frac {n_1+n_2} {n_1n_2}\right) \\
&= \frac{(\bar Y - \bar X)^2 }{\sigma^2\left(\frac 1 {n_1} +\frac 1{n_2}\right)}
\end{aligned}\]

\newpage

And the LRT is:

\[\begin{aligned}T_{LR} &= -2 [\ell (\hat \theta_{RMLE}) - \ell(\hat \theta_{RMLE})] \\
&=-2 [2c- \frac 1 2 (n_1+n_2)\log \sigma_{RMLE}^2 - \frac 1 {2\sigma_{RMLE}^2} \sum_{i=1}^{n_1}(Y_i-\mu)^2 - \frac 1 {2\sigma_{RMLE}^2} \sum_{i=1}^{n_2}(X_i-\mu)^2] \\
&~~~~~~+2 [2c-\frac 1 2 (n_1+n_2)\log \sigma_{MLE}^2 - \frac 1 {2\sigma_{MLE}^2} \sum_{i=1}^{n_1}(Y_i-\mu_1)^2 - \frac 1 {2\sigma_{MLE}^2} \sum_{i=1}^{n_2}(X_i-\mu_2)^2] \\
&=(n_1+n_2) \log \left(\frac{\sigma_{RMLE}^2}{\sigma_{MLE}^2} \right)\\
&= (n_1+n_2) \log \left(\frac{\sigma^2_{MLE}+\frac{n_1(\bar Y- \mu)^2+n_2(\bar X- \mu)^2  }{n_1+n_2}}{\sigma_{MLE}^2} \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac 1{n_1+n_2}\left(n_1(\bar Y- \frac{n_2 \bar X + n_1 \bar Y}{n_1+n_2})^2+n_2(\bar X- \frac{n_2 \bar X + n_1 \bar Y}{n_1+n_2})^2  \right)}
{\sigma^2_{MLE}} \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac 1{n_1+n_2}\left(n_1(\frac{n_2 \bar Y- n_2 \bar X}{n_1+n_2})^2+n_2(\frac{n_1 \bar X - n_1 \bar Y}{n_1+n_2})^2  \right)}{\sigma_{MLE}^2 } \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac 1{(n_1+n_2)^3}\left(n_1n_2^2( \bar Y- \bar X)^2+n_1^2n_2(\bar X - \bar Y)^2  \right)}{\sigma_{MLE}^2 } \right)\\
&= (n_1+n_2) \log \left(1+\frac{\frac{n_1n_2}{(n_1 + n_2)^2} \left(\bar Y - \bar X\right)^2}
{\sigma_{MLE}^2 } \right)\\
&= (n_1+n_2) \log \left(1+\frac{ \left(\bar Y - \bar X\right)^2}
{(n_1+n_2)\sigma_{MLE}^2 \left(\frac 1{n_1} + \frac 1 {n_2}\right)} \right)\\
&= (n_1+n_2) \log \left(1+\frac{ \left(\bar Y - \bar X\right)^2}
{(n_1+n_2-2)S_p^2 \left(\frac 1{n_1} + \frac 1 {n_2}\right)} \right)\\
&= (n_1+n_2) \log \left(1+\frac{t^2}{(n_1+n_2-2)} \right)
\end{aligned}
\]

\newpage

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
\end{enumerate}

Let \(n_1= n_2=5\). These tests reject \(H_0\) at approximate level .05
if they are larger than 3.84. Find exact expressions for
\(P(T_W \geq 3.84), P(T_S \geq 3.84),\) and \(P(T_{LR} \geq 3.84)\)
using the fact that \(t^2\) has an \(F(1,n_1+n_2-2)\) under \(H_0\).

Solution:

\[\begin{aligned}
P(T_W \geq 3.84)= P(T_S \geq 3.84) &= P\left(\frac{(\bar Y- \bar X)^2}{\hat \sigma^2_{MLE} \left(\frac 1 {5} +\frac 1 {5}\right)} \geq 3.84\right) \\
&= 1-\chi^2_{(1)}(3.84) \\
P(T_{LR} \geq 3.84) &= P\left((5+5) \log \left(1+\frac{t^2}{(5+5-2)} \right) \geq 3.84\right) \\
&= P\left(1+t^2/8  \geq \exp(0.384)\right)\\
&= P\left(t^2  \geq 8(\exp(0.384)-1)\right)\\
&= 1-F_{8\exp(0.384)-8}(1,8)
\end{aligned}\]

\newpage

\hypertarget{section-22}{%
\section{3.9}\label{section-22}}

Suppose that \(Y_1,\dots, Y_n\) are independently distributed as Poisson
random variables with means \(\lambda_1, \dots, \lambda_n\),
respectively. Thus,
\(P(Y_i=y) = f(y;\lambda_i) =\frac{\lambda_i^y e^{-\lambda_i}}{y!}I(y \in \mathbb N)\).
Show that the score statistic for testing
\(H_0: \lambda_1 = \lambda_2 = \dots = \lambda_n=\lambda\) (i.e., that
the \(Y_i\) 's all have the same distribution) is given by
\(T_S = \sum_{i=1}^n \frac{(Y_i-\bar Y)^2}{\bar Y}\).

Solution:

\[\begin{aligned}
\mathcal L(\boldsymbol \lambda) &=\prod_{i=1}^n \frac{\lambda_i^{y_i} e^{-\lambda_i}}{y_i!} \\
\ell (\boldsymbol \lambda) &=\sum_{i=1}^n y_i \log \lambda_i -\lambda_i -\log(y_i!) \\
S_i (\boldsymbol \lambda) &= \frac {y_i} {\lambda_i}-1 \\
I_T (\boldsymbol \lambda) &= diag\left(-E\left(\frac{-y_i}{\lambda_i^2}\right)\right) = diag(1/\lambda_i) \\
I_T^{-1} (\boldsymbol \lambda)&= diag(\lambda_i)
\end{aligned}\]

\[\begin{aligned}
0 &=S_i (\boldsymbol \lambda) = \frac {y_i} {\lambda_i}-1 \\
\implies \hat \lambda_{i_{MLE}} &=y_i~~~,~~~~ \hat \lambda_{RMLE} =\bar y
\end{aligned}\]

\[\begin{aligned}
T_S &= \mathbf {S(\boldsymbol{\hat \lambda}_{RMLE})^T I_T^{-1} (\boldsymbol {\hat \lambda_{RMLE}})S(\boldsymbol{\hat \lambda}_{RMLE})}\\
&= \left\{\frac {Y_i} {\bar Y}-1 \right\}diag(\bar Y)\left\{\frac {Y_i} {\bar Y}-1 \right\}\\
&= \sum_{i=1}^n \bar Y\left(\frac {Y_i} {\bar Y}-1 \right)^2=\sum_{i=1}^n \bar Y\left(\frac {Y_i - \bar Y} {\bar Y}\right)^2\\
&= \sum_{i=1}^n \frac {(Y_i-\bar Y)^2} {\bar Y}\\
\end{aligned}\]

\newpage

\hypertarget{section-23}{%
\section{3.12}\label{section-23}}

Show that the \(T_W\), \(T_S\) and \(T_{LR}\) statistics defined in
Example 3.2 (p.~129) are asymptotically equivalent under \(H_0\) by
showing their differences converge to 0 in probability.

Solution:

\[\begin{aligned}
\ell (\hat p) &= \ell(p_0) + \ell'(p_0)(\hat p - p_0) +\frac 1 2 \ell^{2}(p_0)(\hat p - p_0)^2+ \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \\
\implies T_{LR} &=-2\left[\ell (\hat p_0) - \ell (\hat p) \right] \\
&=-2\left[\ell (\hat p_0) - \ell(p_0) - \ell'(p_0)(\hat p - p_0) -\frac 1 2 \ell^{2}(p_0)(\hat p - p_0)^2- \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \right] \\
&=2\left[S(p_0)(\hat p - p_0) -\frac 1 2 I_T(p_0)(\hat p - p_0)^2+ \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \right] \\
&=2\left[\frac{n \hat p - n p_0}{p_0 (1-p_0)}(\hat p -p_0)-\frac 1 2 \left(\frac{n \hat p}{p_0^2}+ \frac{n-n\hat p}{(1-p_0)^2}\right)(\hat p - p_0)^2+ \frac 1 6 \ell^3 (p^*)(\hat p - p_o)^3 \right] \\
&=n (\hat p -p_0)^2\left[\frac{2}{p_0 (1-p_0)}-\left(\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}\right)+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_o) \right] \\
\implies T_W- T_{LR} &= \frac{n(\hat p- p_0)^2}{\hat p (1-\hat p)} -n (\hat p -p_0)^2\left[\frac{2}{p_0 (1-p_0)}-\left(\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}\right)+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_o) \right] \\
&= n(\hat p- p_0)^2\left[\frac 1 {\hat p (1-\hat p)}-\frac{2}{p_0 (1-p_0)}+\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_o) \right]
\end{aligned}\]

Note that \(n(\hat p- p_0)^2 \overset d \to \chi^2_{something}\) and,

\[\begin{aligned}
&\lim_{n \to \infty}\left[\frac 1 {\hat p (1-\hat p)}-\frac{2}{p_0 (1-p_0)}+\frac{\hat p}{p_0^2}+ \frac{1-\hat p}{(1-p_0)^2}+ \frac 1 {3n} \ell^3 (p^*)(\hat p - p_0) \right] \\
&=\frac 1 {p_0 (1-p_0)}-\frac{2}{p_0 (1-p_0)}+\frac{p_0}{p_0^2}+ \frac{1-p_0}{(1-p_0)^2}+0\\
&=-\frac{1}{p_0 (1-p_0)}+\frac{1}{p_0}+ \frac{1}{1-p_0}\\
&= -\frac{1}{p_0 (1-p_0)}+\frac{1-p_0+p_0}{p_0(1-p_0)}\\
&= 0
\end{aligned}
\]

Therefore, by Slutsky's theorem \(T_w - T_{LR} \overset P \to 0\).

\[\begin{aligned}
T_W - T_S &= \frac{n(\hat p- p_0)^2}{\hat p (1-\hat p)} - \frac{n(\hat p- p_0)^2}{p_0 (1-p_0)}\\
&= n \left(\frac{p_0 (1-p_0)(\hat p- p_0)^2-\hat p (1-\hat p)(\hat p- p_0)^2}{\hat p p_0(1-\hat p) (1-p_0)} \right)\\
&= n \left(\frac{[(p_0-p_0^2)-(\hat p-\hat p^2)](\hat p- p_0)^2}{\hat p p_0(1-\hat p) (1-p_0)} \right)\\
&= \left[\sqrt n (\hat p- p_0) \right]^2\left(\frac{p_0 -\hat p + \hat p^2-p_0^2}{\hat p p_0(1-\hat p) (1-p_0)} \right)\\
\text{Note that } &p_0 - \hat p \overset p \to 0 \text{ by WLLN, so}
\left(\frac{p_0 -\hat p + \hat p^2-p_0^2}{\hat p p_0(1-\hat p) (1-p_0)} \right) \overset p \to 0\\
\text{and, }&\left[\sqrt n (\hat p- p_0) \right]^2 \overset d \to \chi^2_{something} \text{ by CLT} \\
\implies T_W - T_S &\overset P \to 0 \text{ by Slutsky's Theorem}
\end{aligned}\]

\newpage

\hypertarget{section-24}{%
\section{3.16}\label{section-24}}

\textbf{Derive the score statistic \(T_S\) in (3.21, p.~148)}

\[\begin{aligned}
logit(p_{1+2(j-1)}) = \beta_j + \beta_{k+1} ~~&,~~logit(p_{2+2(j-1)}) = \beta_j\\
p_{1+2(j-1)} =  \frac{\exp(\beta_j + \beta_{k+1})}{1+\exp(\beta_j + \beta_{k+1})}~~
&,~~p_{2+2(j-1)} =  \frac{\exp(\beta_j)}{1+\exp(\beta_j)} \\
H_0: \beta_{k+1} = 0
\end{aligned}\]

\[\begin{aligned}
\mathcal L(\boldsymbol{\beta}_j, \beta_{k+1}) &= \prod_{j=1}^k \left(p_{1+2(j-1)}\right)^{a_j}
\left(1-p_{1+2(j-1)}\right)^{c_j}
\left(p_{2+2(j-1)}\right)^{b_j}
\left(1-p_{2+2(j-1)}\right)^{d_j} \\
&= \prod_{j=1}^k \left(\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{a_j}
\left(1-\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{c_j}\\
&~~~~~~~~~~~~~~~ \times
\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)^{b_j} 
\left(1-\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)^{d_j} \\
&= \prod_{j=1}^k \left(\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{a_j}
\left(\frac{1}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)^{c_j}\\
&~~~~~~~~~~~~~~~ \times
\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)^{b_j}
\left(\frac{1}{1+\exp(\boldsymbol{\beta}_j)}\right)^{d_j} \\
\ell(\boldsymbol{\beta}_j, \beta_{k+1})&= \sum_{j=1}^k
a_j \log \left(\frac{\exp(\boldsymbol{\beta}_j + \beta_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
-c_j\log\left({1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)\\
&~~~~~~~~~~~~~~~ +
b_j\log\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)
-d_j\log\left({1+\exp(\boldsymbol{\beta}_j)}\right) \\
S(\boldsymbol{\beta}_j, \beta_{k+1})&= \begin{pmatrix}
\left[
a_j \left(1- \frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-c_j\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)+
b_j\left(1-\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)-
d_j\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)\right]_{\substack{j^{th}~ entry~ of \\ k x 1 ~vector}} \\
\sum_{j=1}^k
a_j \left(1- \frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-c_j\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[
a_j +b_j  -(a_j + c_j)\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-(b_j+d_j)\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)\right]_{\substack{j^{th}~ entry~ of \\ k x 1 ~vector}} \\
\sum_{j=1}^k
a_j-(a_j+c_j)\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}\right)
-m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right) \right]_{\substack{j^{th}~ entry~ of \\ k x 1 ~vector}} \\
\sum_{j=1}^k
a_j-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{1+\exp(\boldsymbol{\beta}_j + \beta_{k+1})}\right)
\end{pmatrix}
\end{aligned}\]

\[\begin{aligned}
I_T (\boldsymbol{\beta}_j, \beta_{k+1} ) &= - \begin{pmatrix}
diag \left\{-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1}))^2}\right)-m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)\right\}_{k\times k} 
&  \left\{-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1}))^2}\right)\right\}_{k\times 1}\\
\left\{-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1}))^2}\right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j + \boldsymbol{\beta}_{k+1})}{(1+\exp(\boldsymbol{\beta}_j + \beta_{k+1}))^2}\right) \right\}_{1\times 1}
\end{pmatrix}
\end{aligned}\]

Note, under the restricted MLE, \(\beta_{k+1} = 0\) implies
\[ \begin{aligned}
p_j&=\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j)}= \frac{b_{j}}{c_j}= \frac{n_{1j}}{t_j}\\
1-p_j&= \frac{n_{2j}}{t_j} = 1-\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j)}=\frac{1}{(1+\exp(\boldsymbol{\beta}_j)}\\
\implies p_j(1-p_j) &= \frac{n_{1j}n_{2j}}{t_j^2} 
= \frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}
\end{aligned}\]

Thus, \[\begin{aligned}
\overset \sim I_T (\boldsymbol{\beta}_j, \beta_{k+1} ) &= \begin{pmatrix}
diag \left\{m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)+m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)\right\}_{k\times k} 
&  \left\{m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{(1+\exp(\boldsymbol{\beta}_j))^2}\right)\right\}_{k\times 1}\\
\left\{m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j ))^2}\right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j )}{(1+\exp(\boldsymbol{\beta}_j))^2}\right) \right\}_{1\times 1}
\end{pmatrix} \\
&= \begin{pmatrix}
diag\left\{\frac{(m_{1j}+ m_{2j})n_{1j}n_{2j}}{t_j^2} \right\}_{k\times k} 
&  \left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{k\times 1}\\
\left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right) \right\}_{1\times 1}
\end{pmatrix} \\
&= \begin{pmatrix}
diag\left\{\frac{t_jn_{1j}n_{2j}}{t_j^2} \right\}_{k\times k} 
&  \left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{k\times 1}\\
\left\{m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right)\right\}_{1\times k}
& \left\{\sum_{j=1}^k
m_{1j}\left(\frac{n_{1j}n_{2j}}{t_j^2} \right) \right\}_{1\times 1}
\end{pmatrix} \\
\implies \overset \sim I_{T,22}^{-1} &= [\overset \sim I_{T,22} -\overset \sim I_{T,21} \overset \sim I_{T,11}^{-1} \overset \sim I_{T,12} ]^{-1} \\
&= \left(\sum_{j=1}^k
\frac{m_{1j}n_{1j}n_{2j}}{t_j^2} - \left(\frac{m_{1j}n_{1j}n_{2j}}{t_j^2}\right)^2\frac{t_j}{n_{1j}n_{2j}}\right)^{-1}\\
&= \left(\sum_{j=1}^k
\frac{m_{1j}t_jn_{1j}^2n_{2j}^2-m_{1j}^2n_{1j}^2n_{2j}^2}{t_j^3n_{1j}n_{2j}}\right)^{-1}\\
&= \left(\sum_{j=1}^k
\frac{m_{1j}(t_j-m_{1j})n_{1j}n_{2j}}{t_j^3}\right)^{-1}\\
&=\frac{1}{\sum_{j=1}^km_{1j}m_{2j}n_{1j}n_{2j}/t_j^3}\\
\end{aligned}
\]

Now, to compute the score under the null hypothesis, \[\begin{aligned}
\overset \sim S(\beta_j, \beta_{k+1})&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)
-m_{2j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right) \right]_{k \times 1} \\
\sum_{j=1}^k
a_j-m_{1j}\left(\frac{\exp(\boldsymbol{\beta}_j)}{1+\exp(\boldsymbol{\beta}_j)}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{n_{1j}}{t_j}\right)
-m_{2j}\left(\frac{n_{1j}}{t_j}\right) \right]_{k \times 1} \\
\sum_{j=1}^k
a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right)
\end{pmatrix} \\
&= \begin{pmatrix}
\left[n_{1j}  -m_{1j}\left(\frac{n_{1j}}{t_j}\right)
-m_{2j}\left(\frac{n_{1j}}{t_j}\right) \right]_{k \times 1} \\
\sum_{j=1}^k a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right)
\end{pmatrix} \\
&= \begin{pmatrix}0 \\
\sum_{j=1}^k a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right) \end{pmatrix} \\
\implies \overset \sim S(\beta_j, \beta_{k+1})^T \overset \sim S(\beta_j, \beta_{k+1})&=
\left[\sum_{j=1}^k a_j-m_{1j}\left(\frac{n_{1j}}{t_j}\right)\right]^2
\end{aligned}\]

And the score statistic follows (matching Wikipedia, not the book
\textbf{OUR CLASS CONCLUDED TEXTBOOK HAS ERROR FROM A PAPER THAT HAD A
TYPO}):

\[\begin{aligned}
T_S &= \overset \sim S(\beta_j, \beta_{k+1})^T [\overset \sim I_{T,22} -\overset \sim I_{T,21} \overset \sim I_{T,11}^{-1} \overset \sim I_{T,12} ]^{-1}\overset \sim S(\beta_j, \beta_{k+1})\\
&=\frac{\left[\sum_{j=1}^k a_j-m_{1j}n_{1j}/t_j\right]^2}{\sum_{j=1}^km_{1j}m_{2j}n_{1j}n_{2j}/t_j^3}
\end{aligned}\]

\hypertarget{section-25}{%
\section{3.18}\label{section-25}}

Consider having two independent iid samples, the first with a
\(normal(\mu_1,1)\) distribution and sample size \(n_1\), the second
with a \(normal(\mu_2,1)\) distribution and sample size \(n_2\). For
\(H_0: \mu_1= \mu_2\) versus \(H_a: \mu_1 < \mu_2\), find \(T_{LR}\) and
the testing procedure at \(\alpha = 0.05\).

Solution:

Note that under \(H_a\) the maximum likelihood estimators are the usual
ones if \(\bar Y_1 \leq \bar Y_2\), but
\(\hat \mu_1 = \hat \mu_2 = (n_1 \bar Y_1 +n_2 \bar Y _2)/(n_1+n_2)\) if
\(\bar Y_1 > \bar Y_2\). Also, note that \(P(2,2) = 1/2\) in (3.23,
p.~152) for this case since the probability is 1/2 that the restricted
estimators are the usual sample means with \(l=2\) distinct values.

\[\begin{aligned}
\mathcal L(\mu_1,\mu_2) &= \prod_{i=1}^{n_1}\frac{1}{\sqrt{2 \pi}} e ^{-\frac 1 2 (y_{i1}- \mu_1)^2}\prod_{j=1}^{n_2}\frac{1}{\sqrt{2 \pi}} e ^{-\frac 1 2 (y_{j2}- \mu_2)^2} \\
\ell(\mu_1,\mu_2) &= C -\frac 1 2\sum_{i=1}^{n_1} (y_{i1}- \mu_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \mu_2)^2\\
T_{LR} &= -2 (\ell (\hat \theta_{RMLE})-\ell (\hat \theta_{MLE})) \\
&=\begin{cases} -2 \bigg(\left[-\frac 1 2\sum_{i=1}^{n_1} (y_{i1}- \bar y_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \bar y_2)^2\right] \\
~~~~~~~~~~~~~~-\left[-\frac 1 2\sum_{i=1}^{n_1} (y_{i1}- \bar y_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \bar y_2)^2\right] \bigg), & \bar Y_1 \leq \bar Y_2\\
-2 \bigg(\left[-\frac 1 2\sum_{i=1}^{n_1} \left(y_{i1}-  \frac{(n_1 \bar Y_1 +n_2 \bar Y _2)}{(n_1+n_2)}\right)^2-\frac 1 2\sum_{j=1}^{n_2} \left(y_{j2}- \frac{(n_1 \bar Y_1 +n_2 \bar Y _2)}{(n_1+n_2)}\right)^2\right]\\
~~~~~~~~~~~~~~-\left[-\frac 1 2\sum_{i=1}^{n_1} y_{i1}- \bar y_1)^2-\frac 1 2\sum_{j=1}^{n_2} (y_{j2}- \bar y_2)^2\right] \bigg) , &\bar Y_1 > \bar Y_2 \end{cases} \\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
\sum_{i=1}^{n_1} \left(y_{i1}-  \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2+\sum_{j=1}^{n_2} \left(y_{j2}- \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2\\
~~~~~~~~~~~~~~-\sum_{i=1}^{n_1} (y_{i1}^2- 2 y_{i1}\bar y_1 + \bar y_1^2)-\sum_{j=1}^{n_2} (y_{j2}^2-2y_{j2}\bar Y_2 +\bar Y_2^2) , &\bar Y_1 > \bar Y_2 \end{cases}
\end{aligned}\] \[\begin{aligned}
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
\sum_{i=1}^{n_1} y_{i1}^2-  2n_1\bar Y_1 \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}+n_1\left(\frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2+\sum_{j=1}^{n_2} y_{j2}^2-2n_2 \bar Y_2 \frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2} + n_2 \left(\frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2\\
~~~~~~~~~~~~~~-\sum_{i=1}^{n_1} y_{i1}^2 + 2 n_1 \bar Y_1^2 - n_1\bar Y_1^2-\sum_{j=1}^{n_2} y_{j2}^2+2n_2\bar Y_2^2 -n_2\bar Y_2^2 , &\bar Y_1 > \bar Y_2 \end{cases} \\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
-  2\frac{(n_1 \bar Y_1 +n_2 \bar Y _2)^2}{n_1+n_2}+(n_1+n_2)\left(\frac{n_1 \bar Y_1 +n_2 \bar Y _2}{n_1+n_2}\right)^2+  n_1 \bar Y_1^2 +n_2\bar Y_2^2 , &\bar Y_1 > \bar Y_2 \end{cases} \\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
n_1 \bar Y_1^2 +n_2\bar Y_2^2 -  \frac{(n_1 \bar Y_1 +n_2 \bar Y _2)^2}{n_1+n_2}, &\bar Y_1 > \bar Y_2
\end{cases}\\
&=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \frac{(n_1+n_2)(n_1 \bar Y_1^2 +n_2\bar Y_2^2) -(n_1 \bar Y_1 +n_2 \bar Y _2)^2}{n_1+n_2}, &\bar Y_1 > \bar Y_2 \end{cases} \\
 &=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \frac{(n_1^2 \bar Y_1^2 + n_1n_2 \bar Y_1^2 +n_1n_2 \bar Y_2^2 +n_2^2\bar Y_2^2) -(n_1^2 \bar Y_1^2 +2n_1n_2 \bar Y_1 \bar Y_2 +n_2^2 \bar Y _2^2)}{n_1+n_2}, &\bar Y_1 > \bar Y_2 \end{cases} \\
  &=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \left(\frac{n_1n_2}{n_1+n_2}\right)(\bar Y_1^2 -2 \bar Y_1 \bar Y_2 + \bar Y_2^2), &\bar Y_1 > \bar Y_2 \end{cases} \\
  &=\begin{cases} 0, & \bar Y_1 \leq \bar Y_2\\
 \left(\frac{n_1n_2}{n_1+n_2}\right)(\bar Y_1-\bar Y_2)^2, &\bar Y_1 > \bar Y_2 \end{cases} 
\end{aligned}\]

From 3.6.1a (p.~151), the \(T_{LR}\) follows a non-chi-square
distribution, \(\bar \chi^2_{k=2}\). Note, that the following page
states
\(P(\bar \chi^2_{k=2} \geq C) = \frac 1 2 I(C=0) + \frac 1 2P(\chi^2_{k=2} \geq C)\)

So I need to find the value \(C\) such that:

\[.05 = \alpha =P(\bar \chi^2_{k=2} \geq C) = \frac 1 2 I(C=0) + \frac 1 2P(\chi^2_{2} \geq C)\]
Note that \(C \neq 0\), because if it were, we would get
\(1= P(\bar \chi^2_{k=2} \geq C)\).

Thus, I am left with, \[ \begin{aligned}
.05 &=  \frac 1 2P(\chi^2_{2} \geq C) \\
\implies .1 &= P(\chi^2_{2} \geq C) \\
\implies C &= 4.60517 &\text{by r command qchisq(.1, df = 2, lower.tail = F)}
\end{aligned}\]

Therefore the testing procedure is to reject \(H_0\) if
\(\bar Y_1> Y_2\) AND
\(\left(\frac{n_1n_2}{n_1+n_2}\right)(\bar Y_1-\bar Y_2)^2 > 4.60517\).

\bookmarksetup{startatroot}

\hypertarget{chapter-5-large-sample-theory-the-basics}{%
\chapter{Chapter 5: Large Sample Theory: The
Basics}\label{chapter-5-large-sample-theory-the-basics}}

\hypertarget{section-26}{%
\section{5.1}\label{section-26}}

Suppose that \(Y_1, \dots, Y_n\) are identically distributed with mean
\(E(Y_1)= \mu\), \(Var(Y_1)= \sigma^2\) and covariances given by
\(Cov(Y_{i},Y_{i+j})= \begin{cases} \rho\sigma^2, & |j| \leq2 \\ 0, &|j| >2 \end{cases}\).
Prove that \(\bar Y \overset{p}{\to} \mu\) as \(n \to \infty\).

Solution:

\[
\begin{aligned}
Var(\bar Y) &= \frac{1}{n^2}\left[\sum_{i=1}^n \sigma_i^2 + 2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n} \sigma_{ij}\right] \\
&= \frac{1}{n^2}\left[\sum_{i=1}^n \sigma^2 + 2 \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} \sigma_{ij}\right]\\
&= \frac{1}{n^2}\left[\sum_{i=1}^n \sigma^2 + 2 \sum_{i=1}^{n-1}\left(\sigma_{i,i+1} +\sigma_{i,i+2}+\sigma_{i,i+3} + \dots+\sigma_{i,n}\right)\right] \\
&= \frac{1}{n^2}\left[\sum_{i=1}^n \sigma^2 + 2 \sum_{i=1}^{n-1}\left(\rho \sigma^2 +\rho \sigma^2\right)\right] \\
&= \frac{1}{n^2}\left[n\sigma^2 + 4 (n-1) \rho \sigma^2\right] \\
& \to 0 \text{ as } n \to \infty.
\end{aligned}
\] Thus, by Theorem 5.3, \(\bar Y - \bar \mu \overset p \to 0\) as
\(n \to \infty\). As, in this case, \(\bar \mu = \mu\), it is shown that
\(\bar Y \overset p \to \mu\) as \(n \to \infty\).

\newpage

\hypertarget{section-27}{%
\section{5.2}\label{section-27}}

Suppose that \(Y_1, \dots, Y_n\) are independent random variables with
\(Y_n \sim N(\mu,\sigma_n^2)\), where the sequence
\(\sigma_n^2 \to \sigma^2 >0\) as \(n \to \infty\). Prove that there is
no random variable \(Y\) such that \(Y_n \overset{p}{\to}Y\). (Hint:
Assume there is such a \(Y\) and obtain a contradiction from
\(|Y_n-Y_{n+1}| \leq |Y_n-Y|+|Y_{n+1}-Y|\).)

Solution:

Assume \(Y_n \overset{p}{\to}Y\), then
\(|Y_n-Y_{n+1}| \leq |Y_n-Y|+|Y_{n+1}-Y|\) by the triangle inequality.

Note that \[\begin{aligned}
Y_n - Y_{n+1} &\sim N(0,\sigma_n^2+ \sigma_{n+1}^2) \\
\implies \lim_{n \to \infty} Y_n - Y_{n+1} &\sim N(0,2\sigma^2) \\
&\sim \sqrt{2\sigma}N(0,1) \\
\implies \lim_{n \to \infty} (Y_n - Y_{n+1})^2 &\sim {2\sigma^2}\chi^2(1) \\
\implies E \left[\lim_{n \to \infty}(Y_n - Y_{n+1})^2\right] &= {2\sigma^2} > 0.
\end{aligned}
\]

\[
\begin{aligned}
|Y_n-Y_{n+1}| &\leq |Y_n-Y|+|Y_{n+1}-Y| \\
\implies |Y_n-Y_{n+1}|^2 &\leq (|Y_n-Y|+|Y_{n+1}-Y|)^2 \\
\implies (Y_n-Y_{n+1})^2 &\leq (Y_n-Y)^2+2|Y_n-Y||Y_{n+1}-Y|+(Y_{n+1}-Y)^2 \\
\end{aligned}
\] Thus, \[
\begin{aligned}
0 < {2 \sigma^2} &= E\left[\lim_{n\to \infty}(Y_n-Y_{n+1})^2\right] \\
&\leq E\left[\lim_{n\to \infty}(Y_n-Y)^2+2|Y_n-Y||Y_{n+1}-Y|+(Y_{n+1}-Y)^2\right] \\
&\leq E\left[\lim_{n\to \infty}(0)^2+2|0||0|+(0)^2\right] ~~~~~~~~~~~~~~~~~~~~~~~(Y_n \overset p \to Y) \\
&= 0
\end{aligned}
\]

We have achieved a contradiction. Thus, there is no random variable
\(Y\) such that \(Y_n \overset{p}{\to}Y\).

\newpage

\hypertarget{section-28}{%
\section{5.3}\label{section-28}}

Show that \(Y_n \overset d \to c\) for some constant \(c\) implies
\(Y_n \overset p \to c\) by directly using the definitions of
convergence in probability and in distribution. Start with
\(P(|Y_n-c| > \epsilon)\).

Solution:

Assume \(Y_n \overset d \to c\), then
\(\lim_{n \to \infty} F_{Y_n}(y) = c\) for all points \(y\) where
\(F_{Y_n}(y)\) is continuous.

\[ \begin{aligned}
\lim_{n \to \infty}P(|Y_n-c| > \epsilon) &\leq \lim_{n \to \infty}\frac{E(Y_n-c)^2}{\epsilon^2} \\
&\leq \lim_{n \to \infty}\frac{\int_y(Y_n-c)^2 \frac{d}{dy}\left\{F_{Y_n}(y)\right\}dy}{\epsilon^2} \\
&\leq \lim_{n \to \infty}\frac{\int_y(Y_n-c)^2 (0)dy}{\epsilon^2} ~~~since~\lim_{n \to \infty} F_{Y_n}(y) = c\\
&= 0
\end{aligned}\]

Thus, \(Y_n \overset p \to c\) by definition.

\newpage

\hypertarget{section-29}{%
\section{5.5}\label{section-29}}

Consider the simple linear regression setting,
\(Y_i = \alpha + \beta x_i + e_i, ~~~i=1,\dots, n\), where the \(x_i\)
are known constants and \(e_1, \dots, e_n\) are iid with mean \(0\) and
finite variance \(\sigma^2\). After a little algebra, the least squares
estimator has the following representation,
\(\hat \beta - \beta = \frac{\sum_{i=1}^n (x_i-\bar x)e_i}{\sum_{i=1}^n (x_i-\bar x)^2}\).
Using that representation, prove that
\(\hat \beta \overset p \to \beta\) as \(n \to \infty\) if
\(\sum_{i=1}^n(x_i-\bar x)^2 \to \infty\)

Solution:

\[ \begin{aligned}
\lim_{n \to \infty} P(|\hat \beta - \beta| > \epsilon) &\leq \lim_{n \to \infty} \frac{E|\hat \beta - \beta|^r}{\epsilon^r} & \text{ by the Markov Inequality} \\
&\leq \lim_{n \to \infty} E \left|\frac{\sum_{i=1}^n (x_i-\bar x)e_i}{\epsilon \sum_{i=1}^n (x_i-\bar x)^2}\right|^r \\
&\leq \lim_{n \to \infty}E \left |\frac{\sum_{i=1}^n x_ie_i- n\bar x \bar e}{\epsilon \sum_{i=1}^n (x_i-\bar x)^2}\right|^r \\
&\leq \lim_{n \to \infty}E \left |\frac{\sum_{i=1}^n x_ie_i}{\epsilon \sum_{i=1}^n (x_i-\bar x)^2}\right|^r & \bar e \to 0 ~ by ~ WLLN\\
&\leq \lim_{n \to \infty}\left |\frac{\sum_{i=1}^n x_iE(e_i)}{\epsilon \sum_{i=1}^n (x_i-\bar x)^2}\right|^r & note,~E(e_i) =0, so~\lim_{n \to \infty}\sum_{i=1}^n x_iE(e_i)=0\\
&=0 & \text{ if } \lim_{n \to \infty} \sum_{i=1}^n (x_i-\bar x)^2= \infty
\end{aligned}\]

Thus, by definition, \(\hat \beta \overset p \to \beta\) as
\(n \to \infty\) if \(\sum_{i=1}^n(x_i-\bar x)^2 \to \infty\).

\hypertarget{section-30}{%
\section{5.9}\label{section-30}}

Let \(X_1, \dots, X_n\) be iid from a distribution with mean \(\mu\),
variance \(\sigma^2\), and finite central moments \(\mu_3\) and
\(\mu_4\). Consider \(\hat \theta = \bar X /s_n\), a measure of ``effect
size'' used in meta-analysis. Prove that
\(\hat \theta \overset p \to \theta = \mu/\sigma\) as \(n \to \infty\).

Solution:

\[ \begin{aligned}
\bar X  & \overset p \to \mu  & (WLLN) \\
s_n^2 & \overset p \to \sigma^2 & (Example~5.12,~p.~ 227) \\
\frac{1}{s_n} & \overset p \to \frac 1 \sigma & (Continuous~Mapping~Theorem) \\
\implies \frac{\bar X}{s_n} & \overset p \to \frac{\mu} \sigma & (Slutsky's~Theorem)
\end{aligned}\]

\newpage

\hypertarget{section-31}{%
\section{5.24}\label{section-31}}

When two independent binomials, \(X_1\) is binomial\((n_1,p_1)\) and
\(X_2\) is binomial\((n_2,p_2)\), are put in the form of a \(2\times 2\)
table (see Example 5.31, p.~240), then one often estimates the odds
ratio
\[\theta = \frac{\frac{p_1}{1-p_1}}{\frac{p_2}{1-p_2}} = \frac{p_1(1-p_2)}{p_2(1-p_1)}.\]
The estimate \(\hat \theta\) is obtained by inserting
\(\hat{p_1} = X_1/n_1\) and \(\hat p_2 = X_2/n_2\) in the above
expression. Show that \(\log(\hat \theta)\) has asymptotic variance
\[\frac{1}{n_1p_1(1-p_1)}+\frac{1}{n_1p_2(1-p_2)}.\]

Solution:

Let \(\lambda = \frac{n_1}{n_1+n_2}= \frac{n_1}{n}\) Then, following
class notes and by central limit theorem,

\[
\sqrt n  \ \begin{pmatrix} \left(\frac{X_1}{n_1} - p_1 \right) \\
\left( \frac{X_2}{n_2} - p_2 \right) \end{pmatrix} \overset d \to MVN \left(\mathbf 0, \begin{bmatrix} \frac{p_1 \left(1 - p_1 \right)}{\lambda} & 0 \\
0 & \frac{p_2 \left(1 - p_2 \right)}{1-\lambda} \end{bmatrix}  \right)
\] Define \[ \begin{aligned}
g \begin{pmatrix} p_1 \\ p_2 \end{pmatrix} &= \log \left(\frac{p_1(1-p_2)}{p_2(1-p_1)}\right) \\
\implies g' \begin{pmatrix} p_1 \\ p_2 \end{pmatrix}  &= \begin{pmatrix} \frac{p_2(1-p_1)}{p_1(1-p_2)} \frac{p_2(1-p_1)(1-p_2)- p_1(1-p_2)(-p_2)}{p_2^2(1-p_1)^2} \\ \frac{p_2(1-p_1)}{p_1(1-p_2)} \frac{p_2(1-p_1)(-p_1)- p_1(1-p_2)(1-p_1)}{p_2^2(1-p_1)^2}  \end{pmatrix} \\
&=\begin{pmatrix} \frac{p_2(1-p_1)}{p_1(1-p_2)}  \frac{(1-p_1)(1-p_2)+ p_1(1-p_2)}{p_2(1-p_1)^2} \\ \frac{p_2(1-p_1)}{p_1(1-p_2)} \frac{p_2(-p_1)- p_1(1-p_2)}{p_2^2(1-p_1)}  \end{pmatrix} \\
&=\begin{pmatrix} \frac{p_2(1-p_1)}{p_1(1-p_2)} \frac{1-p_2}{p_2(1-p_1)^2} \\ \frac{p_2(1-p_1)}{p_1(1-p_2)} \frac{-p_1}{p_2^2(1-p_1)}  \end{pmatrix} \\
&=\begin{pmatrix} \frac{1}{p_1(1-p_1)} \\ \frac{-1}{(1-p_2)p_2}\end{pmatrix}
\end{aligned}\]

Thus, by delta method, \[\begin{aligned}
\log(\hat \theta) &= g \begin{pmatrix} \frac{X_1}{n_1} - p_1 \\ \frac{X_2}{n_2} - p_2 \end{pmatrix} \\
&\sim AN \left( \begin{pmatrix} p_1 \\ p_2 \end{pmatrix},\begin{pmatrix} \frac{1}{p_1(1-p_1)} \\ \frac{-1}{(1-p_2)p_2}\end{pmatrix}^T \begin{pmatrix} \frac{p_1 \left(1 - p_1 \right)}{\lambda} & 0 \\
0 &  \frac{p_2 \left(1 - p_2 \right)}{1-\lambda} \end{pmatrix} \begin{pmatrix} \frac{1}{p_1(1-p_1)} \\ \frac{-1}{(1-p_2)p_2}\end{pmatrix}/n\right) \\
&\sim AN \left( \begin{pmatrix} p_1 \\ p_2 \end{pmatrix},\begin{pmatrix} \frac{1}{\lambda} & \frac{1}{1-\lambda}  \end{pmatrix} \begin{pmatrix} \frac{1}{p_1(1-p_1)} \\ \frac{-1}{(1-p_2)p_2}\end{pmatrix}/n\right)\\
&\sim AN \left( \begin{pmatrix} p_1 \\ p_2 \end{pmatrix},\left( \frac{1}{\lambda p_1(1-p_1)} + \frac{1}{(1-p_2)p_2 (1-\lambda)}\right)/n\right)\\
&\sim AN \left( \begin{pmatrix} p_1 \\ p_2 \end{pmatrix},\left( \frac{1}{\left(\frac{n_1}{n}\right) p_1(1-p_1)} + \frac{1}{(1-p_2)p_2 \left(1-\left(\frac{n_1}{n_1+n_2}\right)\right)}\right)/n\right)\\
&\sim AN \left( \begin{pmatrix} p_1 \\ p_2 \end{pmatrix},\left( \frac{1}{n_1 p_1(1-p_1)} + \frac{1}{n_2p_2(1-p_2) }\right)\right)\\
\end{aligned}\]

\newpage

\hypertarget{a-2}{%
\section{5.27 a)}\label{a-2}}

For an iid sample \(Y_1,\dots, Y_n\), consider finding the asymptotic
joint distribution of \((\bar Y, s_n, s_n/ \bar Y)\) using Theorem 5.20
(p.~239) and (5.34, p.~256). Find the matrices
\(\mathbf{g'(\boldsymbol \theta)}\) and \(\boldsymbol \Sigma\) used to
compute the asymptotic covariance
\(\mathbf{g'(\boldsymbol \theta) \boldsymbol \Sigma g'(\boldsymbol \theta)^T}\).

Solution:

By Theorem 5.20, \[ \begin{aligned}
\sqrt n \begin{pmatrix} \bar Y - \mu \\  s_n^2-\sigma^2\end{pmatrix} &\overset d \to N\left(\mathbf 0, \begin{pmatrix} \sigma^2 & \mu_3 \\ \mu_3 & \mu_4-\sigma^4 \end{pmatrix} \right) \\
\implies \begin{pmatrix} \bar Y \\  s_n^2 \end{pmatrix} & is~ AN\left( \begin{pmatrix} \mu \\  \sigma^2\end{pmatrix}, \begin{pmatrix} \sigma^2 & \mu_3 \\ \mu_3 & \mu_4-\sigma^4 \end{pmatrix}/n \right)
\end{aligned}\]

Define
\(g \begin{pmatrix} a \\b \end{pmatrix} = \begin{pmatrix} a \\ \sqrt b \\ \frac {\sqrt b} a \end{pmatrix}\)
Then,
\(g' \begin{pmatrix} a \\b \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & \frac{1}{2 \sqrt b} \\ \frac{-\sqrt b}{a^2} & \frac{1}{2a\sqrt b}\end{pmatrix}\).

By Delta Method,

\[\begin{aligned}
g\begin{pmatrix} \bar Y \\  s_n^2 \end{pmatrix} & is~ AN\left( g \begin{pmatrix} \mu \\  \sigma^2\end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 0 & \frac{1}{2 \sqrt b} \\ \frac{-\sqrt b}{a^2} & \frac{1}{2a\sqrt b}\end{pmatrix}\begin{pmatrix} \sigma^2 & \mu_3 \\ \mu_3 & \mu_4-\sigma^4 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & \frac{1}{2 \sqrt b} \\ \frac{-\sqrt b}{a^2} & \frac{1}{2a\sqrt b}\end{pmatrix}^T/n\right)
\end{aligned}\]

In conclusion, \[ \begin{aligned}
g'(\boldsymbol \theta) =\begin{pmatrix} 1 & 0 \\ 0 & \frac{1}{2 \sqrt b} \\ \frac{-\sqrt b}{a^2} & \frac{1}{2a\sqrt b}\end{pmatrix} \\
\boldsymbol \Sigma = \begin{pmatrix} \sigma^2 & \mu_3 \\ \mu_3 & \mu_4-\sigma^4 \end{pmatrix} \\
g'(\boldsymbol \theta)^T =\begin{pmatrix} 1 & 0 &  \frac{-\sqrt b}{a^2} \\ 0 & \frac{1}{2 \sqrt b} & \frac{1}{2a\sqrt b}\end{pmatrix} \\
 \end{aligned}\]

\newpage

\hypertarget{section-32}{%
\section{5.29}\label{section-32}}

In most of Chapter 5 we have dealt with iid samples of size \(n\) of
either univariate or multivariate random variables. Another situation of
interest is when we have a number of different independent samples of
different sizes. For simplicity, consider the case of two iid samples,
\(X_1, \dots, X_m\) and \(Y_1, \dots Y_n\), with common variance
\(\sigma^2\) and under a null hypothesis they have a common mean, say
\(\mu\). Then the two-sample pooled t statistic is
\[t_p = \frac{\bar X- \bar Y}{\sqrt{s_p^2(\frac{1}{m}+ \frac 1 n)}},\]
where \[s_p^2= \frac{(m-1)s_X^2 +(n-1)s_Y^2}{m+n-2}\] and
\[s_X^2 = \frac{1}{m-1} \sum_{i=1}^m (X_i-\bar X)^2,~~s_Y^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2.\]
It can be shown that \(t_p \overset d \to N(0,1)\) as
\(\min(m,n) \to \infty\). However, the proof is fairly tricky. Instead
it is common to assume that both sample sizes go to 1 at a similar rate,
i.e., \(\lambda_{m,n} = m/(m+n) \to \lambda >0\) as
\(\min(m,n) \to \infty\). Under this assumption prove that
\(t_p \overset d \to N(0,1)\). Hint: show that
\(t_p = \left[\sqrt{1- \lambda_{m,n}}\sqrt m (\bar X-\mu)- \sqrt{1- \lambda_{m,n}}\sqrt n (\bar Y-\mu) \right]/s_p\).

Solution:

\[\begin{aligned}
t_p &= \frac{\bar X- \bar Y}{\sqrt{s_p^2(\frac{1}{m}+ \frac 1 n)}} \\
&= \frac{(\bar X - \mu)- (\bar Y-\mu)}{s_p\sqrt{\frac{n+m}{nm}}} \\
&= \frac{\sqrt{\frac{nm}{n+m}}(\bar X - \mu)- \sqrt{\frac{nm}{n+m}}(\bar Y-\mu)}{s_p} \\
&= \frac{\sqrt{\frac{n+m-m}{n+m}} \sqrt m(\bar X - \mu)- \sqrt{\frac{m}{n+m}}\sqrt n(\bar Y-\mu)}{s_p} \\
&= \frac{\sqrt{1-\lambda_{m,n}} \sqrt m(\bar X - \mu)- \sqrt{\lambda_{m,n}}\sqrt n(\bar Y-\mu)}{s_p} \\ 
\end{aligned}\]

Note that: \[\begin{aligned}
s_p^2 &= \frac{(m-1)s_X^2 +(n-1)s_Y^2}{m+n-2} \\
&\overset P \to \frac{(m-1)\sigma^2 +(n-1)\sigma^2}{m+n-2} \\
&= \frac{(m+n-2)\sigma^2}{m+n-2} = \sigma^2 \\
\implies s_p &\overset P \to \sigma \\
\sqrt m(\bar X - \mu) &\overset d \to N(0,\sigma^2) \\
\sqrt n(\bar Y - \mu) &\overset d \to N(0,\sigma^2)
\end{aligned}\]

\[\begin{aligned}
t_p &= \frac{\sqrt{1-\lambda_{m,n}} \sqrt m(\bar X - \mu)- \sqrt{\lambda_{m,n}}\sqrt n(\bar Y-\mu)}{s_p} \\
& \overset d \to \frac{\sqrt{1-\lambda}N(0,\sigma^2)- \sqrt{\lambda}N(0,\sigma^2)} \sigma \\
& \overset d \to N(0,1-\lambda)-N(0, \lambda) \\
& \overset d \to N(0,1)
\end{aligned}\]

\newpage

\hypertarget{section-33}{%
\section{5.33}\label{section-33}}

Let \((X_1,Y_1),\dots ,(X_n,Y_n)\) be iid pairs with \(E(X_1) = \mu_1\),
\(E(Y_1) =\mu_2\), \(Var(X_1) = \sigma_1^2\), \(Var(Y_1) = \sigma_2^2\),
and \(Cov(X_1, Y_1) = \sigma_{12}\).

\hypertarget{a.-6}{%
\subsection{a.}\label{a.-6}}

What can we say about the asymptotic distribution of
\((\bar X, \bar Y)^T\)?

Solution:

By Central Limit theorem, \((\bar X, \bar Y)^T\)is
\(AN\left( \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{pmatrix} \bigg/n \right)\).

\hypertarget{b.-suppose-that-mu_1mu_20-and-let-t-bar-xbar-y.-show-that-nt-overset-d-to-q-as-n-to-infty-and-describe-the-random-variable-q.}{%
\subsection{\texorpdfstring{b. Suppose that \(\mu_1=\mu_2=0\) and let
\(T = (\bar X)(\bar Y)\). Show that \(nT \overset d \to Q\) as
\(n \to \infty\) and describe the random variable
Q.}{b. Suppose that \textbackslash mu\_1=\textbackslash mu\_2=0 and let T = (\textbackslash bar X)(\textbackslash bar Y). Show that nT \textbackslash overset d \textbackslash to Q as n \textbackslash to \textbackslash infty and describe the random variable Q.}}\label{b.-suppose-that-mu_1mu_20-and-let-t-bar-xbar-y.-show-that-nt-overset-d-to-q-as-n-to-infty-and-describe-the-random-variable-q.}}

Solution:

\textbf{Attempt 1: INCORRECT if X and Y are correlated}

By CLT, \(\sqrt n \bar X \overset d \to N(0,\sigma_1^2)\),
\(\sqrt n \bar Y \overset d \to N(0,\sigma_2^2)\). Thus,
\[\begin{aligned}\frac{\sqrt n \bar X}{\sigma_1} &\overset d \to N(0,1)\\
\frac{\sqrt n \bar Y}{\sigma_2} &\overset d \to N(0,1)\\
\implies \left(\frac{\sqrt n \bar X}{\sigma_1} \right)\left(\frac{\sqrt n \bar Y}{\sigma_2} \right) &\overset d \to \chi^2(1) ~~~ \mathbf{assumes}~~ \sigma_{12}=0\\
\implies n \bar X \bar Y &\overset d \to \sigma_1\sigma_2\chi^2(1)
\end{aligned} \]

\textbf{Attempt 2: INCORRECT if \(\sigma_{12} =0\), we should get a
\(\chi^2\) distribution}

Note that \(\bar X -0= \frac{1}{n} \sum_{i=1}^n h_1(X_i,Y_i) + R_{n1}\),
where \(h_1(X_i) = X_i\) and \(R_{n1} = 0\). Similarly,
\(\bar Y - 0 = \frac{1}{n} \sum_{i=1}^n h_2(X_i,Y_i) + R_{n2}\), where
\(h_2(Y_i) = Y_i\) and \(R_{n2} = 0\). Note, in this case
\(E(h_1(X_i,Y_i)) =E(h_2(X_i,Y_i)) =0\).

Define \(g((a,b)^T) = ab\), which implies \(g'(a,b) = (b,a)^T\). Note
that g is a real valued function with partial derivatives existing in
the neighborhood of \(\mathbf 0\) and is continuous at \(\mathbf 0\). By
Thm 5.27, \[\begin{aligned}
g((\bar X, \bar Y)) -g(\boldsymbol{\theta}) &= \frac{1}{n} \sum_{i=1}^n \left[g_1' (\boldsymbol \theta)h_1(X_i,Y_i)+g_2'(\boldsymbol \theta)h_2(X_i,Y_i)\right] + R_n, ~~where~\sqrt n R_n \overset p \to 0 \\
\bar X \bar Y -g(\mathbf 0) &= \frac{1}{n} \sum_{i=1}^n \left[g_1' (\mathbf 0)X_i+g_2'(\mathbf 0)Y_i\right] + R_n \\
&= \frac{1}{n} \sum_{i=1}^n [0] + R_n \\
\end{aligned}\] Thus, by Theorem 5.23,
\(\bar X \bar Y \overset d \to N(0,0)\)

\textbf{Attempt 3: LEADS NOWHERE}

Define \(g((a,b)^T) = ab\). Then, by (Taylor, 1712)

\[\begin{aligned}
g((\bar X,  \bar Y)^T) - g((\mu_1,\mu_2)^T) &= \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix}^T \begin{pmatrix} \mu_2 \\ \mu_1 \end{pmatrix} + \frac 1 2 \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix}^T \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix} \\
&~~~~~~~~~+ \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix}^T \begin{pmatrix} 0 & 0 & 0 & 0\\ 0 & 0& 0 &0 \end{pmatrix} \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix} \otimes\begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix} \\
&= \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix}^T \begin{pmatrix} \mu_2 \\ \mu_1 \end{pmatrix} + \frac 1 2 \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix}^T \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} \bar X - \mu_1  \\ \bar Y- \mu_2\end{pmatrix} 
\end{aligned}\] Thus, in the case where \(\mu_1=\mu_2= 0\)
\[\begin{aligned}
g((\bar X,  \bar Y)^T) - g((0,0)^T) &=  \begin{pmatrix} \bar X  & \bar Y \end{pmatrix} \begin{pmatrix} 0 \\ 0 \end{pmatrix} + \frac 1 2 \begin{pmatrix} \bar X & \bar Y\end{pmatrix} \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} \bar X  \\ \bar Y \end{pmatrix} \\
\implies \bar X \bar Y &= 0 + \frac 1 2 \begin{pmatrix} \bar Y & \bar X\end{pmatrix} \begin{pmatrix} \bar X  \\ \bar Y \end{pmatrix} =  \bar X \bar Y
\end{aligned}\]

So doing a Taylor expansion is not useful.

\hypertarget{c.-2}{%
\subsection{c.}\label{c.-2}}

Suppose that \(\mu_1 = 0, \mu_2 \neq 0\) and let
\(T = (\bar X) (\bar Y)\). Show that \(\sqrt n T \overset d \to R\) as
\(n \to \infty\) and describe the random variable \(R\).

Solution:

Define \(g (a , b)^T = ab \implies g( \bar X, \bar Y)= \bar X \bar Y\),
\(g'\begin{pmatrix} a\\ b \end{pmatrix} = \begin{pmatrix} b \\ a \end{pmatrix}\).
By Theorem 5.26,

\[\begin{aligned}
g\begin{pmatrix} \bar X \\ {\bar Y} \end{pmatrix} - g\begin{pmatrix} {0} \\ \mu_2 \end{pmatrix} 
&= \frac 1 n \sum_{i=1}^n \begin{pmatrix} \mu_2 & 0 \end{pmatrix} \begin{pmatrix} X_i \\ Y_i - \mu_2 \end{pmatrix} + R_n,  ~~ where \sqrt n R_n \overset p \to 0 \\
&= \mu_2 \bar X + R_n \\
\implies  \sqrt n \left( g\begin{pmatrix} \bar X \\ {\bar Y} \end{pmatrix} - g\begin{pmatrix} {0} \\ \mu_2 \end{pmatrix} \right) &=  \mu_2(\sqrt n \bar X) +\sqrt n R_n \\
\implies  \sqrt n  \bar X {\bar Y}  &=  \mu_2(\sqrt n \bar X) +\sqrt n R_n \\
& \overset d \to N(0, \sigma_1^2 + \mu_2^2)
\end{aligned}\]

\hypertarget{section-34}{%
\section{5.32}\label{section-34}}

Suppose that \(\hat \theta_1, \dots, \hat \theta_k\) each satisfy the
assumptions of Theorem 5.23 (p.~242):
\[\hat \theta_i - \theta_i = \frac 1 n \sum_{j=1}^n h_i(X_j) + R_{in},~~~\sqrt n R_{in} \overset p \to 0,\]
and \(E[h_i(X_1)] = 0\) and \(var[h_i(X_1)] = \sigma_{hi}^2 < \infty\).
Let \(T = \sum_{i=1}^k c_i \hat \theta_i\) for any set of constants
\(c_1, \dots, c_k\). Find the correct approximating function \(h_T\) for
\(T\), show that Theorem 5.23 may be used (verify directly without using
later theorems), and find the limiting distribution of \(T\).

Solution:

\[\begin{aligned}
\hat \theta_i - \theta_i &= \frac 1 n \sum_{j=1}^n h_i(X_j) + R_{in},~~~\sqrt n R_{in} \overset p \to 0 \\
\implies c_i(\hat \theta_i - \theta_i) &= c_i\left(\frac 1 n \sum_{j=1}^n h_i(X_j) + R_{in} \right),~~~\sqrt n R_{in} \overset p \to 0 \\
\implies c_i \hat \theta_i - c_i\theta_i &= \frac 1 n \sum_{j=1}^n c_ih_i(X_j) + c_iR_{in},~~~\sqrt n R_{in} \overset p \to 0 \\
\end{aligned}\]

Note that \(c_i \sqrt nR_{in} \overset p \to 0\) by Slutsky's Theorem.
\(E[c_i h_i (X_i)] = 0\),
\(Var[c_i h_i (X_1)] = c_i^2 \sigma_{hi}^2 < \infty\).

\[\begin{aligned}
c_i \hat \theta_i - c_i\theta_i &= \frac 1 n \sum_{j=1}^n c_ih_i(X_j) + c_iR_{in},~~~\sqrt n c_iR_{in} \overset p \to 0 \\
\implies \sum_{i=1}^k c_i \hat \theta_i - c_i\theta_i &= \sum_{i=1}^k \frac 1 n \sum_{j=1}^n c_ih_i(X_j) + c_iR_{in},~~~\sqrt n c_iR_{in} \overset p \to 0 \\
\implies T -\sum_{i=1}^k c_i\theta_i &= \frac 1 n \sum_{j=1}^n \left(\left[\sum_{i=1}^k c_ih_i(X_j) \right] + \left[\sum_{i=1}^k c_iR_{in} \right]\right),~~~\sqrt n c_iR_{in} \overset p \to 0 \\
\end{aligned}\]

As \(\left[\sum_{i=1}^k c_iR_{in} \right] \overset p \to 0\) because a
finite sum of random variables that converge in probability to zero will
converge in probability to 0 by Slutsky's Theorem.

Therefore, the correct approximating of T is
\(h_T =\left[\sum_{i=1}^k c_ih_i(X_j) \right]\), which has mean 0 and
variance \(\sum_{i=1}^k c_i^2 \sigma_{hi}^2 < \infty\).

Thus, all conditions apply to use Theorem 5.23 and
\(\sqrt n\left(T -\sum_{i=1}^k c_i\theta_i \right) \overset d \to N\left(0, \sum_{i=1}^k c_i^2 \sigma_{hi}^2\right)\).
In other words, the limiting distribution of \(T\) is
\(AN\left(0, \frac 1 n \sum_{i=1}^k c_i^2 \sigma_{hi}^2\right)\).

\newpage

\hypertarget{section-35}{%
\section{5.40}\label{section-35}}

Formulate an extension of Theorem 5.27 (p.~247) for the situation of two
independent samples \(X_1, \dots, X_m\) and \(Y_1, \dots, Y_n\). The
statistic of interest is \(T = g(\hat \theta_1, \hat \theta_2)\), and
the conclusion is
\[g(\hat \theta_1, \hat \theta_2) - g(\theta_1, \theta_2) = \frac 1 m \sum_{i=1}^m g_1'(\boldsymbol \theta) h_1(X_i) + \frac 1 n \sum_{i=1}^n g_2'(\boldsymbol \theta) h_2(Y_i) + R_{mn},~~ \sqrt{\max(m,n)} R_{m,n} \overset p \to 0.\]

Solution:

I define \(\boldsymbol \theta = (\theta_1, \theta_2)^T\)

\[ \begin{aligned}
g(\hat \theta_1, \hat \theta_2) - g(\theta_1, \theta_2) &= (\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2) g'(\theta_1,\theta_2) + \frac{1}{2}(\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2) g^{(2)}(\theta^*) (\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2)^T \\
\end{aligned}
\] For some \(\boldsymbol \theta^*\) in the neighborhood between
\(\boldsymbol {\hat \theta}\) and \(\boldsymbol \theta\). Looking
carefully at the first component of the right hand side,
\[ \begin{aligned}
(\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2) g'(\theta_1,\theta_2) &=
(\hat \theta_1 - \theta_1)g_1'(\boldsymbol \theta) +(\hat \theta_2 - \theta_2)g_2'(\boldsymbol \theta) \\
by~ Theorem~5.27&= \left(\frac 1 m \sum_{i=1}^m h_1(X_i) +R_{m1} \right)g_1'(\boldsymbol \theta) +\left(\frac 1 n \sum_{i=1}^n h_2(Y_i) +R_{n2} \right)g_2'(\boldsymbol \theta),~~where~ \substack{\sqrt n R_{n2} \overset p \to 0 \\ \sqrt m R_{m1} \overset p \to 0 } \\
&= \frac 1 m \sum_{i=1}^m h_1(X_i)  g_1'(\boldsymbol \theta) +\frac 1 n \sum_{i=1}^n h_2(Y_i)  g_2'(\boldsymbol \theta)+R_{m1}g_1'(\boldsymbol \theta)+R_{n2}g_2'(\boldsymbol \theta)
\end{aligned}
\] Combining this with the second component, we get:

\[g(\hat \theta_1, \hat \theta_2) - g(\theta_1, \theta_2) = \frac 1 m \sum_{i=1}^m h_1(X_i)  g_1'(\boldsymbol \theta) +\frac 1 n \sum_{i=1}^n h_2(Y_i)  g_2'(\boldsymbol \theta) + R_{mn}\]
where,
\[R_{mn} = R_{m1}g_1'(\boldsymbol \theta)+R_{n2}g_2'(\boldsymbol \theta) + \frac{1}{2}(\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2) g^{(2)}(\theta^*) (\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2)^T.\]

Note that \(\sqrt m R_{m1}g_1'(\boldsymbol \theta) \overset p \to 0\)
and \(\sqrt n R_{n2}g_2'(\boldsymbol \theta) \overset p \to 0\) by
Slutsky's Theorem. To finish this prove I only need to show
\(\frac{1}{2}(\hat \theta_1 - \theta_1 , \hat \theta_2 - \theta_2) g^{(2)}(\theta^*) (\hat \theta_1 - \theta_1 , \hat \theta_2 - \theta_2)^T \overset p \to 0\)

\[
\begin{aligned}
\frac{1}{2}&(\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2) g^{(2)}(\boldsymbol \theta^*) (\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2)^T = \frac{1}{2}(\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2)\begin{pmatrix}g_{11}^{(2)}(\boldsymbol \theta^*) &g_{12}^{(2)}(\boldsymbol \theta^*) \\ g_{21}^{(2)}(\boldsymbol \theta^*) &g_{22}^{(2)}(\boldsymbol \theta^*)\end{pmatrix} (\hat \theta_1  - \theta_1 , \hat \theta_2 - \theta_2)^T \\
&= \frac{1}{2}\left((\hat \theta_1  - \theta_1)g_{11}^{(2)}(\boldsymbol \theta^*) +(\hat \theta_2  - \theta_2)g_{21}^{(2)}(\boldsymbol \theta^*), (\hat \theta_1  - \theta_1)g_{12}^{(2)}(\boldsymbol \theta^*) +(\hat \theta_2  - \theta_2)g_{22}^{(2)}(\boldsymbol \theta^*)\right) \begin{pmatrix} \hat \theta_1  - \theta_1 \\ \hat \theta_2  - \theta_2\end{pmatrix} \\
&= \frac{1}{2}\left((\hat \theta_1  - \theta_1)^2g_{11}^{(2)}(\boldsymbol \theta^*) +(\hat \theta_1  - \theta_1)(\hat \theta_2  - \theta_2)g_{21}^{(2)}(\boldsymbol \theta^*) + (\hat \theta_1  - \theta_1)(\hat \theta_2  - \theta_2)g_{12}^{(2)}(\boldsymbol \theta^*) +(\hat \theta_2  - \theta_2)^2g_{22}^{(2)}(\boldsymbol \theta^*)\right) 
\end{aligned}
\] We know that \(\sqrt m (\hat \theta_1 - \theta_1) \to 0\) and
\(\sqrt n (\hat \theta_2 - \theta_2) \to 0\). This implies
\(\sqrt {\max(m,n)} (\hat \theta_1 - \theta_1) \to 0\) and
\(\sqrt {\max(m,n)} (\hat \theta_2 - \theta_2) \to 0\).

Accordingly, \(\max(m,n) (\hat \theta_1 - \theta_1)^2 \to 0\),
\(\max(m,n) (\hat \theta_2 - \theta_2)^2 \to 0\), and
\(\max(m,n) (\hat \theta_1 - \theta_1)(\hat \theta_2 - \theta_2) \to 0\).

Thus,
\[\frac{\max(m,n)}{2}\left((\hat \theta_1  - \theta_1)^2g_{11}^{(2)}(\boldsymbol \theta^*) +(\hat \theta_1  - \theta_1)(\hat \theta_2  - \theta_2)g_{21}^{(2)}(\boldsymbol \theta^*) + (\hat \theta_1  - \theta_1)(\hat \theta_2  - \theta_2)g_{12}^{(2)}(\boldsymbol \theta^*) +(\hat \theta_2  - \theta_2)^2g_{22}^{(2)}(\boldsymbol \theta^*)\right) \overset p \to 0\]

I have shown
\[g(\hat \theta_1, \hat \theta_2) - g(\theta_1, \theta_2) = \frac 1 m \sum_{i=1}^m h_1(X_i)  g_1'(\boldsymbol \theta) +\frac 1 n \sum_{i=1}^n h_2(Y_i)  g_2'(\boldsymbol \theta) + R_{mn},\]
where \(\sqrt{max(m,n)} R_{mn} \overset p \to 0\).

\hypertarget{section-36}{%
\section{5.42}\label{section-36}}

Thinking of the \(k^{th}\) central moment as a functional,
\(T_k(F)= \int (t-\mu)^k d F(t)\), show that the Gateaux derivative is
given by
\(T_k(F;\Delta) = \int \left\{t-T_1(F)\right\}^k d \Delta(t) - T_1(F;\Delta) \int k \left\{t-T_1(F) \right\}^{k-1} d F(t)\),
where \(T_1(F;\Delta) = \int t d \Delta(t)\) is the Gateaux derivative
for the mean functional diven in Example 5.5.8i (p.253). Then,
substitute \(\Delta(t)= \delta_x(t)- F(t)\) and obtain \(h_k\) given in
Theorem 5.24 (p.~243).

Solution:

\[
\begin{aligned}
T_k(F) &= \int (t-\mu)^k d F(t) =\int \left(t-\int_s s dF(s)\right)^k dF(t) \\
\implies T_k(F;\Delta) &= \frac{\partial}{\partial \epsilon} T_k(F+\epsilon \Delta) \bigg|_{\epsilon = 0^+} = \int\frac{\partial}{\partial \epsilon} \left\{ \left(t-\int_s s d\{F(s)+\epsilon \Delta(s) \}\right)^k d \{F(t)+\epsilon \Delta(t) \} \right\}  \bigg|_{\epsilon = 0^+} \\
&= \int k\left(t-\int_s s d\{F(s)+\epsilon \Delta(s) \}\right)^{k-1}\left(-\int_s s d \Delta(s)\right) d \{F(t)+\epsilon \Delta(t) \} \\
&~~~~~~~~~~~~~~~~~~~+\left(t-\int_s s d\{F(s)+\epsilon \Delta(s) \}\right)^k d\Delta(t) \bigg|_{\epsilon = 0^+} \\
&= \int k\left(t-\int_s s dF(s)\right)^{k-1}\left(-\int_s s d \Delta(s)\right) d F(t) +\left(t-\int_s s dF(s)\right)^k d\Delta(t)  \\
&=\int \left(t-T_1(F)\right)^k d\Delta(t) -T_1(F;\Delta)\int k\left(t-T_1(F)\right)^{k-1} d F(t)   \\
\end{aligned}
\] Substituting \(\Delta(t)= \delta_x(t)- F(t)\), and noting that
\(T_1(F;\Delta) = x-\mu\) \[
\begin{aligned}
&= \int \left(t-T_1(F)\right)^k d\left\{\delta_x(t)- F(t)\right\} -T_1(F;\Delta)\int k\left(t-T_1(F)\right)^{k-1} d F(t) \\
&= \int \left(t-\mu\right)^k d\delta_x(t)- \int \left(t-\mu\right)^kdF(t) -(x-\mu)\int k\left(t-\mu\right)^{k-1} d F(t) \\
&= \left(x-\mu\right)^k - \mu_k -k (x-\mu)\mu_{k-1}  
\end{aligned}
\] Which is the same \(h_k\) given in Theorem 5.24 (p.~243).

\newpage

\hypertarget{section-37}{%
\section{5.44}\label{section-37}}

A location M-estimator may be represented as \(T(F_n)\), where
\(T(\cdot)\) satisfies \[\int \psi (t- T(F)) d F(t) = 0,\] and \(\psi\)
is a known differentiable function. Using implicit differentiation, show
that the Gateaux derivative is
\(T(F;\Delta) = \frac{\int \psi (t- T(F))d\Delta(t)}{\int \psi' (t- T(F))dF(t)}\),
then substitute \(\Delta(t) = \delta_x (t) - F(t)\) and obtain the
influence function \(h(x)\).

Solution:

\[
\begin{aligned}
0=T(F;\Delta)  &= \frac{\partial}{\partial \epsilon} T(F+\epsilon \Delta) \bigg|_{\epsilon = 0^+}=\frac{\partial}{\partial \epsilon} \int \psi (t- T(F(t)+\epsilon \Delta(t))) d \{F(t)+\epsilon \Delta(t) \}  \bigg|_{\epsilon = 0^+}\\
&=\int \psi' (t- T(F(t)+\epsilon \Delta(t)))\left(-T'(F(t)+\epsilon \Delta(t))\right)\Delta(t)d \{F(t)+\epsilon \Delta(t) \}  \bigg|_{\epsilon = 0^+}\\ 
&~~~~~~~~~~~~~~~~~+\int \psi (t- T(F(t)+\epsilon \Delta(t)))d \Delta(t)  \bigg|_{\epsilon = 0^+}\\
0&=\int -T'(F(t))\psi' (t- T(F(t)))\Delta(t)dF(t)  +\int \psi (t- T(F(t)))d \Delta(t)
\end{aligned}
\] \[
\begin{aligned}
\implies \int T'(F)\psi' (t- T(F))\Delta(t)dF&= \int \psi (t- T(F(t)))d \Delta(t)\\
\\
\implies T'(F) = T(F; \Delta) &= \frac{\int \psi (t- T(F))d\Delta(t)}{\int \psi' (t- T(F))dF(t)}
\end{aligned}
\] Substituting \(\Delta(t) = \delta_x (t) - F(t)\),

\[
\begin{aligned}
\frac{\int \psi (t- T(F))d\Delta(t)}{\int \psi' (t- T(F))dF(t)} &= \frac{\int \psi (t- T(F))d\{\delta_x (t) - F(t)\}}{\int \psi' (t- T(F))dF(t)} \\
&= \frac{\psi (x- T(F)) - \int \psi (t- T(F))dF(t)}{\int \psi' (t- T(F))dF(t)} \\
&= \frac{\psi (x- T(F))}{\int \psi' (t- T(F))dF(t)} \\
\end{aligned}
\]

Thus, \(h(x) = \frac{\psi (x- T(F))}{\int \psi' (t- T(F))dF(t)}\)

\newpage

\hypertarget{section-38}{%
\section{5.45}\label{section-38}}

One representation of a ``smooth'' linear combination of order
statistics is \(T(F_n)\), where \(T(F)= \int_0^1 J(p) F^{-1} (p)dp\),
and \(J\) is a weighting function. Using the results in Example 5.5.8j
(p.~254), find the influence function \(h(x)\).

Solution:

\[
\begin{aligned}
T(F)&= \int_0^1 J(p) F^{-1} (p)dp \\
T(F;\Delta)  &= \frac{\partial}{\partial \epsilon} T(F+\epsilon \Delta) \bigg|_{\epsilon=0^+} \\
&= \frac{\partial}{\partial \epsilon} \int_0^1 J(p) (F+\epsilon \Delta)^{-1} (p)dp \bigg|_{\epsilon=0^+} \\
&=\int_0^1 \left[\frac{\partial}{\partial \epsilon} \{J(p)\} (F+\epsilon \Delta)^{-1} (p)\right] +\left[ J(p) \frac{\partial}{\partial \epsilon}\{(F+\epsilon \Delta)^{-1} (p)\}\right]dp \bigg|_{\epsilon=0^+} \\
&=\int_0^1  J(p) \frac{\partial}{\partial \epsilon}\{(F+\epsilon \Delta)^{-1} (p)\}dp \bigg|_{\epsilon=0^+} \\
&=-\int_0^1  J(p) \frac{\Delta (p)}{F'(p)} dp ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \text{by Example 5.5.8j results}\\ 
\end{aligned}
\] Setting \(\Delta(p) = \delta_{x}(p) - F(p)\) \[
\begin{aligned}
h(x) &= -\int_0^1  J(p) \frac{\Delta (p)}{F'(p)} dp \\
&= -\int_0^1  J(p) \frac{\delta_{x}(p) - F(p)}{F'(p)} dp \\
&= \int_0^1  J(p) \frac{F(p)-\delta_{x}(p)}{F'(p)} dp
\end{aligned}
\]

\newpage

\hypertarget{section-39}{%
\section{5.48}\label{section-39}}

Use Theorem 5.4 (p.~219), the univariate CLT and Theorem 5.31 (p.~256)
to prove Theorem 5.7 (p.~255, the multivariate CLT). Perhaps it is
easier to use an alternate statement of the conclusion of the univariate
CLT than given in Theorem 5.4:
\(\sqrt n (\bar X - \mu) \overset d \to Y\), where
\(Y \sim N(0,\sigma^2)\).

Solution:

CLT: \(\sqrt n (\bar X - \mu ) \overset d \to Y, Y \sim N(0,\sigma^2)\).

Cramer-Wold Device: \(\mathbf Y_n \overset d \to \mathbf Y\) iff
\(\mathbf c^T \mathbf Y_n \overset d \to \mathbf c ^T \mathbf Y ,~ \forall \mathbf c \in \mathbb R^K\).

Proof: Let \(\mathbf{X_1,\dots, X_n}\) be iid random k-vectors with
finite mean \(E(\mathbf X_1)= \boldsymbol \mu\) and covariance matrix
\(\boldsymbol \Sigma\).

Consider a vector \(\mathbf c \in \mathbb R ^k\). Then
\(\mathbf c^T \mathbf X_n\) is some scalar and
\[E(\mathbf c^T \mathbf X_n) = \mathbf c^T E(\mathbf X_n) = \mathbf c^T \boldsymbol  \mu\]
\[Var(\mathbf c^T \mathbf X_n) = \mathbf c^T Var(\mathbf X_n) \mathbf c = \mathbf c^T \boldsymbol \Sigma \mathbf c\]
Thus, by the CLT (scalar case):

\[
\begin{aligned}
\sqrt n \left(\frac {\sum_{i=1}^n \mathbf c^T \mathbf X_i}{n} - \mathbf c^T \boldsymbol  \mu \right) &\overset d \to W,~~~~W \sim N(0,\mathbf c^T \boldsymbol \Sigma \mathbf c) \\
\implies \mathbf c^T \sqrt n \left(\frac {\sum_{i=1}^n\mathbf X_i}{n} - \boldsymbol  \mu \right)& \overset d \to \mathbf c^T Y,~~~~Y \sim N(0, \boldsymbol \Sigma) &\text{by properties of normal distribution}\\
\implies \sqrt n \left(\mathbf {\bar X} - \boldsymbol  \mu \right)& \overset d \to \mathbf Y,~~~~Y \sim MVN(\mathbf 0, \boldsymbol \Sigma) & \text{by Cramer-Wold Device}
\end{aligned}
\] \newpage

\hypertarget{section-40}{%
\section{5.52}\label{section-40}}

Consider a Gauss-Markov linear model
\(\mathbf{Y= X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\) where
\(\mathbf Y\) is \(n \times 1\), the components of
\(\mathbf e = (e_1,\dots, e_n)^T\) are \(iid(0,\sigma^2)\), and
\(\mathbf X\) is \(n \times p_n\). Note that the number of predictors,
\(p_n\) depends on \(n\). Let \(\mathbf{H=X(X^TX)^{-1}X^T}\) denote the
projection (or ``hat'') matrix with entries \(h_{i,j}\). Note that
\(h_{i,j}\) also depend on \(n\). We are interested in the asymptotic
properties of the \(i^{th}\) residual, \(Y_i- \hat{Y}_i\), from this
regression model, for a fixed \(i\). Prove that if \(n\) and
\(p_n \to \infty\) such that \(h_{i,i} \to c_i\) for some
\(0\leq c_i <1\), and
\(\max_{\overset{1\leq j \leq n}{j\neq i}} |h_{i,j}| \to 0\), then
\(Y_i- \hat{Y}_i \overset d \to (1-c_i)e_i+\{(1-c_i)c_i\}^{1/2}\sigma Z\),
where \(Z\) is a standard normal random variable independent of \(e_i\).
There is a hint in the textbook.

Solution:

\[\begin{aligned}
Y_i - \hat{Y}_i &=  Y_i - HY_i \\
&= \mathbf{(I-H)Y}_i\\
&= \mathbf{(I-H)(X \boldsymbol \beta + e)}_i \\
&=\mathbf{[(IX-HX) \boldsymbol \beta + (I-H)e]}_i\\
&=\mathbf{[(X-X(X^TX)^{-1}X^TX) \boldsymbol \beta + (I-H)e]}_i \\
&=\mathbf{[(I-H)e]}_i \\
&= \left[\mathbf e - \begin{bmatrix} h_{1,1} & \dots & h_{1,n} \\
\vdots &\ddots &\vdots \\
h_{n1} & \dots &h_{n,n}
\end{bmatrix} \begin{pmatrix} e_1 \\ \vdots \\e_n \end{pmatrix} \right]_i\\
&= e_i - \sum_{j=1}^n h_{i,j}e_i \\
&= (1-h_{i,i})e_i - \sum_{j=1, i \neq j}^n h_{i,j}e_i
\end{aligned}\]

From Slutsky's \((1-h_{i,i})e_i \overset d \to (1-c_i) e_i\).

So I need to figure out what \(\sum_{j=1, i \neq j}^n h_{i,j}e_i\)
converges to. This is a double-array, so I will need to use the Lindberg
Condition, which requires the variance to be finite and the mean to be
zero. The mean is zero by Slutsky's Theorem.

So, to show the variance is finite I can compute the variance.

First, I want to show \(\mathbf H\) is idempotent:

\[\begin{aligned}
\mathbf H^2 &= \mathbf{[X(X^TX)^{-1}X^T][X(X^TX)^{-1}X^T]} \\
&=\mathbf{X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T} \\
&=\mathbf{X(X^TX)^{-1}X^T} \\
&= \mathbf H
\end{aligned}\]

Thus,

\[
\begin{aligned}
\mathbf H^2  &= \begin{bmatrix} h_{1,1} & \dots & h_{1,n} \\
\vdots &\ddots &\vdots \\
h_{n1} & \dots &h_{n,n}
\end{bmatrix} \begin{bmatrix} h_{1,1} & \dots & h_{1,n} \\
\vdots &\ddots &\vdots \\
h_{n1} & \dots &h_{n,n}
\end{bmatrix}\\
&=\begin{bmatrix} \sum_{i=1}^n h_{1,i}^2 & \sum_{i=1}^n h_{1,i}h_{2,i}  &\dots & \sum_{i=1}^n h_{1,i}h_{n,i} \\
\vdots &\ddots &&\vdots \\
\sum_{i=1}^n h_{n,i}h_{1,i}  && \dots &\sum_{i=1}^n h_{n,i}^2 
\end{bmatrix} \\
&= \mathbf H =  \begin{bmatrix} h_{1,1} & \dots & h_{1,n} \\
\vdots &\ddots &\vdots \\
h_{n1} & \dots &h_{n,n}
\end{bmatrix}
\end{aligned}
\]

From the main diagonal, it is clear that
\(\sum_{i=1}^n{h_{j,i}^2}=h_{j,j}\).

Thus,
\(\sum_{i=1}^n{h_{j,i}^2} = h_{i,i}^2+\sum_{i=1,i\neq j}^n{h_{i,j}^2}=h_{i,i}\)
and \(\sum_{i=1,i\neq j}^n{h_{i,j}^2} = h_{i,i} -h_{i,i}^2\).

Therefore, because of independence, \[ \begin{aligned}
Var \left( \sum_{i=1,i\neq j}^n h_{i,j} e_j  \right) &= (h_{i,i} -h_{i,i}^2)Var(e_j) \\ 
&=(h_{i,i} -h_{i,i}^2)\sigma^2
\end{aligned}
\]

Now that I have shown the variance is finite, and it is clear that the
mean \(E(h_{i,j} e_j) =0\), I can move on to setting up the
Lindeberg-Feller Condition.

I will define
\(X_{j,i} = \frac{h_{i,j}e_i}{\sigma\sqrt{h_{i,i}-h_{i,i}^2}}\),
(denominator is just the sqrt of variance found earlier)

\[
\begin{aligned}
\lim_{n \to \infty} \sum_{j=1, i \neq j}^n  E\left[X_{j,i}^2 I(|X_{j,i}|> \delta)\right] 
&=\lim_{n \to \infty} \sum_{j=1, i \neq j}^n  E\left[\left(\frac{h_{i,j}e_i}{\sigma\sqrt{h_{i,i}-h_{i,i}^2}}\right)^2 I\left(\left|\frac{h_{i,j}e_i}{\sigma\sqrt{h_{i,i}-h_{i,i}^2}}\right| > \delta\right)\right] \\
&=\frac{1}{\sigma^2(c_i-c_i^2)}\lim_{n \to \infty} \sum_{j=1, i \neq j}^n E\left[h_{i,j}^2e_i^2 I\left(\left|h_{i,j}e_i\right| > \delta \left|\sigma\sqrt{h_{i,i}-h_{i,i}^2} \right| \right)\right] \\
&=\frac{1}{\sigma^2(c_i-c_i^2)}\lim_{n \to \infty} \sum_{j=1, i \neq j}^n h_{i,j}^2E\left[e_i^2 I\left(\left|e_i\right| > \frac{\delta \left|\sigma\sqrt{h_{i,i}-h_{i,i}^2} \right|}{|h_{i,j}|}\right)\right] \\
\end{aligned}
\]

Noting that if
\(\max_{\overset{1\leq j \leq n}{j\neq i}} |h_{i,j}| \to 0\), then all
\(|h_{i,j}| \to 0\). Therefore, we get:

\[\begin{aligned}
&\frac{1}{\sigma^2(c_i-c_i^2)}\lim_{n \to \infty} \sum_{j=1, i \neq j}^n h_{i,j}^2E\left[e_i^2 I\left(\left|e_i\right| > \frac{\delta \left|\sigma\sqrt{c_i-c_i^2} \right|}{|h_{i,j}|}\right)\right]\\
&=\frac{1}{\sigma^2(c_i-c_i^2)}\lim_{n \to \infty} \sum_{j=1, i \neq j}^n h_{i,j}^2E\left[e_i^2 I\left(\left|e_i\right| > \infty \right)\right] \\
&=\frac{1}{\sigma^2(c_i-c_i^2)}\lim_{n \to \infty} \sum_{j=1, i \neq j}^n h_{i,j}^2 \cdot 0 \\
&= 0
\end{aligned}\]

Therefore, by Lindberg-Feller,
\(\lim_{n \to \infty} \sum_{j=1, i \neq j}^n \frac{h_{i,j}e_i}{\sigma\sqrt{h_{i,i}-h_{i,i}^2}} \overset d \to N(0,1)\),

And so,
\(\lim_{n \to \infty} \sum_{j=1, i \neq j}^n h_{i,j}e_i \overset d \to N(0,(c_i -c_i^2)\sigma^2)\)

Thus,
\(Y_i- \hat{Y}_i \overset d \to (1-c_i)e_i+\{(1-c_i)c_i\}^{1/2}\sigma Z\).

\bookmarksetup{startatroot}

\hypertarget{chapter-6-large-sample-results-for-likelihood-based-methods}{%
\chapter{Chapter 6: Large Sample Results for Likelihood-Based
Methods}\label{chapter-6-large-sample-results-for-likelihood-based-methods}}

\hypertarget{section-41}{%
\section{6.2}\label{section-41}}

Consider the density
\[f(y;\sigma) = \frac{2y}{\sigma^2}\exp\left(-\frac {y^2}{\sigma^2}\right) I(y>0,\sigma>0)\]
Verify that the regularity conditions 1. to 5. of Theorem 6.6 (p.~284)
hold for the asymptotic normality of the maximum likelihood estimator of
\(\sigma\).

Solution:

\textbf{Condition 1: Identifiability
\(\theta_1 \neq \theta_2 \implies \exists y ~s.t.~ F(y;\theta_1) \neq F(y;\theta_2)\).}

\[\begin{aligned}
F(y;\sigma) &= \int_{-\infty}^y\frac{2t}{\sigma^2}\exp\left(-\frac {t^2}{\sigma^2}\right) I(t>0,\sigma>0)dt & \substack{let~ u = \frac {t^2}{\sigma^2} \\du = \frac {2t}{\sigma^2}dt}\\
&= \int_{0}^{y^2/\sigma^2} e^{-u} \,I(u,\sigma>0) du \\
&= [-e^{y^2/\sigma^2}+e^0]  \,I(y,\sigma>0) \\
&= [1-e^{y^2/\sigma^2}]I(y,\sigma>0) \\
\end{aligned}\]

Thus, if \(\sigma_1 \neq \sigma_2\), then

\[\begin{aligned}
F(y; \sigma_1) &= [1-e^{-y^2/\sigma_1^2}]I(y,\sigma_1>0) \\
F(y; \sigma_2) &= [1-e^{-y^2/\sigma_2^2}]I(y,\sigma_2>0) \\
F(y; \sigma_1) -F(y; \sigma_2)&=  e^{y^2/\sigma_2^2}I(y,\sigma_2>0)-e^{y^2/\sigma_1^2}I(y,\sigma_1>0) \\
&\neq 0  \\
\implies F(y; \sigma_1) &\neq F(y; \sigma_2)
\end{aligned}
\]

\textbf{Condition 2: \(\forall \theta \in \Theta, F(y;\theta)\) has the
same support not depending on \(\theta\).}

This is true as the support is all
\((y, \sigma) \in \mathbb R^+ \times \mathbb R^+\), which is independent
of \(\sigma\).

\textbf{Condition 3: \(\forall \theta \in \Theta, F(y;\theta)\) the
first three partial derivatives of \(\log f(y;\theta)\) with respect to
\(\theta\) exist for y is the support of \(F(y;\theta)\)}

\[\begin{aligned}
f(y;\sigma) &= \frac{2y}{\sigma^2}\exp\left(-\frac {y^2}{\sigma^2}\right) \\
\log f(y;\sigma) &= \log (2y)- 2\log(\sigma)  -y^2\sigma^{-2}  \\
\frac{\partial \log f(y;\sigma)}{\partial \sigma} &= - 2 \sigma^{-1} +2y^2\sigma^{-3} \\
\frac{\partial^2 \log f(y;\sigma)}{\partial \sigma^2} &= 2 \sigma^{-2} -6y^2\sigma^{-4} \\
\frac{\partial^3 \log f(y;\sigma)}{\partial \sigma^3} &= -4 \sigma^{-3} +24y^2\sigma^{-5} \\
\end{aligned}
\]

The first three partial derivatives are defined for all strictly
positive values of \(y\) and \(\sigma\), which is the support of
\(F(y;\sigma)\).

\textbf{Condition 4: \(\forall \theta_0 \in \Theta\), there exist a
function \(g(y)\) such that \(\forall \theta\) in a neighborhood of
\(\theta\),
\(|\partial^3 \log f(y;\theta)/\partial \theta^3| \leq g(y)\) for all
\(y\) where \(\int g(y)dF(y;\theta_0) < \infty\).}

Let
\(g(y)= |\partial^3 \log f(y;\sigma)/\partial \sigma^3| = |-4 \sigma^{-3} +24y^2\sigma^{-5}|\).

Then,

\[\begin{aligned}
|-4 \sigma_0^{-3} +24y^2\sigma_0^{-5}| &\leq g(y) & \text{by def } g(y) \\ 
\int g(y)dF(y;\theta_0) &= \int |-4 \sigma_0^{-3} +24y^2\sigma_0^{-5}|dF(y; \sigma_0) \\
&\leq \int [|4 \sigma_0^{-3}| +|24y^2\sigma_0^{-5}|]f(y;\sigma_0)dy & \text{by Triangle Ineq.} \\
&\leq \int [4 \sigma_0^{-3} +24y^2\sigma_0^{-5}]f(y;\sigma_0)dy & \text{by Triangle Ineq.} \\
&\leq E(4 \sigma_0^{-3}) + E(24\sigma_0^{-5}Y^2) \\
&\leq 4 \sigma_0^{-3} + 24\sigma_0^{-5}E(Y^2)\\
&\leq 4 \sigma_0^{-3} + 24\sigma_0^{-5}\sigma_0^2 \\
&\leq 28\sigma_0^{-3} \\
&< \infty
\end{aligned}\]

\textbf{Condition 5:
\(\forall \theta \in \Theta, E[\partial \log f(Y_1;\theta)/\partial \theta] = 0, I(\theta)=E([\partial \log f(Y_1;\theta)/\partial \theta]^2) = E[-\partial^2 \log f(Y_1;\theta)/\partial \theta^2]\)}

\[\begin{aligned}
E[\partial \log f(Y;\sigma)/\partial \sigma] &= E(- 2 \sigma^{-1} +2Y^2\sigma^{-3})\\
&= - 2 \sigma^{-1} +2E(Y^2)\sigma^{-3} &\substack{X= Y^2 \implies E(Y^2)= E(X) = \sigma^2\\ X \sim Exponential(\sigma^2)}\\
&= - 2 \sigma^{-1} +2\sigma^{-1}\\
&= 0
\end{aligned}\]

\[\begin{aligned}
I(\sigma) &= E([\partial \log f(Y;\theta)/\partial \theta]^2) \\
&= E([- 2 \sigma^{-1} +2Y^2\sigma^{-3}]^2)\\
&= E(4\sigma^{-2} -8Y^2\sigma^{-4} +4Y^4\sigma^{-6}) \\
&= -4\sigma^{-2} +4E(Y^4)\sigma^{-6} & \substack{X= Y^2 \implies E(Y^4)= E(X^2) = Var(X) + [E(X)]^2\\ X \sim Exponential(\sigma^2) \implies E(X^4) = (\sigma^2)^2+(\sigma^2)^2=2\sigma^4}\\
&= -4\sigma^{-2} +4(2\sigma^{4})\sigma^{-6} \\
&= -4\sigma^{-2} +8\sigma^{-2}\\
&= 4\sigma^{-2} \\
&= 6\sigma^{-2}-2\sigma^{-2} \\
&= E[6\sigma^2\sigma^{-4}-2\sigma^{-2}]\\
&= E[-(2\sigma^{-2}-6Y^2\sigma^{-4})]\\
&= E[-\partial^2 \log f(Y_1;\theta)/\partial \theta^2]
\end{aligned}\]

\newpage

\hypertarget{section-42}{%
\section{6.3}\label{section-42}}

In Condition 5 of Theorem 6.6 (p.~284) we have the assumption that
\(E[\partial \log f(Y_1;\theta)/\partial \theta] = 0\). For continuous
distributions this mean zero assumption follows if
\[\int\left[\frac \partial {\partial \theta} f(y;\theta)\right] dy=\frac \partial {\partial \theta}\int\left[ f(y;\theta)dy\right] \]
because this latter integral is one by the definition of a density
function. The typical proof that this interchange of differentiability
and integration is allowed assumes that for each
\(\theta_0 \in \Theta\), there is a bounding function \(g_1(y)\)
(possibly depending on \(\theta_0\)) and a neighborhood of \(\theta_0\)
such that for all \(y\) and for all \(\theta\) in the neighborhood of
\(|\partial f(y;\theta)/\partial \theta| \leq g_1(y)\) and
\(\int g_1(y)dy < \infty\). Use the dominated convergence theorem to
show that this condition allows the above interchange.

Solution:

I begin by assuming what was stated in ``the typical proof).'' That for
each \(\theta_0 \in \Theta\), there is a bounding function \(g_1(y)\)
(possibly depending on \(\theta_0\)) and a neighborhood of \(\theta_0\)
such that for all \(y\) and for all \(\theta\) in the neighborhood of
\(|\partial f(y;\theta)/\partial \theta| \leq g_1(y)\).

Note that: \[\begin{aligned}
\frac{f(y;\theta+h_n) - f(y;\theta)}{h_n}&= \frac{\partial f(y;\theta)}{\partial \theta}\bigg|_{\theta_* \in (\theta, \theta + h_n)} &\text{ by mean value theorem}\\
&\leq g_1(y) & since~|\partial f(y;\theta)/\partial \theta| &\leq g_1(y)
\end{aligned}\]

Thus,
\(\frac{f(y;\theta+h_n) - f(y;\theta)}{h_n} \to \frac{f(y;\theta+h) - f(y;\theta)}{h}\)
satisfies conditions for the dominated convergence theorem.

Let \(h_n\) be a sequence converging to 0:

\[\begin{aligned}
\frac \partial {\partial \theta}\int\left[ f(y;\theta)dy\right]
&= \lim_{h_n \to 0} \frac{\int f(y;\theta+h_n)dy - \int f(y;\theta)dy}{h_n} \\
&= \lim_{h_n \to 0} \int \frac{f(y;\theta+h_n) - f(y;\theta)}{h_n} dy \\
&= \lim_{n \to \infty} \int \frac{f(y;\theta+h_n) - f(y;\theta)}{h_n} dy \\
&=\int\lim_{n \to \infty}  \frac{f(y;\theta+h_n) - f(y;\theta)}{h_n} dy &\text{dominated convergence theorem} \\ 
&=\int\lim_{h_n \to 0}  \frac{f(y;\theta+h_n) - f(y;\theta)}{h_n} dy \\
&=\int\left[\frac \partial {\partial \theta} f(y;\theta)\right] dy
\end{aligned}\]

\newpage

\hypertarget{section-43}{%
\section{6.6}\label{section-43}}

The proof of the asymptotic normality of the maximum likelihood
estimator does not use an approximation by averages. Show, however, that
one can extend the proof to obtain an approximation by averages result
for the maximum likelihood estimator. Hint: add and subtract the
numerator of (6.10, p.~285) divided by the probability limit of the
denominator of (6.10, p.~285).

Solution:

From (6.10, p.285), where \(\hat \theta\) is the MLE and \(\theta_0\) is
the true value,

\[\begin{aligned}
\sqrt n (\hat \theta -\theta) &= \frac{-S(\theta)/\sqrt n}{\frac 1 n S'(\theta) + \frac 1 {2n} S''(\hat \theta^*)( \hat \theta - \theta)} \\
&= \frac{-S(\theta)/\sqrt n}{\frac 1 n S'(\theta) + \frac 1 {2n} S''(\hat \theta^*)( \hat \theta - \theta)} +\frac{S(\theta)/\sqrt n-S(\theta)/\sqrt n}{-I(\theta)} \\
\implies \hat \theta - \theta&= \frac{-S(\theta)/ n}{\frac 1 n S'(\theta) + \frac 1 {2n} S''(\hat \theta^*)( \hat \theta - \theta)} +\frac{S(\theta)/n-S(\theta)/ n}{-I(\theta)} \\
&= \frac{-S(\theta)/ n}{-I(\theta)} + R_n ~~~~ where~ R_n =\frac{-S(\theta)/ n}{\frac 1 n S'(\theta) + \frac 1 {2n} S''(\hat \theta^*)( \hat \theta - \theta)}+\frac{S(\theta)/n}{-I(\theta)}\\
&= \frac 1 n \sum_{i=1}^n \frac{-\partial \log f(Y_i; \theta)/\partial \theta}{-I(\theta)} + R_n \\
&= \frac 1 n \sum_{i=1}^n \frac{\partial \log f(Y_i; \theta)/\partial \theta}{I(\theta)} + R_n
\end{aligned}\]

Note \(\sqrt n R_n \to 0\) since the denominators probability limits go
to the negative information (see textbook).

Also note
\(E\left[\frac{\partial \log f(Y_i; \theta)/\partial \theta}{I(\theta)}\right] = 0\)
since the expected value of a score is zero.

Lastly, \[\begin{aligned}
Var\left[\frac{\partial \log f(Y_i; \theta)/\partial \theta}{I(\theta)}\right]
&=\frac{E[(\partial \log f(Y_i; \theta)/\partial \theta)^2]}{I^2(\theta)}\\
&= \frac{I(\theta)}{I^2(\theta)} \\
&= I^{-1}(\theta)
\end{aligned}
\]

Thus, the approximation by averages of \(\hat \theta_{MLE}\) is:
\[\hat \theta -\theta =\frac 1 n \sum_{i=1}^n \frac{\partial \log f(Y_i; \theta)/\partial \theta}{I(\theta)} + R_n\]
And
\(\sqrt n (\hat \theta -\theta) \overset d \to N(0, I^{-1}(\theta))\) by
Theorem 5.23 (p.242).

\bookmarksetup{startatroot}

\hypertarget{chapter-7-m-estimation-estimating-equations}{%
\chapter{Chapter 7: M-Estimation (Estimating
Equations)}\label{chapter-7-m-estimation-estimating-equations}}

\hypertarget{section-44}{%
\section{7.3}\label{section-44}}

Suppose that \(Y_1, \dots , Y_n\) are iid from a
gamma\((\alpha, \beta)\) distribution.

\hypertarget{a.-7}{%
\subsection{a.}\label{a.-7}}

One version of the method of moments is to set \(\bar Y\) equal to
\(E(Y_1) = \alpha \beta\) and \(n^{-1} \sum_{i=1}^n Y_i^2\) equal to
\(E(Y_1^2) = \alpha \beta^2 + (\alpha \beta)^2\) and solve for the
estimators. Use Maple (at least it's much easier if you do) to find
\(V= A^{-1} B \{A^{-1}\}^T\). Here it helps to know that
\(E(Y_1^3) = \alpha(1+\alpha )(2+\alpha ) \beta^3\) and
\(E(Y_1^4) = \alpha(1+\alpha)(2+\alpha)(3+\alpha)\beta^4\). Show your
derivation of A and B and attach Maple output.

Solution:

\[\begin{aligned}
\theta &= (\alpha, \beta)^T ~~~~~
\psi(Y_i, \theta) &=  \begin{pmatrix} Y_i - \alpha \beta \\ Y_i^2 - \alpha \beta^2 - (\alpha \beta)^2 \end{pmatrix} ~~~~~~~~~
\frac{\partial \psi(Y_i, \theta)}{\partial \theta} &=  \begin{bmatrix} -  \beta & - \alpha \\
- \beta^2 - 2\alpha \beta^2 & - 2\alpha \beta - 2\alpha^2 \beta \end{bmatrix}
\end{aligned}
\] \[ \begin{aligned}
A &= E[- \psi'(Y_i, \theta)] =  \begin{bmatrix}   \beta &  \alpha \\
 \beta^2 + 2\alpha \beta^2 &  2\alpha \beta + 2\alpha^2 \beta \end{bmatrix} \\
B &= E[\psi (Y_i, \theta)\psi (Y_i, \theta)^T] =  E\left[\begin{pmatrix} Y_i - \alpha \beta \\ Y_i^2 - \alpha \beta^2 - (\alpha \beta)^2 \end{pmatrix} \begin{pmatrix} Y_i - \alpha \beta & Y_i^2 - \alpha \beta^2 - (\alpha \beta)^2 \end{pmatrix} \right]\\
&= E\begin{bmatrix} (Y_i -\alpha \beta)^2 & (Y_i -\alpha \beta)(Y_i^2 -\alpha \beta^2 -\alpha^2\beta^2) \\
(Y_i -\alpha \beta)(Y_i^2 -\alpha \beta^2 -\alpha^2\beta^2) & (Y_i^2 -\alpha \beta^2 -\alpha^2\beta^2)^2\end{bmatrix} \\
&= E\begin{bmatrix} Y_i^2 - 2 \alpha \beta Y_i +\alpha^2 \beta^2 &
Y_i^3-\alpha \beta Y_i^2 - \alpha^2 \beta^2 Y_i -\alpha \beta^2 Y_i + \alpha^3 \beta^3 + \alpha^2\beta^3\\
Y_i^3-\alpha \beta Y_i^2 - \alpha^2 \beta^2 Y_i -\alpha \beta^2 Y_i + \alpha^3 \beta^3 + \alpha^2\beta^3 &
Y_i^4 -2 \alpha^2 \beta^2 Y_i^2 - 2 \alpha \beta^2 Y_i^2 + \alpha^4 \beta^4 +\alpha^2 \beta^4 + 2 \alpha^3 \beta^4\end{bmatrix} \\
&= \begin{bmatrix} (\alpha \beta^2 + \alpha^2 \beta^2) - 2 \alpha \beta \alpha \beta  +\alpha^2 \beta^2 &
\alpha(1+\alpha)(2+\alpha)\beta^3-\alpha \beta (\alpha \beta^2 + \alpha^2 \beta^2) - \alpha^2 \beta^2 \alpha \beta  -\alpha \beta^2 \alpha \beta  + \alpha^3 \beta^3 + \alpha^2\beta^3\\
\dots Symmetric \dots &
E(Y_i^4) -2 \alpha^2 \beta^2 (\alpha \beta^2 + \alpha^2 \beta^2) - 2 \alpha \beta^2 (\alpha \beta^2 + \alpha^2 \beta^2) + \alpha^4 \beta^4 +\alpha^2 \beta^4 + 2 \alpha^3 \beta^4\end{bmatrix} \\
&= \begin{bmatrix} \alpha \beta^2 &
\alpha(1+\alpha)(2+\alpha)\beta^3-\alpha^2 \beta^3 - \alpha^3 \beta^3 \\
\alpha(1+\alpha)(2+\alpha)\beta^3-\alpha^2 \beta^3 - \alpha^3 \beta^3 &
\alpha(1+\alpha)(2+\alpha)(3+\alpha)\beta^4  -2 \alpha^3 \beta^4 - \alpha^4 \beta^4 -  \alpha^2 \beta^4 \end{bmatrix} \\
&= \begin{bmatrix} \alpha \beta^2 &
(\alpha^3+3\alpha^2+2\alpha)\beta^3-\alpha^2 \beta^3 - \alpha^3 \beta^3 \\
(\alpha^3+3\alpha^2+2\alpha)\beta^3-\alpha^2 \beta^3 - \alpha^3 \beta^3 &
(\alpha^4 + 6 \alpha^3+ 11 \alpha^2 + 6\alpha)\beta^4 -2 \alpha^3 \beta^4 - \alpha^4 \beta^4 -  \alpha^2 \beta^4 \end{bmatrix} \\
&= \begin{bmatrix} \alpha \beta^2 &
2(\alpha^2+\alpha)\beta^3 \\
2(\alpha^2+\alpha)\beta^3 &
(4 \alpha^3+ 10 \alpha^2 + 6\alpha)\beta^4  \end{bmatrix} \\
V &= A^{-1} B \{ A^{-1} \}^T ~~~~ Using~Microsoft~Mathematics \\
&= \begin{bmatrix} 2(\alpha^2 +\alpha) & -2(1+ \alpha) \beta \\
-2(1+ \alpha) \beta & \left(2 + \frac 3 \alpha \right)\beta^2 \end{bmatrix}
\end{aligned}\]

\newpage

\hypertarget{b.-5}{%
\subsection{b.}\label{b.-5}}

The second version of the method of moments (and perhaps the easier
method) is to set \(\bar Y\) equal to \(\alpha \beta\) and \(s^2\) equal
to \(var(Y_1) = \alpha \beta^2\) and solve for the estimators. You could
use either the ``\(n-1\)'' or ``\(n\)'' version of \(s^2\), but here we
want to use the ``\(n\)'' version in order to fit into the M-estimator
theory. Compute \(V\) as in a) except that the second component of the
function is different from a) (but V should be same). Here it helps to
know that \(\mu_3= 2\alpha\beta^3\) and
\(\mu_4 = 3[\alpha^2+2\alpha]\beta^4\).

Solution:

\[\begin{aligned}
\psi(Y_i, \theta) &=  \begin{pmatrix} Y_i - \alpha \beta \\ (Y_i - \alpha \beta)^2 - \alpha \beta^2\end{pmatrix}
~~~~~\frac{\partial \psi(Y_i, \theta)}{\partial \theta} =
\begin{bmatrix}  -  \beta  & - \alpha\\
-2 \beta (Y_i - \alpha \beta) - \beta^2 & -2\alpha(Y_i - \alpha \beta) -2\alpha \beta\end{bmatrix}
\end{aligned}\]

\[ \begin{aligned}
A &= E[- \psi'(Y_i, \theta)] = E \begin{bmatrix}    \beta  &  \alpha\\
2 \beta (Y_i - \alpha \beta) + \beta^2 & 2\alpha(Y_i - \alpha \beta) +2\alpha \beta\end{bmatrix} \\
&=\begin{bmatrix}    \beta  &  \alpha\\
\beta^2 &  2\alpha \beta\end{bmatrix} \\
B &= E[\psi (Y_i, \theta)\psi (Y_i, \theta)^T] =  E\left[\begin{pmatrix} Y_i - \alpha \beta \\ (Y_i - \alpha \beta)^2 - \alpha \beta^2 \end{pmatrix} \begin{pmatrix} Y_i - \alpha \beta & (Y_i - \alpha \beta)^2 - \alpha \beta^2 \end{pmatrix} \right]\\
&=  E \begin{bmatrix} (Y_i - \alpha \beta)^2 & (Y_i - \alpha \beta)[(Y_i-\alpha \beta)^2 - \alpha \beta^2] \\
(Y_i - \alpha \beta)[(Y_i-\alpha \beta)^2 - \alpha \beta^2] & [(Y_i-\alpha \beta)^2 - \alpha \beta^2]^2\end{bmatrix}\\
&=   \begin{bmatrix} Var(Y_i) & \mu_3 \\
\mu_3 & \mu_4 -2 \alpha \beta^2 Var(Y_i) + \alpha^2\beta^4\end{bmatrix}
=   \begin{bmatrix} \alpha \beta^2 & 2 \alpha \beta^3 \\
2 \alpha \beta^3  & 3(\alpha^2+2\alpha)\beta^4-2 \alpha \beta^2(\alpha \beta^2) + \alpha^2\beta^4\end{bmatrix}\\
&=   \begin{bmatrix} \alpha \beta^2 & 2 \alpha \beta^3 \\
2 \alpha \beta^3  & (2\alpha^2+6\alpha)\beta^4 \end{bmatrix}\\
V &= A^{-1} B \{ A^{-1} \}^T ~~~~ Using~Microsoft~Mathematics \\
&= \begin{bmatrix} 2(\alpha^2 +\alpha) & -2(1+ \alpha) \beta \\
-2(1+ \alpha) \beta & \left(2 + \frac 3 \alpha \right)\beta^2 \end{bmatrix}
\end{aligned}\]

\hypertarget{c.-3}{%
\subsection{c.}\label{c.-3}}

The asymptotic variance of the MLEs for \(\alpha\) and \(\beta\) are
\(Avar(\hat \alpha_{MLE})= 1.55/n\) for \(\alpha = 1.0\) and
\(Avar(\hat \alpha_{MLE})= 6.90/n\) for \(\alpha = 2.0\). Similarly,
\(Avar(\hat \beta_{MLE})= 2.55\beta^2/n\) for \(\alpha = 1.0\) and
\(Avar(\hat \beta_{MLE})= 3.45\beta^2/n\) for \(\alpha = 2.0\). Now
calculate the asymptotic relative efficiencies of the MLEs to the method
of moment estimators for \(\alpha = 1.0\) and \(\alpha = 2.0\) using
results from a.

Solution:

\[\begin{aligned}
Avar(\hat \alpha_{MOM}) |_{\alpha = 1.0} &= 2(\alpha^2 +\alpha)/n \big|_{\alpha = 1.0} = 4/n \\
Avar(\hat \alpha_{MOM}) |_{\alpha = 2.0} &= 2(\alpha^2 +\alpha)/n \big|_{\alpha = 2.0} = 12/n \\
Avar(\hat \beta_{MOM}) |_{\alpha = 1.0} &=\left(2 + \frac 3 \alpha \right)\beta^2 \big|_{\alpha = 1.0} = 5 \beta^2 /n \\
Avar(\hat \beta_{MOM}) |_{\alpha = 2.0} &= \left(2 + \frac 3 \alpha \right)\beta^2 \big|_{\alpha = 2.0} = 7 \beta^2/(2n) \\ \\
ARE(\hat \alpha_{MLE},\hat \alpha_{MOM})|_{\alpha = 1.0} &= 1.55/4 = 0.3875 \\
ARE(\hat \alpha_{MLE},\hat \alpha_{MOM})|_{\alpha = 2.0} &= 6.9/12 = 0.575 \\
ARE(\hat \beta_{MLE},\hat \beta_{MOM})|_{\alpha = 1.0} &= 2.55/(5) = 0.52 \\
ARE(\hat \beta_{MLE},\hat \beta_{MOM})|_{\alpha = 2.0} &= 3.45/(7/2) \approx 0.986 \\
\end{aligned}\]

As expected, the MLE is more efficient in all cases.

\newpage

\hypertarget{section-45}{%
\section{7.4}\label{section-45}}

Suppose that \(Y_1, \dots, Y_n\) are iid and
\(\boldsymbol{\hat \theta}\) satisfies
\(\sum_{i=1}^n \boldsymbol \psi (Y_i, \boldsymbol{\hat \theta}) = \mathbf c_n\)
where we assume:

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\item
  \(\boldsymbol{\hat \theta} \overset p \to \boldsymbol \theta_0\)
\item
  \(\mathbf c_n / \sqrt n \overset p \to 0\)
\item
  The remainder term \(R_n\) from the expansion
  \[G_n (\boldsymbol{\hat \theta}) = n^{-1} \sum_{i=1}^n \boldsymbol \psi (Y_i, \boldsymbol{\hat \theta}) = G_n(\boldsymbol \theta_0) +G_n'(\boldsymbol \theta_0)(\boldsymbol{\hat \theta} -\boldsymbol \theta_0) + \mathbf R_n\]
  satisfies \(\sqrt n \mathbf R_n \overset p \to 0\).
\end{enumerate}

Show that \(\boldsymbol{\hat \theta}\) is
\(AN(\boldsymbol \theta_0, V(\boldsymbol \theta_0)/n)\), i.e., the same
result as for the usual case when \(\mathbf c_n = 0\).

Solution:

Let
\(G_n (\boldsymbol{\hat \theta}) = n^{-1} \sum_{i=1}^n \boldsymbol \psi (Y_i, \boldsymbol{\hat \theta}) = n^{-1} \mathbf c_n\).

Expanding using Taylor's Theorem about \(\boldsymbol \theta_0\)
\[\begin{aligned}
 n^{-1} \mathbf c_n &= G_n (\boldsymbol{\hat \theta}) =G_n(\boldsymbol \theta_0) +G_n'(\boldsymbol \theta_0)(\boldsymbol{\hat \theta} -\boldsymbol \theta_0) + \mathbf R_n \\
 \implies \frac {\mathbf c_n} {\sqrt{n}} &= \sqrt n G_n(\boldsymbol \theta_0) + \sqrt n G_n'(\boldsymbol \theta_0)(\boldsymbol{\hat \theta} -\boldsymbol \theta_0) + \sqrt n \mathbf R_n
 \end{aligned}\] \[\begin{aligned}
\implies  \sqrt n G_n(\boldsymbol \theta_0) + \sqrt n G_n'(\boldsymbol \theta_0)(\boldsymbol{\hat \theta} -\boldsymbol \theta_0) &\overset p \to 0 & \text{by ii and iii}\\
\implies \sqrt n G_n'(\boldsymbol \theta_0)(\boldsymbol{\hat \theta} -\boldsymbol \theta_0) \overset p \to -\sqrt n G_n(\boldsymbol \theta_0) \\
\implies \sqrt n (\boldsymbol{\hat \theta} -\boldsymbol \theta_0) \overset p \to -G_n'^{-1}(\boldsymbol \theta_0)\sqrt n   G_n(\boldsymbol \theta_0)
\end{aligned}\]

Note that, by CLT \[\begin{aligned}
\sqrt n   [G_n(\boldsymbol \theta_0)- E[G_n(\boldsymbol \theta_0)]]  &\overset d \to N(0, E(G_n(\boldsymbol \theta_0)G_n(\boldsymbol \theta_0)^T)) \\
\implies \sqrt n   \left[G_n(\boldsymbol \theta_0)- E[n^{-1} \sum_{i=1}^n \boldsymbol \psi (Y_i, \boldsymbol \theta_0)]\right]  &\overset d \to N\left(0, E\left[n^{-2} \sum_{i=1}^n \boldsymbol \psi (Y_i, \boldsymbol \theta_0)\boldsymbol \sum_{i=1}^n \psi^T (Y_i, \boldsymbol \theta_0)\right] \right) \\
\implies \sqrt n   \left[G_n(\boldsymbol \theta_0)- n^{-1} \mathbf c_n\right]  &\overset d \to N\left(0, E\left[ \boldsymbol \psi (Y_1, \boldsymbol \theta_0)\boldsymbol \psi^T (Y_2, \boldsymbol \theta_0)\right] \right) \\
\implies \sqrt n   G_n(\boldsymbol \theta_0) &\overset d \to N\left(0, \mathbf B(\boldsymbol \theta_0) \right)
\end{aligned}\]

Also, by SLLN \[\begin{aligned}
G_n'(\boldsymbol \theta_0) &=  n^{-1} \sum_{i=1}^n \boldsymbol \psi' (Y_i, \boldsymbol{\hat \theta}) \overset p \to E(\psi' (Y_1, \boldsymbol \theta_0)) = - A(\boldsymbol \theta_0)
\end{aligned}\]

Thus, by Slutsky's Theorem,

\[\begin{aligned}
\implies \sqrt n (\boldsymbol{\hat \theta} -\boldsymbol \theta_0) &\overset p \to -G_n'^{-1}(\boldsymbol \theta_0)\sqrt n   G_n(\boldsymbol \theta_0) \\
&\overset d \to N\left(0, \mathbf A^{-1}(\boldsymbol \theta_0) \mathbf B(\boldsymbol \theta_0) \left\{\mathbf A^{-1}(\boldsymbol \theta_0) \right\}^T \right) \\
&\overset d \to N\left(0, \mathbf V(\boldsymbol \theta_0) \right) \\
\implies \boldsymbol{\hat \theta} ~is ~ & ~~ AN\left( \boldsymbol \theta_0, \frac{\mathbf V(\boldsymbol \theta_0)}{n} \right)
\end{aligned}\]

\newpage

\hypertarget{section-46}{%
\section{7.6}\label{section-46}}

(Delta Theorem via M-estimation). Suppose that
\(\boldsymbol{\hat \theta}\) is a b-dimensional M-estimator with
defining function \(\boldsymbol \psi(y, \boldsymbol \theta)\) such that
the usual quantities \(\mathbf A\) and \(\mathbf B\) exist. Here we want
to essentially reproduce Theorem 5.19 (p.~238) for
\(g(\boldsymbol{\hat \theta})\), where \(g\) satisfies the assumptions
of Theorem 5.19 and
\(b_n^2 \boldsymbol \Sigma = n^{-1} \mathbf V(\boldsymbol \theta)\),
where
\(\mathbf V(\boldsymbol \theta )= \mathbf A(\boldsymbol \theta)^{-1} \mathbf B(\boldsymbol \theta)\{\mathbf A(\boldsymbol \theta)^{-1}\}^T\)
So add the \(\boldsymbol \psi\) function
\(g(\boldsymbol \theta) - \theta_{b+1}\) to
\(\boldsymbol \psi(y, \boldsymbol \theta)\), compute the relevant
matrices, say \(\mathbf {A}^* , \mathbf{B}^*,\) and \(\mathbf{V}^*\),
and show that the last diagonal element of \(\mathbf{V}^*\) is
\(g'(\boldsymbol \theta) \mathbf V(\boldsymbol \theta)g'(\boldsymbol \theta)^T\).

Solution:

Define
\(\theta^* = E[g(\boldsymbol \theta)], \boldsymbol \beta = (\boldsymbol \theta, \theta^*)^T\)

\[\begin{aligned}
\psi^* (Y_i, \boldsymbol \beta) &= \begin{pmatrix} \psi(Y, \boldsymbol \theta) \\ g(\boldsymbol \theta) - \theta^* \end{pmatrix} \\
\mathbf A^* &= - E \begin{bmatrix} \frac{\partial \psi^* (Y_i, \boldsymbol \beta)}{\partial \boldsymbol \beta} \end{bmatrix} 
= - E \begin{bmatrix} \frac{\partial \psi(Y, \boldsymbol \theta)}{\partial \boldsymbol \theta}  &
\frac{\partial \psi(Y, \boldsymbol \theta)}{\partial \theta^*} \\
\frac{\partial}{\partial \boldsymbol \theta} \left\{ g(\boldsymbol \theta) - \theta^*\right\} & 
\frac{\partial}{\partial \theta^*} \left\{ g(\boldsymbol \theta) - \theta^*\right\} \end{bmatrix} \\
&= \begin{bmatrix} \mathbf A & 0 \\ -g'(\boldsymbol \theta) & 1\end{bmatrix} \\
\mathbf B^*&= E\begin{bmatrix} \psi^* {\psi^*}^T \end{bmatrix} 
= E\begin{bmatrix} \begin{pmatrix} \psi(Y, \boldsymbol \theta) \\ g(\boldsymbol \theta) - \theta^* \end{pmatrix}
\begin{pmatrix} \psi^T(Y, \boldsymbol \theta) & g(\boldsymbol \theta) - \theta^* \end{pmatrix} \end{bmatrix} \\
&=E\begin{bmatrix} \psi(Y, \boldsymbol \theta) \psi^T(Y, \boldsymbol \theta) & 
\psi(Y, \boldsymbol \theta)[g(\boldsymbol \theta) - \theta^*] \\
\psi(Y, \boldsymbol \theta)[g(\boldsymbol \theta) - \theta^*] &
[g(\boldsymbol \theta) - \theta^*]^2 \end{bmatrix} \\
&= \begin{bmatrix} \mathbf B & 0 \\ 0 & 0 \end{bmatrix} \\
\mathbf V^* &= {\mathbf A^*}^{-1}\mathbf B^* \left\{{\mathbf A^*}^{-1}\right\}^T \\
&= \begin{bmatrix} \mathbf A & 0 \\ -g'(\boldsymbol \theta) & 1\end{bmatrix}^{-1}
\begin{bmatrix} \mathbf B & 0 \\ 0 & 0 \end{bmatrix}
\left\{\begin{bmatrix} \mathbf A & 0 \\ -g'(\boldsymbol \theta) & 1\end{bmatrix}^{-1}\right\}^T \\
&= \mathbf A^{-1}\begin{bmatrix} 1 & 0 \\  g'(\boldsymbol \theta)& \mathbf A\end{bmatrix}
\begin{bmatrix} \mathbf B & 0 \\ 0 & 0 \end{bmatrix}
\left\{\mathbf A^{-1}\begin{bmatrix} 1 & 0\\  g'(\boldsymbol \theta)& \mathbf A\end{bmatrix}\right\}^T \\
&= \begin{bmatrix} \mathbf A^{-1} & 0\\  g'(\boldsymbol \theta)\mathbf A^{-1}& 1\end{bmatrix}
\begin{bmatrix} \mathbf B & 0 \\ 0 & 0 \end{bmatrix}
\left\{\begin{bmatrix} \mathbf A^{-1} & 0\\  g'(\boldsymbol \theta)\mathbf A^{-1}& 1\end{bmatrix}\right\}^T \\
&= \begin{bmatrix} \mathbf {A^{-1}BA^{-1}}^T & \mathbf {A^{-1}BA^{-1}}^T g'(\boldsymbol \theta)^T \\
g'(\boldsymbol \theta)\mathbf {A^{-1}BA^{-1}}^T  & g'(\boldsymbol \theta) \mathbf {A^{-1}BA^{-1}}^T g'(\boldsymbol \theta)^T\end{bmatrix} ~~~~\text{using Microsoft Mathematics for matrix multiplication}
\end{aligned}\]

The \(g'(\hat {\boldsymbol \theta})\) is
\(AN \left( \boldsymbol \theta_0, \frac{g'(\boldsymbol \theta)\mathbf V g'(\boldsymbol \theta)^T } n \right)\),
where \(\mathbf V = \mathbf {A^{-1}BA^{-1}}^T\)

\hypertarget{section-47}{%
\section{7.7}\label{section-47}}

The generalized method of moments (GMM) is an important estimation
method found mainly in the econometrics literature and closely related
to M-estimatiion. Suppose that we have iid random variables
\(Y_1, \dots, Y_n\) and a \(p\) dimensional unknown parameter
\(\boldsymbol \theta\). The key idea is that there are a set of
\(g \geq p\) possible estimating equations
\[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta) = 0, ~~~~~~~j=1, \dots, q\]
motivated by the fact that \(E_{\psi_j}(Y_1; \boldsymbol \theta_0)\)
where \(\boldsymbol \theta_0\) is the true value. These motivating zero
expectations come from the theory in the subject area being studied. But
notice that if \(q > p\), then we have too many equations. The GMM
approach is to minimize the objective function

\[T= \left[\frac 1 n \sum_{i=1}^n \boldsymbol \psi (Y_i; \boldsymbol \theta)\right]^T \mathbf W \left[\frac 1 n \sum_{i=1}^n \boldsymbol \psi (Y_i; \boldsymbol \theta)\right]\]

where \(\boldsymbol \psi = (\psi_1, \dots, \psi_q)^T\) and \(\mathbf W\)
is a matrix of weights. Now let's simplify the problem by letting
\(q=2, p=1\) so that \(\theta\) is real-valued, and
\(\mathbf W = \text{diag}(w_1,w_2)\). Then T reduces to

\[T= w_1\left[\frac 1 n \sum_{i=1}^n \psi_1 (Y_i; \boldsymbol \theta)\right]^2 +w_2\left[\frac 1 n \sum_{i=1}^n \psi_2 (Y_i; \boldsymbol \theta)\right]^2\]

To find \(\hat \theta\) we just take the partial derivative of T with
respect to \(\theta\) and set it equal to 0:

\[\begin{aligned}
S(\mathbf Y; \theta) &= 2w_1\left[\frac 1 n \sum_{i=1}^n \psi_1 (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i; \boldsymbol \theta)\right] +2w_2\left[\frac 1 n \sum_{i=1}^n \psi_2 (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i; \boldsymbol \theta)\right] \\
&= 0
\end{aligned}\]

\hypertarget{a.-8}{%
\subsection{a.}\label{a.-8}}

Prove that \(S(\mathbf Y ; \theta_0) \overset p \to 0\) making any
moment assumptions that you need. (This should suggest to you that the
solution of the equation \(S(\mathbf Y; \theta)=0\) is consistent.)

Solution:

\[\begin{aligned}
\frac 1 n \sum_{i=1}^n \psi_j(Y_i; \theta_0) &\overset p \to E\left[\frac 1 n \sum_{i=1}^n \psi_j(Y_i; \theta_0) \right] = 0 & \text{ by WLLN} \\
\frac 1 n \sum_{i=1}^n \psi_j'(Y_i; \theta_0) &\overset p \to E\left[\frac 1 n \sum_{i=1}^n \psi_j'(Y_i; \theta_0) \right] & \text{ by WLLN} \\
\text{Assuming } E\left[\frac 1 n \sum_{i=1}^n \psi_j'(Y_i; \theta_0) \right] &< \infty,\\
w_j \left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]&\left[\frac 1 n \sum_{i=1}^n \psi_j '(Y_i; \boldsymbol \theta)\right] \overset p \to 0 & \text{by Slutsky's Theorem}
\end{aligned}\] \[\begin{aligned}
\implies S(\mathbf Y; \theta) &= 2w_1\left[\frac 1 n \sum_{i=1}^n \psi_1 (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i; \boldsymbol \theta)\right] +2w_2\left[\frac 1 n \sum_{i=1}^n \psi_2 (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i; \boldsymbol \theta)\right] \\
&\overset p \to 0
\end{aligned}\]

\newpage

\hypertarget{b.-6}{%
\subsection{b.}\label{b.-6}}

To get asymptotic normality for \(\hat \theta\), a direct approach is to
expand \(S(\mathbf Y; \hat \theta)\) around \(\theta_0\) and solve for
\(\hat \theta - \theta_0\).
\[\hat \theta - \theta_0 = \left[-\frac{\partial S(\mathbf Y; \theta_0)}{\partial \theta^T} \right]^{-1}S(\mathbf Y; \theta_0)+ \left[-\frac{\partial S(\mathbf Y; \theta_0)}{\partial \theta^T} \right]^{-1}R_n\]
Then one ignores the remainder term and uses Slutsky's Theorem along
with asymptotic normality of \(S(\mathbf Y; \theta_0)\). But how to get
the asymptotic normality of \(S(\mathbf Y; \theta_0)\)? Find
\(h(Y_i; \theta_0)\) such that
\[S(\mathbf Y; \theta_0) = \frac 1 n \sum_{i=1}^n h(Y_i; \theta_0) + R_n^*\]
No proofs are required.

Solution:

\[\begin{aligned}
S(\mathbf Y; \theta) &= 2w_1\left[\frac 1 n \sum_{i=1}^n \psi_1 (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i; \boldsymbol \theta)\right] +2w_2\left[\frac 1 n \sum_{i=1}^n \psi_2 (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i; \boldsymbol \theta)\right] \\
&= \sum_{j=1}^2 2w_j\left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_j '(Y_i; \boldsymbol \theta)\right] \\
&= \sum_{j=1}^2 2w_j\left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_j '(Y_i; \boldsymbol \theta) + E\left(\psi_j '(Y_i; \boldsymbol \theta) \right)- E\left(\psi_j '(Y_i; \boldsymbol \theta) \right)\right] \\
&=\sum_{j=1}^2 2w_jE\left(\psi_j '(Y_i; \boldsymbol \theta) \right)\left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]+ 2w_j\left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_j '(Y_i; \boldsymbol \theta)- E\left(\psi_j '(Y_i; \boldsymbol \theta) \right)\right] \\
\end{aligned}\]

\[\begin{aligned}
\text{Let }R_n^* &= \sum_{j=1}^2 2w_j\left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_j '(Y_i; \boldsymbol \theta)- E\left(\psi_j '(Y_i; \boldsymbol \theta) \right)\right] \\
\implies \sqrt n R_n^* &= \sum_{j=1}^2 2w_j\left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]\sqrt n\left[\frac 1 n \sum_{i=1}^n \psi_j '(Y_i; \boldsymbol \theta)- E\left(\psi_j '(Y_i; \boldsymbol \theta) \right)\right] \\
&\overset p \to 0 ~~~ \text{by WLLN, Central Limit Theorem, and Slutsky's Theorem}
\end{aligned}\] \[\begin{aligned}
\implies S(\mathbf Y; \theta) &= \sum_{j=1}^2 2w_jE\left(\psi_j '(Y_i; \boldsymbol \theta) \right)\left[\frac 1 n \sum_{i=1}^n \psi_j (Y_i; \boldsymbol \theta)\right]+ R_n \\
&= \frac 1 n \sum_{i=1}^n \sum_{j=1}^2 2w_j\psi_j (Y_i; \boldsymbol \theta)E\left(\psi_j '(Y_i; \boldsymbol \theta) \right) + R_n \\
\implies h(Y_i; \theta_0) &=\sum_{j=1}^2 2w_j\psi_j (Y_i; \boldsymbol \theta)E\left(\psi_j '(Y_i; \boldsymbol \theta) \right) \\
&=2w_1\psi_1 (Y_i; \boldsymbol \theta)E\left(\psi_1 '(Y_i; \boldsymbol \theta) \right)+2w_2\psi_2 (Y_i; \boldsymbol \theta)E\left(\psi_2 '(Y_i; \boldsymbol \theta) \right)
\end{aligned}\]

\hypertarget{c.-4}{%
\subsection{c.~}\label{c.-4}}

The equation \(S(\mathbf Y; \theta) = 0\) is not in the form for using
M-estimation results (because the product of sums is not a simple sum).
Show how to get it in M-estimation form by adding two new parameters,
\(\theta_2\) and \(\theta_3\), and two new equations so that the result
is a system of three equations with three \(\psi\) functions; call them
\(\psi_1^*, \psi_2^*\), and \(\psi_3^*\) because \(\psi_1^*\) is
actually a function of the original \(\psi_1\) and \(\psi_2\).

Solution:

\[\begin{aligned}
S(\mathbf Y; \theta) &= 2w_1\left[\frac 1 n \sum_{i=1}^n \psi_1 (Y_i;  \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i;  \theta)\right] +2w_2\left[\frac 1 n \sum_{i=1}^n \psi_2 (Y_i;  \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i;  \theta)\right] \\
\psi_1^* &= 2w_1 \psi_1 (Y_i;  \theta) \theta_2 +2w_2\psi_2 (Y_i;  \theta)\theta_3\\
\psi_2^* &= \psi_1'(Y_i; \theta) - \theta_2 \\
\psi_3^* &= \psi_2'(Y_i; \theta) - \theta_3
\end{aligned}\]

Almost trivially, the solutions to \(\sum_{i=1}^n \psi_2^* = 0\) and
\(\sum_{i=1}^n \psi_3^* = 0\) are respectively
\(\left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i; \theta)\right]\) and
\(\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i; \theta)\right]\).

Thus, \[\begin{aligned}
0&= \sum_{i=1}^n \psi_1^*,\sum_{i=1}^n \psi_2^* = 0,\sum_{i=1}^n \psi_3^* = 0\\
\implies 0 &= \sum_{i=1}^n \left(2w_1 \psi_1 (Y_i;  \theta) \left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i;  \theta)\right] +2w_2\psi_2 (Y_i;  \theta)\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i;  \theta)\right] \right) \\
\implies 0 &= \frac 1 n\sum_{i=1}^n \left(2w_1 \psi_1 (Y_i;  \theta) \left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i;  \theta)\right] +2w_2\psi_2 (Y_i;  \theta)\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i;  \theta)\right] \right) \\
&= 2w_1\left[\frac 1 n \sum_{i=1}^n \psi_1 (Y_i;  \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_1 '(Y_i;  \theta)\right] +2w_2\left[\frac 1 n \sum_{i=1}^n \psi_2 (Y_i;  \theta)\right]\left[\frac 1 n \sum_{i=1}^n \psi_2 '(Y_i;  \theta)\right] \\
&= S(\mathbf Y; \theta)
\end{aligned}\]

\bookmarksetup{startatroot}

\hypertarget{chapter-8-hypothesis-tests-under-misspecification-and-relaxed-assumptions}{%
\chapter{Chapter 8: Hypothesis Tests under Misspecification and Relaxed
Assumptions}\label{chapter-8-hypothesis-tests-under-misspecification-and-relaxed-assumptions}}

\hypertarget{extra-problem-1}{%
\section{Extra Problem 1}\label{extra-problem-1}}

Prove Equation (8.9) on Page 341

Solution:

Note that
\(\frac \partial {\partial \boldsymbol \theta^T} \log f(y;\boldsymbol \theta_g) = \mathbf 0\)
as it is a score equation.

Also note that
\(\frac {\partial^2} {\partial \boldsymbol \theta \partial \boldsymbol \theta^T} \{\log f(y;{\boldsymbol \theta}_g)\} \overset p \to E \left( \frac {\partial^2} {\partial \boldsymbol \theta \partial \boldsymbol \theta^T} \{\log f(y;{\boldsymbol \theta}_g)\} \right)= - \mathbf A\)

Begin by taking a Taylor expansion of the score function of
\(S(y;\boldsymbol \theta) = \frac \partial {\partial \boldsymbol \theta^T} \log f(y;\boldsymbol \theta)\).

\[\begin{aligned}
\frac \partial {\partial \boldsymbol \theta^T} \log f(y;\boldsymbol \theta) &= \frac \partial {\partial \boldsymbol \theta^T} \log f(y;\boldsymbol \theta_g) + n\frac {\partial^2} {\partial \boldsymbol \theta \partial \boldsymbol \theta^T} \{\log f(y;{\boldsymbol \theta}_g)\}(\boldsymbol \theta -  {\boldsymbol \theta}_g) \\
\sqrt n(\boldsymbol \theta -  {\boldsymbol \theta}_g) &= \left[\frac {\partial^2} {\partial \boldsymbol \theta \partial \boldsymbol \theta^T} \{\log f(y;{\boldsymbol \theta}_g)\}\right]^{-1}\begin{pmatrix} \mathbf S_1 (\boldsymbol \theta)/ \sqrt n \\ 0 \end{pmatrix} \\
& \overset d \to -\mathbf A^{-1} \begin{pmatrix} \mathbf Z \\ \mathbf 0 \end{pmatrix} ~~~~ where~ Z \sim MVN\left(\mathbf 0,\mathbf V_{gS_1}\right)
\end{aligned}\]

Taking a Taylor Expansion of \(\log f(y;\boldsymbol \theta)\), we get:

\[\begin{aligned}
\log f(y;\boldsymbol \theta) &= \log f(y;{\boldsymbol \theta}_g) + \frac \partial {\partial \boldsymbol \theta^T} \{\log f(y;  {\boldsymbol \theta}_g)\}(\boldsymbol \theta - {\boldsymbol \theta}_g) + \sqrt n(\boldsymbol \theta - \boldsymbol \theta_g)^T\frac {\partial^2} {\partial \boldsymbol \theta \partial \boldsymbol \theta^T} \{\log f(y;{\boldsymbol \theta}_g)\}\sqrt n(\boldsymbol \theta -  {\boldsymbol \theta}_g) \\
\implies T_{LR} &= -2[\ell(\boldsymbol \theta)- \ell (\boldsymbol \theta_g)] \\
&= -\sqrt n(\boldsymbol \theta - \boldsymbol \theta_g)^T\frac {\partial^2} {\partial \boldsymbol \theta \partial \boldsymbol \theta^T} \{\log f(y;  {\boldsymbol \theta}_g)\}\sqrt n(\boldsymbol \theta -  {\boldsymbol \theta}_g) \\
& \overset d \to \begin{pmatrix} \mathbf Z & \mathbf 0 \end{pmatrix} {\mathbf A^{-1}}^T{\mathbf {AA}^{-1}} \begin{pmatrix} \mathbf Z \\ \mathbf 0 \end{pmatrix} \\
& \overset d \to  \mathbf Z ^T{\mathbf {A}^{-1}}_{11} \mathbf Z \\
& \overset d \to  \mathbf Z ^T[\mathbf{A_{11}- A_{12}A_{22}^{-1}A_{21}}]^{-1} \mathbf Z
\end{aligned}\]

\newpage

\hypertarget{section-48}{%
\section{8.10}\label{section-48}}

Suppose we have data \(X_1, \dots, X_n\) that are iid. The sign test for
\(H_0: median = 0\) is to count the number of \(X\)'s above \(0\), say
\(Y\), and compare \(Y\) to a binomial(\(n,p=1/2\)) distribution.
Starting with the defining M-estimator equation for the sample median
(see Example 7.4.2),

Defining M-estimator for the sample median (Example 7.4.2):

\(\hat \theta = \hat \eta_{1/2} = F_n^{-1}(1/2)\) satisfies
\(\sum \left[\frac 1 2 - I(Y_i \leq \hat \theta) \right] = c_n\), where
\(|c_n| = n\left|F_n^{-1}(\hat \theta)- \frac 1 2\right| \leq 1\).

\[\begin{aligned}
\implies \psi(X_i, \theta) &= \frac 1 2 - I(X_i \leq \theta) \\
A(\theta_0) = f(\theta_0) ~~~~&,~~~B(\theta_0) = \frac 1 2\left(1-\frac 1 2\right)=\frac 1 4 \\
V(\theta_0) &= \frac{1/4}{f^2(\theta_0)}
\end{aligned}\]

\hypertarget{a.-9}{%
\subsection{a.}\label{a.-9}}

Derive the generalized score statistic \(T_{GS}\) for
\(H_0: median = 0\) and note that it is the large sample version of the
two-sided sign test statistic.

Solution:

\[\begin{aligned}
T_{GS} &= n^{-1} \left[\sum \psi(X_i, \overset \sim \theta) \right] \overset \sim V_{\psi}^{-1} \left[\sum \psi(X_i, \overset \sim \theta) \right]  & \text{(8.20) pg. 347, scalar } \psi \text{ so no transpose}\\
V_{\psi} &= B = \frac 1 4  & \text{one dimensional case reduction, pg. 347} \\
\implies T_{GS} &= \frac {1/\overset \sim B} n \left(\sum \frac 1 2 - I(X_i \leq \overset \sim \theta) \right)^2 \\
&= \frac {1} {n \overset \sim p(1- \overset \sim p)} \left(\frac n 2 - \sum I(X_i \leq 0) \right)^2\\
&= \frac {1} {n\bar X \left(1- \bar X \right)} \left(\frac n 2 - \sum I(X_i \leq 0) \right)^2
\end{aligned}\]

\hypertarget{b.-7}{%
\subsection{b.}\label{b.-7}}

Using the expression for the asymptotic variance of the sample median,
write down the form of a generalized Wald statistic \(T_{GW}\), and
explain why it is not as attractive to use her as \(T_{GS}\)

Solution:

\[\begin{aligned}
T_{GW} &= n(\hat \theta - \hat \theta_0)\hat V^{-1}(\hat \theta - \theta_0)\\
&=\frac{n \hat p(1-\hat p)(\hat p- 1/2)^2}{f^2(\hat p)} \\
\end{aligned}\]

The generalized score statistic is more attractive as we don't rely on
\(f\), which could be misspecified.

\newpage

\hypertarget{extra-problem-2}{%
\section{Extra Problem 2}\label{extra-problem-2}}

Derive the generalized score test for the two independent samples of
clustered binary data as described in Example 8.5.

Solution:

\[\begin{aligned}
\boldsymbol \psi(\mathbf x_i,Y_i, \boldsymbol \beta) &= (Y_i - m_i \mathbf x_i^T \boldsymbol \beta) \mathbf x_i = \mathbf 0
\end{aligned}\]

\(\mathbf x_i^T = (1,0)\) for first sample, \((1,-1)\) for second
sample. \(\boldsymbol \beta^T = (\beta_1, \beta_2)=(p_1,p_1-p_2)\)

\(\overset \sim p = \sum_{i=1}^n Y_i/\sum_{i=1}^n m_i, \hat p_1 = \sum_{i=1}^{n_1} Y_i/m_1, , \hat p_2 = \sum_{i=1+n_1}^{n} Y_i/m_1\)

where
\(n = n_1+n_2, m_1 = \sum_{i=1}^{n_1} m_i, m_2 = \sum_{i=1+n_1}^{n} m_i\)

\[\begin{aligned}
\boldsymbol {\psi \psi}^T  &=(Y_i - m_i \mathbf x_i^T \boldsymbol \beta) \mathbf x_i \mathbf x_i^T(Y_i - m_i \mathbf x_i^T \boldsymbol \beta)^T \\
&=(Y_i - m_i \mathbf x_i^T \boldsymbol \beta) (Y_i - m_i \boldsymbol \beta^T\mathbf x_i)\mathbf x_i \mathbf x_i^T & (Y_i - m_i \mathbf x_i^T \boldsymbol \beta) \text{ is scalar}\\
&=(Y_i^2 -Y_i m_i  \boldsymbol \beta^T\mathbf x_i- Y_im_i \mathbf x_i^T \boldsymbol \beta + m_i^2 \mathbf x_i^T \boldsymbol \beta \boldsymbol \beta^T\mathbf x_i)\mathbf x_i \mathbf x_i^T \\
&=(Y_i^2 -2Y_i m_i  \boldsymbol \beta^T\mathbf x_i + m_i^2 \mathbf x_i^T \boldsymbol \beta \boldsymbol \beta^T\mathbf x_i)\mathbf x_i \mathbf x_i^T & \beta^T\mathbf x_i \text{ is scalar} \\
\boldsymbol \psi'  &= - m_i \mathbf x_i^T \mathbf x_i & \text{product rule}
\end{aligned}\]

Using Microsoft Mathematics for matrix multiplication,

\[\begin{aligned}
\beta^T \mathbf x_i &= \begin{cases} \beta_1  \\
\beta_1 - \beta_2 \end{cases}
= \begin{cases} p_1 \text{ if first sample} \\
p_2 \text{ if second sample}\end{cases} \\ \\ 
\mathbf x_i^T \boldsymbol \beta \boldsymbol \beta^T\mathbf x_i &= \begin{cases}
\beta_1^2  \\
(\beta_1 - \beta_2)^2 \end{cases}
=\begin{cases}
p_1^2 \text{ if first sample} \\
p_2^2 \text{ if second sample}\end{cases} \\
\mathbf x_i \mathbf x_i^T &= \begin{cases}
\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \text{ if first sample} \\
\begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix} \text{ if second sample}
\end{cases}
\end{aligned}\]

Also note that \(E(Y_i) =m_ip_j\) and
\(E(Y_i^2) = m_ip_j(1-p_j) + m_i^2p_j^2\) for sample \(j\).

Therefore,

\[\begin{aligned}
\mathbf A &= - \frac 1 n \sum_{i=1}^n \boldsymbol \psi' &\text{p. 301}\\
&= \frac 1 n \sum_{i=1}^n m_i \mathbf x_i^T \mathbf x_i \\
&= \frac 1 n \sum_{i=1}^{n_1} m_i \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} + \frac 1 n \sum_{i=1+n_1}^{n} m_i \begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix} \\
&= \frac 1 n \begin{pmatrix} m_1+m_2 & -m_2 \\ -m_2 & m_2 \end{pmatrix}
\end{aligned}\]

\[\begin{aligned}
\mathbf B &= \frac 1 n \sum_{i=1}^n \boldsymbol \psi \boldsymbol \psi^T \\
&= \frac 1 n \sum_{i=1}^n (Y_i^2 -2Y_i m_i  \boldsymbol \beta^T\mathbf x_i + m_i^2 \mathbf x_i^T \boldsymbol \beta \boldsymbol \beta^T\mathbf x_i)\mathbf x_i \mathbf x_i^T \\
&= \frac 1 n \sum_{i=1}^n (Y_i^2 -2Y_i m_i p_{j[i]} + m_i^2 p_{j[i]}^2)\mathbf x_i \mathbf x_i^T ~~~ j \text{ indicates sample 1 or 2}\\
&= \frac 1 n \sum_{i=1}^{n+1} (Y_i - m_i p_1)^2 \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} + \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 \begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix}\\
&= \frac 1 n \begin{pmatrix} \sum_{i=1}^{n+1} (Y_i - m_i p_1)^2+  \sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2&
- \sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 \\ - \sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 &  \sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2 \end{pmatrix}
\end{aligned}\]

Under \(\beta_2 = p_1- p_2 =0\), we get
\(p_1 = p_2 = \overset \sim p= \beta_1\), thus,

\[\overset \sim {\mathbf B} = \frac 1 n \begin{pmatrix} \sum_{i=1}^{n+1} (Y_i - m_i \overset \sim p )^2+  \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2&
- \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 \\ - \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 &  \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 \end{pmatrix}\]

\[\begin{aligned}
T_{GS} &= n^{-1} \left[\sum \boldsymbol \psi_2(Y_i, \overset \sim {\boldsymbol \beta}) \right] \overset \sim V_{\psi_2}^{-1} \left[\sum \boldsymbol \psi_2(Y_i, \overset \sim {\boldsymbol \beta}) \right]  ~~~~~~~~~~~ \text{pg. 347 }\\
\mathbf V_{\psi_1} &= \mathbf{B_{11} - A_{12}A_{22}^{-1}B_{21}- B_{12}\{A_{22}^{-1}\}^TA_{12}^T+A_{12}A_{22}^{-1}B_{22}{\{A_{22}}^{-1}\}^TA_{12}^T}
\end{aligned}\]

\[\begin{aligned}
\mathbf V_{\psi_2} &= \mathbf{B_{22} - A_{21}A_{11}^{-1}B_{12}- B_{21}\{A_{11}^{-1}\}^TA_{21}^T+A_{21}A_{11}^{-1}B_{11}{\{A_{11}}^{-1}\}^TA_{21}^T} \\
\overset \sim {\mathbf V}_{\psi_2} &= \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 - \left(\frac {-m_2}{n} \right)\left(\frac {n}{m_1+m_2} \right)\left(-\frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 \right) \\
&~~~~~ - \left(-\frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 \right)\left(\frac {n}{m_1+m_2} \right)\left(\frac {-m_2}{n} \right) \\
&~~~~~ + \left( \frac {-m_2}{n} \right)\left( \frac {n}{m_1+m_2}\right)\left( \frac 1 n\sum_{i=1}^{n+1} (Y_i - m_i p_1)^2+ \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i p_2)^2\right) \left(\frac {n}{m_1+m_2} \right) \left(\frac{-m_2}{n} \right) \\
&= \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 - \left(\frac {2m_2}{m_1+m_2} \right)\left(\frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2 \right) \\
&~~~~~ + \left( \frac {m_2^2}{(m_1+m_2)^2}\right)\left( \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2\right)+ \left( \frac {m_2^2}{(m_1+m_2)^2}\right)\left( \frac 1 n \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2\right)
\end{aligned}\] \[\begin{aligned}
&=\left( 1-\frac {m_2}{m_1+m_2}\right)^2\left( \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2\right)+ \left( \frac {m_2}{(m_1+m_2)}\right)^2\left( \frac 1 n \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2\right)\\
&=\left( \frac {m_1}{m_1+m_2}\right)^2\left( \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2\right)+ \left( \frac {m_2}{m_1+m_2}\right)^2\left( \frac 1 n \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2\right)
\end{aligned}\]

Again, using Microsoft mathematics

In the first sample,
\[\boldsymbol \psi =(Y_i - m_i \mathbf x_i^T \boldsymbol \beta) \mathbf x_i = \begin{pmatrix} Y_i -m_i \beta_1 \\0 \end{pmatrix}\]

In the second sample,
\[\boldsymbol \psi =(Y_i - m_i \mathbf x_i^T \boldsymbol \beta) \mathbf x_i = \begin{pmatrix} Y_i -m_i (\beta_1- \beta_2) \\ -Y_i +m_i (\beta_1- \beta_2)\end{pmatrix}\]

Combining those samples,
\[\boldsymbol \psi =(Y_i - m_i \mathbf x_i^T \boldsymbol \beta) \mathbf x_i =
\begin{pmatrix} \sum_{i=1}^{n_1} Y_i -m_i \beta_1 + \sum_{i=1+ n_1}^{n} Y_i -m_i (\beta_1- \beta_2) \\ \sum_{i=1+ n_1}^{n} -Y_i +m_i (\beta_1- \beta_2)\end{pmatrix}\]

Thus, \[\begin{aligned}
T_{GS} &= n^{-1} \left[\sum \boldsymbol \psi_2(Y_i, \overset \sim {\boldsymbol \beta}) \right] \overset \sim V_{\psi_2}^{-1} \left[\sum \boldsymbol \psi_2(Y_i, \overset \sim {\boldsymbol \beta}) \right] \\
&=  \frac{n^{-1}\left[\sum_{i= 1+n_1}^{n_2}-Y_i +m_i (\overset \sim \beta_1- \overset \sim \beta_2) \right]^2}{\left( \frac {m_1}{m_1+m_2}\right)^2\left( \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2\right)+ \left( \frac {m_2}{m_1+m_2}\right)^2\left( \frac 1 n \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2\right)} \\
&=  \frac{n^{-1}\left[\sum_{i= 1+n_1}^{n_2}-Y_i +m_i \overset \sim p \right]^2}{\left( \frac {m_1}{m_1+m_2}\right)^2\left( \frac 1 n \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2\right)+ \left( \frac {m_2}{m_1+m_2}\right)^2\left( \frac 1 n \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2\right)} \\
&=  \frac{(m_1+ m_2)^2\left[m_2 \overset \sim p -\sum_{i= 1+n_1}^{n_2}Y_i \right]^2}{m_1^2 \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2+ m_2^2  \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2} \\
&=  \frac{(m_1+ m_2)^2\left[m_2 \frac{\sum_{i=1}^{n_1} Y_i+\sum_{i=1+n_1}^{n} Y_i}{m_1 + m_2} -\frac{(m_1+m_2)\sum_{i= 1+n_1}^{n_2}Y_i}{m_1+m_2} \right]^2}{m_1^2 \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2+ m_2^2  \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2} \\
&=  \frac{\left[m_2 (\sum_{i=1}^{n_1} Y_i+\sum_{i=1+n_1}^{n} Y_i) -(m_1+m_2)\sum_{i= 1+n_1}^{n_2}Y_i \right]^2}{m_1^2 \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2+ m_2^2  \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2} \\
&=  \frac{\left[m_2 \sum_{i=1}^{n_1} Y_i -m_1\sum_{i= 1+n_1}^{n_2}Y_i \right]^2}{m_1^2 \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2+ m_2^2  \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2} \\
&=  \frac{\left[m_1m_2 \hat p_1 -m_1 m_2 \hat p_2 \right]^2}{m_1^2 \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2+ m_2^2  \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2} \\
&=  \frac{m_1^2m_2^2\left[\hat p_1 -\hat p_2 \right]^2}{m_1^2 \sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2+ m_2^2  \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2} \\
&=  \frac{\left[\hat p_1 -\hat p_2 \right]^2}{\sum_{i=1+n_1}^{n} (Y_i - m_i \overset \sim p )^2/m_2^2+  \sum_{i=1}^{n_1} (Y_i - m_i \overset \sim p )^2/m_1^2} \\
\end{aligned}\]

Which matches the book. (8.25 pg. 350).

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-boos2013essential}{}}%
Boos, D. D., and L. A. Stefanski. 2013. \emph{Essential Statistical
Inference: Theory and Methods}. Springer Texts in Statistics. Springer
New York.
\url{https://link.springer.com/book/10.1007/978-1-4614-4818-1}.

\end{CSLReferences}



\end{document}
