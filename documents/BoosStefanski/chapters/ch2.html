<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Selected Solutions to Boos and Stefanski - Chapter 2: Likelihood Construction and Estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/ch3.html" rel="next">
<link href="../chapters/ch1.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/ch2.html"><span class="chapter-title">Chapter 2: Likelihood Construction and Estimation</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Selected Solutions to Boos and Stefanski</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ch1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 1: Roles of Modeling in Statistical Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ch2.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Chapter 2: Likelihood Construction and Estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ch3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 3: Likelihood-Based Tests and Confidence Regions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ch5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 5: Large Sample Theory: The Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ch6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 6: Large Sample Results for Likelihood-Based Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ch7.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 7: M-Estimation (Estimating Equations)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/ch8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 8: Hypothesis Tests under Misspecification and Relaxed Assumptions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#section" id="toc-section" class="nav-link active" data-scroll-target="#section">2.1</a></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1">2.3</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a">a)</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b">b)</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c">c)</a></li>
  </ul></li>
  <li><a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2">2.4</a>
  <ul class="collapse">
  <li><a href="#a-derive-the-maximum-likelihood-estimator-of-lambda.-call-it-hatlambda_mle_1" id="toc-a-derive-the-maximum-likelihood-estimator-of-lambda.-call-it-hatlambda_mle_1" class="nav-link" data-scroll-target="#a-derive-the-maximum-likelihood-estimator-of-lambda.-call-it-hatlambda_mle_1">a) Derive the maximum likelihood estimator of <span class="math inline">\(\lambda\)</span>. Call it <span class="math inline">\(\hat{\lambda}_{MLE_1}\)</span></a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1">b)</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1">c)</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d">d)</a></li>
  </ul></li>
  <li><a href="#section-3" id="toc-section-3" class="nav-link" data-scroll-target="#section-3">2.9</a></li>
  <li><a href="#section-4" id="toc-section-4" class="nav-link" data-scroll-target="#section-4">2.12</a></li>
  <li><a href="#section-5" id="toc-section-5" class="nav-link" data-scroll-target="#section-5">2.16</a></li>
  <li><a href="#section-6" id="toc-section-6" class="nav-link" data-scroll-target="#section-6">2.20</a></li>
  <li><a href="#section-7" id="toc-section-7" class="nav-link" data-scroll-target="#section-7">2.21</a></li>
  <li><a href="#section-8" id="toc-section-8" class="nav-link" data-scroll-target="#section-8">2.22</a>
  <ul class="collapse">
  <li><a href="#a." id="toc-a." class="nav-link" data-scroll-target="#a.">a.</a></li>
  <li><a href="#b." id="toc-b." class="nav-link" data-scroll-target="#b.">b.</a></li>
  <li><a href="#c." id="toc-c." class="nav-link" data-scroll-target="#c.">c.</a></li>
  <li><a href="#d." id="toc-d." class="nav-link" data-scroll-target="#d.">d.</a></li>
  </ul></li>
  <li><a href="#section-9" id="toc-section-9" class="nav-link" data-scroll-target="#section-9">2.25</a>
  <ul class="collapse">
  <li><a href="#a.-1" id="toc-a.-1" class="nav-link" data-scroll-target="#a.-1">a.</a></li>
  <li><a href="#b.-1" id="toc-b.-1" class="nav-link" data-scroll-target="#b.-1">b.</a></li>
  </ul></li>
  <li><a href="#section-10" id="toc-section-10" class="nav-link" data-scroll-target="#section-10">2.26</a>
  <ul class="collapse">
  <li><a href="#a.-2" id="toc-a.-2" class="nav-link" data-scroll-target="#a.-2">a.</a></li>
  <li><a href="#b.-2" id="toc-b.-2" class="nav-link" data-scroll-target="#b.-2">b.</a></li>
  <li><a href="#c.-1" id="toc-c.-1" class="nav-link" data-scroll-target="#c.-1">c.&nbsp;</a></li>
  <li><a href="#d.-1" id="toc-d.-1" class="nav-link" data-scroll-target="#d.-1">d.</a></li>
  <li><a href="#e." id="toc-e." class="nav-link" data-scroll-target="#e.">e.</a></li>
  <li><a href="#f." id="toc-f." class="nav-link" data-scroll-target="#f.">f.</a></li>
  </ul></li>
  <li><a href="#section-11" id="toc-section-11" class="nav-link" data-scroll-target="#section-11">2.35.</a></li>
  <li><a href="#section-12" id="toc-section-12" class="nav-link" data-scroll-target="#section-12">2.37</a>
  <ul class="collapse">
  <li><a href="#a.-3" id="toc-a.-3" class="nav-link" data-scroll-target="#a.-3">a.</a></li>
  <li><a href="#b.-3" id="toc-b.-3" class="nav-link" data-scroll-target="#b.-3">b.</a></li>
  </ul></li>
  <li><a href="#section-13" id="toc-section-13" class="nav-link" data-scroll-target="#section-13">2.41</a></li>
  <li><a href="#section-14" id="toc-section-14" class="nav-link" data-scroll-target="#section-14">2.43</a></li>
  <li><a href="#section-15" id="toc-section-15" class="nav-link" data-scroll-target="#section-15">2.44</a></li>
  <li><a href="#section-16" id="toc-section-16" class="nav-link" data-scroll-target="#section-16">2.45</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1">a)</a></li>
  <li><a href="#b-2" id="toc-b-2" class="nav-link" data-scroll-target="#b-2">b)</a></li>
  </ul></li>
  <li><a href="#section-17" id="toc-section-17" class="nav-link" data-scroll-target="#section-17">2.47</a>
  <ul class="collapse">
  <li><a href="#a.-4" id="toc-a.-4" class="nav-link" data-scroll-target="#a.-4">a.</a></li>
  <li><a href="#b.-4" id="toc-b.-4" class="nav-link" data-scroll-target="#b.-4">b.</a></li>
  </ul></li>
  <li><a href="#section-18" id="toc-section-18" class="nav-link" data-scroll-target="#section-18">2.48</a></li>
  <li><a href="#section-19" id="toc-section-19" class="nav-link" data-scroll-target="#section-19">2.49</a>
  <ul class="collapse">
  <li><a href="#a.-e-step." id="toc-a.-e-step." class="nav-link" data-scroll-target="#a.-e-step.">a. E step.</a></li>
  <li><a href="#b.-m-step." id="toc-b.-m-step." class="nav-link" data-scroll-target="#b.-m-step.">b. M step.</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 2: Likelihood Construction and Estimation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">2.1</h2>
<p>Let <span class="math inline">\(Y_1,\dots, Y_n\)</span> be iid positive random variables such that <span class="math inline">\(Y^{(\lambda)}\)</span> is assumed to have a normal<span class="math inline">\((\mu,\sigma^2)\)</span> distribution, where</p>
<p><span class="math display">\[ Y^{(\lambda)} =
\begin{cases}
\frac{Y^\lambda-1}{\lambda} &amp;when~ \lambda \neq 0 \\
\log(Y)&amp; when~ \lambda =0.
\end{cases}
\]</span> Derive the log likelihood <span class="math inline">\(\ell_n(\mu,\sigma, \lambda | \mathbf{Y})\)</span> of the observed data <span class="math inline">\(Y_1,\dots, Y_n\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[ \begin{aligned}
P\left(\frac{Y^{(\lambda)}-1} \lambda \leq \frac{y^{(\lambda)}-1} \lambda  \right) &amp;= P\left(Y^{(\lambda)}\leq \frac{y^{(\lambda)}-1} \lambda \right)  &amp; \text{for } \lambda \neq0\\
&amp;=  \Phi \left(\frac{\frac{y^{(\lambda)}-1} \lambda  - \mu}{\sigma} \right) \\
\implies f_{\frac{Y^{(\lambda)}-1} \lambda} &amp;= \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\frac{y^{(\lambda)}-1} \lambda  - \mu}{\sigma}\right)^2\right\}\left(\frac{1}{\lambda\sigma}\right)
\end{aligned}
\]</span></p>
<p><span class="math display">\[ \begin{aligned}
P\left(\log(Y) \leq \log(y\right) &amp;= P\left( Y^{(\lambda)} &lt;\log(y) \right) \text{ for }\lambda =0\\
&amp;= \Phi \left( \frac{\log (y)- \mu}{\sigma} \right)\\
\implies f_{\log(Y)} &amp;= \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\log (y)- \mu}{\sigma}\right)^2\right\}\left(\frac{1}{y\sigma}\right)
\end{aligned}
\]</span> <span class="math display">\[
\begin{aligned}
\mathcal{L} (\mu,\sigma |\mathbf{Y}) &amp;= \prod_{i=1}^n \left[\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\frac{y_i^{(\lambda)}-1} \lambda  - \mu}{\sigma}\right)^2\right\}\left(\frac{1}{\lambda\sigma}\right)\right]^{I(\lambda \neq0)} \left[\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{1}{2}\left(\frac{\log (y_i)- \mu}{\sigma}\right)^2\right\}\left(\frac{1}{y_i\sigma}\right)\right]^{I(\lambda =0)} \\
&amp;= \left[\frac{1}{\sigma^2\sqrt{2\pi }}\right]^n\prod_{i=1}^n \left[\exp\left\{-\frac{1}{2}\left(\frac{\frac{y_i^{(\lambda)}-1} \lambda  - \mu}{\sigma}\right)^2\right\}\left(\frac{1}{\lambda}\right)\right]^{I(\lambda \neq0)} \left[\exp\left\{-\frac{1}{2}\left(\frac{\log (y_i)- \mu}{\sigma}\right)^2\right\}\left(\frac{1}{y_i}\right)\right]^{I(\lambda =0)} \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\implies \ell (\mu,\sigma |\mathbf{Y}) &amp;=\sum_{i=1}^n\left[ \left( -\frac{1}{2}\left(\frac{\frac{y_i^{(\lambda)-\mu}} \lambda- \mu}{\sigma}\right)^2 - \log(\lambda)\right){I(\lambda \neq0)} +
\left(-\frac{1}{2}\left(\frac{\log (y_i)- \mu}{\sigma}\right)^2-\log(y_i)\right){I(\lambda =0)} \right] \\
&amp;~~~~~~~~~~~~~~~~~~~~~~~+n \log\left(\frac{1}{\sigma^2\sqrt{2\pi}}\right)
\end{aligned}\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-1" class="level2">
<h2 class="anchored" data-anchor-id="section-1">2.3</h2>
<p>Recall the ZIP model</p>
<p><span class="math display">\[
\begin{aligned}
P(Y = 0) &amp;= p+(1-p)e^{-\lambda} \\
P(Y=y) &amp;= (1-p)\frac{\lambda^y e^{-\lambda}}{y!} &amp; y=1,2,...
\end{aligned}
\]</span></p>
<section id="a" class="level3">
<h3 class="anchored" data-anchor-id="a">a)</h3>
<p>Reparametrize the model by defining <span class="math inline">\(\pi \equiv P(Y =0) = p+(1-p)e^{-\lambda}\)</span>. Solve for <span class="math inline">\(p\)</span> in terms of <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\lambda\)</span>, then substitute so that the density only depends on them.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\pi &amp;= p+(1-p)e^{-\lambda} \\
&amp;=p (1-e^{-\lambda}) + e^{-\lambda} \\
\implies p &amp;= \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}}
\end{aligned}
\]</span></p>
<p><span class="math display">\[ \begin{aligned}
P(Y = y) &amp;= \left(p+(1-p)e^{-\lambda}\right)^{I(y=0)}\left((1-p)\frac{\lambda^y e^{-\lambda}}{y!}\right)^{I(y \in \mathbb{N}^+)} \\
&amp;= \left[\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)+ \left(1-\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)\right)e^{-\lambda}\right]^{I(y=0)}
\left[\left(1-\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)} \\
&amp;= \left[\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)+ \left( \frac{1-e^{-\lambda}-\pi+e^{-\lambda}}{1-e^{-\lambda}}\right)e^{-\lambda}\right]^{I(y=0)}
\left[\left( \frac{1-e^{-\lambda}-\pi+e^{-\lambda}}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)}\\
&amp;= \left[\left( \frac{\pi-e^{-\lambda}}{1-e^{-\lambda}} \right)+ \left( \frac{1-\pi}{1-e^{-\lambda}}\right)e^{-\lambda}\right]^{I(y=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)} \\
&amp;= \left[\left( \frac{\pi-\pi e^{-\lambda}}{1-e^{-\lambda}}\right)\right]^{I(y=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)} \\
&amp;= \pi^{I(y=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)}
\end{aligned}
\]</span> </p>
</section>
<section id="b" class="level3">
<h3 class="anchored" data-anchor-id="b">b)</h3>
<p>Let <span class="math inline">\(n_0\)</span> represent the number of samples in an iid sample of size <span class="math inline">\(n\)</span>. Assuming the complete data is available, show that the likelihood factors into two pieces and that <span class="math inline">\(\hat{\pi} = n_0/n\)</span>. Also show that the MLE for <span class="math inline">\(\lambda\)</span> is the solution to a simple nonlinear equation involving <span class="math inline">\(\bar{Y}_+\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[ \begin{aligned}
\mathcal{L}(\lambda, p | \mathbf{Y}) &amp;= \prod_{i=1}^n\pi^{I(y_i=0)}
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y_i \in \mathbb{N}^+)} \\
&amp;= \pi^{n_0}(1-\pi)^{n-n_0}\prod_{Y_i\in \mathbb{N}^+}\frac{\lambda^{y_i} e^{-\lambda}}{y_i! ( 1-e^{-\lambda})}
\end{aligned}
\]</span></p>
<p>To find <span class="math inline">\(\hat{\pi}\)</span>, we only need to maximize <span class="math inline">\(g(\pi) = \pi^{n_0}(1-\pi)^{n-n_0}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\log g(\pi) &amp;= n_0 \log (\pi) +(n-n_0)\log(1-\pi) \\
\frac{d \log g}{d\pi} &amp;= \frac{n_0}{\pi } -\frac{n-n_0}{1-\pi } \\
\frac{d^2 \log g}{d\pi^2} &amp;=-\frac{n_0}{\pi^2 } - \frac{n-n_0}{(1-\pi)^2}  &gt; 0
\end{aligned}
\]</span></p>
<p><span class="math display">\[\begin{aligned}
0 =\frac{d \log g}{d\pi} &amp;= \frac{n_0}{\pi } -\frac{n-n_0}{1-\pi }  \\
\implies \frac{\pi}{1-\pi} &amp;= \frac{n_0}{n-n_0} \\
\implies \pi\left(1+ \frac{n_0}{n-n_0}\right) &amp;= \frac{n_0}{n-n_0} \\
\implies \pi &amp;= \frac{n_0}{(n-n_0)\left(1+ \frac{n_0}{n-n_0}\right)} \\
&amp;= \frac{n_0}{n}
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\hat{\pi} = n_0/n\)</span>.</p>
<p>To find <span class="math inline">\(\hat{\lambda}\)</span>, we need to maximize <span class="math inline">\(h(\lambda) =\prod_{Y_i\in \mathbb{N}^+}\frac{\lambda^{y_i} e^{-\lambda}}{y_i! ( 1-e^{-\lambda})}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\log h(\lambda) &amp;= \sum_{Y_i \in \mathbb{N}^+} y_i\log(\lambda) -\lambda - \log(y_i!) -\log(1-e^{-\lambda}) \\
\implies \frac{d \log h}{d \lambda} &amp;=  (n-n_0) \left(\frac{\bar{y}_+}{\lambda}-1-\frac{ e^{-\lambda}}{1-e^{-\lambda}} \right) \\
0= \frac{d \log h}{d \lambda} &amp;=  (n-n_0) \left(\frac{\bar{y}_+}{\lambda}-1-\frac{ e^{-\lambda}}{1-e^{-\lambda}} \right) \\
&amp;=  \bar{y}_+-\lambda -\frac{ \lambda e^{-\lambda}}{1-e^{-\lambda}}
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\hat{\lambda}_{MLE}\)</span> will be a solution to the above simple non-linear equation, which involves <span class="math inline">\(\bar{y}_+\)</span>.</p>
</section>
<section id="c" class="level3">
<h3 class="anchored" data-anchor-id="c">c)</h3>
<p>Now consider the truncated or conditional sample consisting of the <span class="math inline">\(n- n_0\)</span> nonzero values. Write down the conditional likelihood for these values and obtain the same equation for <span class="math inline">\(\hat{\lambda}_{MLE}\)</span> as in a)</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
P(Y=y |Y&gt;0) &amp;= \frac{P(Y&gt;0 \cap Y=y)}{P(Y&gt;0)} \\
&amp;= \frac{
\left[\left( \frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^y e^{-\lambda}}{y!}\right]^{I(y \in \mathbb{N}^+)}}{1-\pi}
\\
&amp;= \left[\frac{\lambda^y e^{-\lambda}}{y!(1-e^{-\lambda})}\right]^{I(y \in \mathbb N^+)}
\end{aligned}
\]</span></p>
<p>Thus, we need to maximize the same function we found in part b), which will be the solution to a non-simple linear equation involving <span class="math inline">\(\bar{y}\)</span>.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-2" class="level2">
<h2 class="anchored" data-anchor-id="section-2">2.4</h2>
<p>In sampling land areas for counts of an animal species, we obtain an iid sample of counts <span class="math inline">\(Y_1, \dots, ,Y_n\)</span>, where each <span class="math inline">\(Y_i\)</span> has a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span></p>
<section id="a-derive-the-maximum-likelihood-estimator-of-lambda.-call-it-hatlambda_mle_1" class="level3">
<h3 class="anchored" data-anchor-id="a-derive-the-maximum-likelihood-estimator-of-lambda.-call-it-hatlambda_mle_1">a) Derive the maximum likelihood estimator of <span class="math inline">\(\lambda\)</span>. Call it <span class="math inline">\(\hat{\lambda}_{MLE_1}\)</span></h3>
<p>Solution:</p>
<p><span class="math display">\[ \begin{aligned}
\mathcal{L} (\lambda, \mathbf{Y}) &amp;= \prod_{i=1}^n \frac{e^{-\lambda} \lambda^{y_i}}{y_i!} \\
\implies \ell (\lambda, \mathbf{Y}) &amp;= \sum_{i=1}^n -\lambda + y_i\log(\lambda)- \log(y_i!) \\
&amp;=  -n\lambda + n\bar{y}\log(\lambda)- \sum_{i=1}^n \log(y_i!) \\
\implies \frac{d \ell}{d \lambda} &amp;= -n+ \frac{n\bar{y}}{\lambda} \\
\implies \frac{d^2 \ell}{d \lambda^2} &amp;= -\frac{n\bar{y}}{\lambda^2}
&lt; 0 \text{ because } \bar{y}, \lambda, n &gt;0.
\end{aligned}
\]</span></p>
<p><span class="math display">\[ \begin{aligned}
0=\frac{d \ell}{d \lambda} &amp;= -n+ \frac{n\bar{y}}{\hat{\lambda}_{MLE_1}}\\
\implies \hat{\lambda}_{MLE_1}= \bar{y}
\end{aligned}
\]</span></p>
</section>
<section id="b-1" class="level3">
<h3 class="anchored" data-anchor-id="b-1">b)</h3>
<p>For simplicity in quadrat sampling, sometimes only the presence or absence of a species is recorded. Let <span class="math inline">\(n_0\)</span> be the number of <span class="math inline">\(Y_i\)</span>’s that are zero. Write down the binomial likelihood based only on <span class="math inline">\(n_0\)</span>. Show that the maximum likelihood estimator of <span class="math inline">\(\lambda\)</span> based only on <span class="math inline">\(n_0\)</span> is <span class="math inline">\(\hat{\lambda}_{MLE_2}= -\log(n_0/n)\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
P(Y=0)&amp;= e^{-\lambda} \\
\implies \mathcal L (\lambda | \mathbf Y) &amp;= {n \choose n_0} (e^{-\lambda})^{n_0}(1-e^{-\lambda})^{n-n_o} \\
\implies \ell (\lambda | \mathbf Y) &amp;=\log{n \choose n_0}- n_0\lambda+(n-n_0)\log(1-e^{-\lambda}) \\
\implies \frac{ d\ell (\lambda | \mathbf Y)}{d \lambda} &amp;= - n_0+\frac{(n-n_0)e^{-\lambda}}{1-e^{-\lambda}} \\
\implies \frac{ d^2 \ell (\lambda | \mathbf Y)}{d \lambda^2} &amp;=  (n-n_0) \frac{-(1-e^{-\lambda})e^{-\lambda} -e^{-\lambda}(e^{-\lambda})}{(1-e^{-\lambda})^2} \\
&amp;=  \frac{-e^{-\lambda}(n-n_0)}{(1-e^{-\lambda})^2} &lt; 0 ~\text{since}~ n_0&lt;n
\end{aligned}
\]</span> <span class="math display">\[
\begin{aligned}
0= \frac{ d\ell (\lambda | \mathbf Y)}{d \lambda} &amp;= - n_0+\frac{(n-n_0)e^{-\lambda}}{1-e^{-\lambda}} \\
\implies e^{-\lambda} &amp;= \frac{n_0}{n-n_0}(1-e^{-\lambda}) \\
\implies e^{-\lambda} \left(1+ \frac{n_0}{n-n_0} \right) &amp;= \frac{n_0}{n-n_0} \\
\implies e^{-\lambda}  &amp;= \frac{n_0}{n-n_0}\left(\frac{n-n_0}{n} \right) \\
\implies \hat{\lambda}_{MLE_2} &amp;= -\log(n_0/n)
\end{aligned}
\]</span></p>
</section>
<section id="c-1" class="level3">
<h3 class="anchored" data-anchor-id="c-1">c)</h3>
<p>Use the delta theorem from Ch. 1 to show that the asymptotic relative efficiency of <span class="math inline">\(\hat{\lambda}_{MLE_1}\)</span> to <span class="math inline">\(\hat{\lambda}_{MLE_2}\)</span> is <span class="math inline">\(\frac{e^{\lambda}-1}{\lambda}\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\sqrt n [\bar{y} - \lambda] &amp;\overset {d}{\to} N(0,\lambda) &amp;\text{CLT, poisson distribution}\\
\sqrt n [n_0/n - e^{-\lambda}] &amp;\overset {d}{\to} N(0,e^{-\lambda}(1-e^{-\lambda})) &amp;\text{CLT, bernoulli distribution} \\
\sqrt n [-\log(n_0/n) - \log(e^{-\lambda}/n)] &amp;\overset {d}{\to} N\left(0,ne^{-\lambda}(1-e^{-\lambda})\left[-\frac{1}{e^{-\lambda}/n}\frac{1}{n}\right]^2 \right) &amp; \text{by Delta Method} \\
&amp;\overset {d}{\to} N\left(0,\frac{(1-e^{-\lambda})}{e^{-\lambda}} \right) \\
&amp;\overset {d}{\to} N\left(0,e^{\lambda}-1 \right) \\
\end{aligned}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[ARE (\hat{\lambda}_{MLE_1},\hat{\lambda}_{MLE_2}) = \frac{AVAR(\hat{\lambda}_{MLE_2})}{AVAR(\hat{\lambda}_{MLE_1})}= \frac{e^\lambda-1}{\lambda}\]</span></p>
</section>
<section id="d" class="level3">
<h3 class="anchored" data-anchor-id="d">d)</h3>
<p>The overall goal of the sampling is to estimate the mean number of the species per unit land area. Comment on the use of <span class="math inline">\(\hat{\lambda}_{MLE_2}\)</span> in place of <span class="math inline">\(\hat{\lambda}_{MLE_1}\)</span>. That is, explain to a researcher for what values and under what distributional assumptions is it reasonable?</p>
<p>Solution:</p>
<p>Since <span class="math inline">\(e^{\lambda}-1 &gt; \lambda\)</span>, <span class="math inline">\(\hat{\lambda}_{MLE_2}\)</span> will always be more efficient. However, the distributional assumptions behind when to use which estimator are inherently different. <span class="math inline">\(\hat{\lambda}_{MLE_2}\)</span> presumes that we have presence-absence data, or zero inflated poisson data at the worst, and is looking to predict the number of land units with the absence of a species. However, if we know the data absolutely comes from a poisson distribution, <span class="math inline">\(\hat{\lambda}_{MLE_1}\)</span> will fit the distributional assumptions better.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-3" class="level2">
<h2 class="anchored" data-anchor-id="section-3">2.9</h2>
<p>The sample <span class="math inline">\(Y_1,\dots, Y_n\)</span> is iid with distribution <span class="math inline">\(F_Y(y;p_o,p_1,\alpha, \beta)= p_0I(0 \leq y) + (1-p_0-p_1)F(y;\alpha,\beta) + p_1I(y\geq 1)\)</span>, where <span class="math inline">\(F(y;\alpha,\beta)\)</span> is the beta distribution. Use the <span class="math inline">\(2h\)</span> method to show that the likelihood is <span class="math inline">\(p_0^{n_0}p_1^{n_1}(1-p_0-p_1)^{n-n_0-n_1} \prod_{0&lt;Y_i&lt;1} f(Y_i;\alpha,\beta)\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal L(p_0,p_1,\alpha, \beta | \mathbf{Y}) &amp;= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n P_{p_0,p_1,p_2}\left(Y_i^* \in (Y_i-h,Y_i+h] |Y_i \right) \\
&amp;= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n F_i(Y_i+h;p_o,p_1,\alpha, \beta)-F_i(Y_i-h;p_o,p_1,\alpha, \beta) \\
&amp;= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n \big\{ p_0I(0 \leq y_i+h) + (1-p_0-p_1)F(y_i+h;\alpha,\beta) + p_1I(y_i+h\geq 1) \\
&amp;~~~~~~~~~-[p_0I(0 \leq y_i-h) + (1-p_0-p_1)F(y_i-h;\alpha,\beta) + p_1I(y_i-h\geq 1)] \big\} \\
&amp;= \lim_{h \to 0^+} (2h)^{-n} \prod_{i=1}^n \big\{ p_0[I(0 \leq y_i+h)-I(0 \leq y_i-h)] \\
&amp;~~~~~~~~~+ (1-p_0-p_1)[F(y_i+h;\alpha,\beta)-F(y_i-h;\alpha,\beta)] \\
&amp;~~~~~~~~~+ p_1[I(y_i+h\geq 1)-I(y_i-h\geq 1)] \big\} \\
&amp;=\prod_{i=1}^n \big\{ p_0\lim_{h \to 0^+} (2h)^{-n}[I(0 \leq y_i+h)-I(0 \leq y_i-h)]\\
&amp;~~~~~~~~~+ p_1\lim_{h \to 0^+} (2h)^{-n}[I(y_i+h\geq 1)-I(y_i-h\geq 1)] \big\} \\
&amp;~~~~~~~~~+ (1-p_0-p_1)\lim_{h \to 0^+} (2h)^{-n}[F(y_i+h;\alpha,\beta)-F(y_i-h;\alpha,\beta)]\big\}  \\
&amp;=\prod_{i=1}^n \big\{ p_0 I(y_i=0) + p_1I(y_i=1) + (1-p_0-p_1)f(y_i;\alpha,\beta)I(0&lt;y_i&lt;1) \\
&amp;=p_0^{n_0}p_1^{n_1}(1-p_0-p_1)\prod_{0&lt;Y_i&lt;1} f(y_i;\alpha,\beta) \\
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-4" class="level2">
<h2 class="anchored" data-anchor-id="section-4">2.12</h2>
<p>For an iid sample <span class="math inline">\(Y_1,\dots, Y_n\)</span>, Type II censoring occurs when we observe only the smallest <span class="math inline">\(r\)</span> values. For example, in a study of light bulb lifetimes, we might stop the study after the first <span class="math inline">\(r=10\)</span> bulbs have failed. Assuming a continuous distribution with density <span class="math inline">\(f(y;\mathbf{\theta})\)</span>, the likelihood is just the joint density of the smallest <span class="math inline">\(r\)</span> order statistics evaluated at those order statistics: <span class="math display">\[\mathcal L\left(\mathbf \theta; Y_{(1)}, \dots, Y_{(r)}\right) = \frac {n!}{(n-r)!}\left[\prod_{i=1}^r f\left(Y_{(i)}; \mathbf \theta \right) \right]\left[1-F\left(Y_{(r)}; \mathbf \theta \right)\right]^{n-r}\]</span> For this situation, let <span class="math inline">\(f(y;\sigma) = e^{-y/\sigma}/\sigma\)</span> and find the MLE of <span class="math inline">\(\sigma\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[ \begin{aligned}
\mathcal L\left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right) &amp;= \frac {n!}{(n-10)!}\left[\prod_{i=1}^{10} \frac {e^{-y_{(i)}/\sigma}} \sigma \right]\left[1-\left(1- e^{-y_{(10)}/\sigma}\right) \right]^{n-10} \\
&amp;= \frac {n!}{(n-10)!\sigma^{10}}\left[\prod_{i=1}^{10} {e^{-y_{(i)}/\sigma}} \right]\left[e^{-y_{(10)}/\sigma} \right]^{n-10} \\
\implies \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right) &amp;= \log \left[\frac {n!}{(n-10)!} \right] - 10 \log(\sigma) -  \frac{\sum_{i=1}^{10}y_{(i)}} {\sigma} - \frac{(n-10)y_{(10)}}{\sigma}\\
\implies \frac {\partial \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial \sigma} &amp;= - \frac{10}{\sigma} +  \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\sigma^2} \\
\implies \frac {\partial^2 \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial \sigma^2} &amp;= \frac{10}{\sigma^2} -  2\frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\sigma^3}
\end{aligned}\]</span> <span class="math display">\[ \begin{aligned}
0= \frac {\partial \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial {\sigma}} &amp;= - \frac{10}{\hat{\sigma}} +  \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\hat{\sigma}^2} \\
\implies  \hat{\sigma} &amp;= \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10}
\end{aligned}
\]</span> Verifying this is a maximum, <span class="math display">\[\begin{aligned}
\frac {\partial^2 \ell \left(\sigma; Y_{(1)}, \dots, Y_{(r)}\right)}{\partial \sigma^2} \bigg|_{\sigma = \hat \sigma}&amp;= \frac{10}{\left(\frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10}\right)^2} - 2 \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}} {\left( \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10} \right)^3} \\
&amp;=  \frac{-10^3}{\left({(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}\right)^2}\\
&amp;&lt; 0. ~~~~~~~~~~~\text{(denominator is clearly positive)}
\end{aligned}\]</span> Conclusively,</p>
<p><span class="math display">\[\hat{\sigma}_{MLE} = \frac{(n-10)y_{(10)}+ \sum_{i=1}^{10}y_{(i)}}{10}\]</span></p>
<p>More generally, I can follow the same steps outside this specific situation to find</p>
<p><span class="math display">\[\hat{\sigma}_{MLE} = \frac{(n-r)y_{(r)}+ \sum_{i=1}^{r}y_{(i)}}{r}\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-5" class="level2">
<h2 class="anchored" data-anchor-id="section-5">2.16</h2>
<p>The standard Box-Cox regression model (Box and Cox 1964) assumes that after transformation of the observed <span class="math inline">\(Y_i\)</span> to <span class="math inline">\(Y_i^{(\lambda)}\)</span>. we have the linear model <span class="math display">\[ Y_i^{(\lambda)} = \mathbf{x_i^T \beta} + e_i,~~~~~ i =1,\dots,n \]</span> where <span class="math inline">\(Y_i\)</span> is assumed positive and the <span class="math inline">\(x_i\)</span> <span class="math inline">\(i= 1, \dots, n\)</span>. In addition assume that <span class="math inline">\(e_1,\dots,e_n\)</span> are iid normal<span class="math inline">\((0, \sigma^2)\)</span> errors. Recall that the Box-Cox transformation is defined in Problem 2.1 (p.&nbsp;107) and is strictly increasing for all <span class="math inline">\(\lambda\)</span> Show that the likelihood is <span class="math display">\[\mathcal L \left(\beta,\sigma,\lambda| \{Y_i, \mathbf x_i\}_{i=1}^n \right) = \left(\sqrt{2\pi}\sigma\right)^n \exp\left[-\sum_{i=1}^n \frac{ \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \prod_{i=1}^n \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|\]</span></p>
<p>Solution:</p>
<p>We know from the Jacobian method of transformations that <span class="math display">\[f_Y(y;\theta) = f_{Y^{(\lambda)}} (y^{(\lambda)};\theta)\left|J\right|=f_{Y^{(\lambda)}} (y;\theta)\left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y}\right|\]</span>.</p>
<p>Thus, since <span class="math inline">\(Y_i^{(\lambda)} \overset {iid}\sim N(\mathbf{x^T \beta}, \sigma^2)\)</span>,</p>
<p><span class="math display">\[f\left(Y_i, \mathbf x_i|\beta,\sigma,\lambda \right) = \left(\sqrt{2\pi}\sigma\right) \exp\left[\frac{- \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|\]</span> which implies, <span class="math display">\[\begin{aligned}\mathcal L \left(\beta,\sigma,\lambda| \{Y_i, \mathbf x_i\}_{i=1}^n \right)
&amp;= \prod_{i=1}^n \left(\sqrt{2\pi}\sigma\right) \exp\left[\frac{- \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|\\
&amp;= \left(\sqrt{2\pi}\sigma\right)^n \exp\left[-\sum_{i=1}^n \frac{ \left(Y_i^{(\lambda)} - \mathbf{x}_i^T\mathbf \beta \right)^2}{2 \sigma^2}  \right] \prod_{i=1}^n \left|\frac{\partial t^{(\lambda)}}{\partial t} \bigg\rvert_{t= Y_i} \right|
\end{aligned}\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-6" class="level2">
<h2 class="anchored" data-anchor-id="section-6">2.20</h2>
<p>One version of the negative binomial probability mass function is given by <span class="math display">\[f(y;\mu,k) = \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)} \left( \frac{k}{\mu+k}\right)^k \left( 1- \frac{k}{\mu+k}\right)^y I(y \in \mathbb N)\]</span> where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(k\)</span> are parameters. Assume that <span class="math inline">\(k\)</span> is known and put <span class="math inline">\(f(y; \mu,k)\)</span> in the GLM form (2.14, p.&nbsp;53), identifying <span class="math inline">\(b(\theta)\)</span>, etc., and derive the mean and variance of <span class="math inline">\(Y, E(Y) = \mu, Var(Y)= \mu + \mu^2/k\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[ \begin{aligned}
f(y;\mu,k) &amp;= \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)} \left( \frac{k}{\mu+k}\right)^k \left( 1- \frac{k}{\mu+k}\right)^y  \\
&amp;= \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)} \left( \frac{k}{\mu+k}\right)^k \left( \frac{\mu}{\mu+k}\right)^y\\
\log f(y;\mu,k) &amp;= \log \left({\Gamma(y+k)} \right)- \log \left( \Gamma(k)\right)- \log \left({\Gamma(y+1)} \right) + k \log \left({k}\right) -k \log \left({\mu+k} \right)+ y \log \left( \frac{\mu}{\mu+k}\right) \\
&amp;= \{y \log \left( \frac{\mu}{\mu+k}\right)-k \log \left({\mu+k}\right)\}+\log \left({\Gamma(y+k)} \right)- \log \left( \Gamma(k)\right)- \log \left({\Gamma(y+1)} \right) + k \log \left({k}\right)
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\theta = \log \left( \frac{\mu}{\mu+k}\right)\)</span>. Then, setting <span class="math inline">\(a(\phi) = 1\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
c(y)&amp;=\log \left({\Gamma(y+k)} \right)- \log \left( \Gamma(k)\right)- \log \left({\Gamma(y+1)} \right) + k \log \left({k}\right) \\
b(\theta) &amp;= k \log \left({\mu+k}\right) \\
&amp;= k \log \left((\mu + k)\frac{k}{k}\right) \\
&amp;= k \log \left(\frac{k}{\frac{k}{(\mu + k)}}\right) \\
&amp;= k \log \left(\frac{k}{\frac{\mu+k - \mu}{(\mu + k)}}\right) \\
&amp;= k \log \left(\frac{k}{1-\frac{\mu}{(\mu + k)}}\right) \\
&amp;= k \log \left(\frac{k}{1-e^{\theta}}\right)
\end{aligned}\]</span></p>
<p>Because <span class="math inline">\(k\)</span> is known, I have shown this parametrization of the negative binomial distribution can be manipulated to be in Generalized Linear Model form.</p>
<p>Thus, <span class="math inline">\(E(y) = b'(\theta) = \frac{ke^{\theta}}{1-e^\theta} = \frac{\frac{k\mu}{\mu+k}}{1- \frac{\mu}{\mu+k}} = \frac{\frac{k\mu}{\mu+k}}{1- \frac{\mu}{\mu+k}}= \frac{\frac{k\mu}{\mu+k}}{\frac{k}{\mu+k}}= \mu\)</span>.</p>
<p>And, <span class="math inline">\(Var(Y)= b''(\theta)= \frac{(1-e^\theta)ke^\theta - ke^\theta(-e^\theta)}{(1-e^\theta)^2}= \frac{\left(\frac{k}{\mu+k}\right)k\left(\frac{\mu}{\mu+k}\right)+ k\left(\frac{\mu}{\mu+k}\right)^2}{\left(\frac{k}{\mu+k}\right)^2}= \mu+ \frac{\mu^2}{k}\)</span>.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-7" class="level2">
<h2 class="anchored" data-anchor-id="section-7">2.21</h2>
<p>The usual gamma density is given by <span class="math display">\[f(y;\alpha,\beta) = \frac{1}{\Gamma (\alpha)\beta^\alpha}y^{\alpha-1}e^{-y/\beta} I(0 \leq y &lt; \infty) ~~~ \alpha,\beta&gt;0\]</span> and has mean <span class="math inline">\(\alpha \beta\)</span> and variance <span class="math inline">\(\alpha \beta^2\)</span>. First reparameterize by letting <span class="math inline">\(\mu= \alpha \beta\)</span> so that the parameter vector is now <span class="math inline">\((\mu, \alpha)\)</span>. Now put this gamma family in the form of a generalized linear model, identifying <span class="math inline">\(\theta, b(\theta), a_i(\phi)\)</span>, and <span class="math inline">\(c(y,\phi)\)</span>. Note that <span class="math inline">\(\alpha\)</span> is unknown here and should be related to <span class="math inline">\(\phi\)</span> Make sure that <span class="math inline">\(b(\theta)\)</span> is actually a function of <span class="math inline">\(\theta\)</span> and take the first derivative to verify that <span class="math inline">\(b(\theta)\)</span> is correct.</p>
<p>Solution:</p>
<p><span class="math display">\[\begin{aligned}
f(y;\alpha,\beta) &amp;= \frac{1}{\Gamma (\alpha)\left(\frac{\mu}{\alpha}\right)^\alpha}y^{\alpha-1}e^{-y (\alpha/\mu)} \\
\log f(y;\alpha,\beta) &amp;= -\log \left(\Gamma (\alpha) \right) -\alpha \log (\mu) +\alpha \log(\alpha)+ (\alpha-1) \log(y) -y (\alpha/\mu) \\
&amp;= \left[y \left(\frac{-\alpha}{\mu} \right) -\alpha \log (\mu) \right]  +(\alpha-1)\log(y)+\alpha \log(\alpha)-\log \left(\Gamma (\alpha) \right)  \\
&amp;= \left[ \frac{y(-1/\mu)- \log (\mu)}{1/\alpha} \right]  +(\alpha-1)\log(y)+\alpha \log(\alpha)-\log \left(\Gamma (\alpha) \right)  \\
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\theta = -1/\mu\)</span>, <span class="math inline">\(\phi = 1/\alpha\)</span>.Then,</p>
<p><span class="math display">\[
\log f(y;\alpha,\beta) = \left[ \frac{y\theta- \log \left(-1/\theta\right)}{\phi} \right]  +\left(\frac{1}{\phi}-1\right)\log(y)+\left(\frac{1}{\phi}\right) \log\left(\frac{1}{\phi}\right)-\log \left(\Gamma \left(\frac{1}{\phi}\right) \right)
\]</span></p>
<p>Thus, <span class="math display">\[ \begin{aligned}
b(\theta) &amp;= \log(-1/\theta) \\
a(\phi) &amp;= \phi \\
c(y, \phi) &amp;= \left(\frac{1}{\phi}-1\right)\log(y)+\left(\frac{1}{\phi}\right) \log\left(\frac{1}{\phi}\right)-\log \left(\Gamma \left(\frac{1}{\phi}\right) \right)
\end{aligned}
\]</span></p>
<p>And, <span class="math inline">\(b'(\theta) = \left(\frac{1}{-1/\theta}\right)\left(\frac{-1}{\theta^2}\right) = 1/\theta = \mu\)</span>.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-8" class="level2">
<h2 class="anchored" data-anchor-id="section-8">2.22</h2>
<p>Consider the standard one-way ANOVA situation with <span class="math inline">\(Y_{ij}\)</span> distributed as <span class="math inline">\(N(\mu_i,\sigma^2)\)</span>, <span class="math inline">\(i=1,\dots,k\)</span>, <span class="math inline">\(j=1,\dots,n_i\)</span> and all the random variables are independent.</p>
<section id="a." class="level3">
<h3 class="anchored" data-anchor-id="a.">a.</h3>
<p>Form the log likelihood, take derivatives, and show that the MLEs are <span class="math inline">\(\hat \mu_i = \bar{Y_i}\)</span>, <span class="math inline">\(i=1,\dots,k\)</span>, <span class="math inline">\(\hat \sigma^2 = SSE/N\)</span>, where <span class="math inline">\(SSE= \sum_{i=1}^k \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y_i})^2\)</span> and <span class="math inline">\(N=\sum_{i=1}^k n_i\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal L (\mu_i, \sigma^2) &amp;= \prod_{i=1}^k \prod_{j=1}^{n_i} \frac{1}{\sqrt{2\pi\sigma^2}} {\exp\left(\frac{-(y_{ij}-\mu_i)^2}{2\sigma^2}\right)} \\
\ell(\mu_i, \sigma^2) &amp;= \sum_{i=1}^k \sum_{j=1}^{n_i} \frac{1}{\sqrt{2\pi\sigma^2}} {\exp\left(\frac{-(y_{ij}-\mu_i)^2}{2\sigma^2}\right)} \\
&amp;= -\frac{N}{2}\log(2\pi)-\frac{N}{2}\log(\sigma^2)-\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{(y_{ij}-\mu_i)^2}{2\sigma^2} \\
\end{aligned}
\]</span> Finding <span class="math inline">\(\hat \mu_i\)</span>, <span class="math display">\[
\begin{aligned}
0 =\frac{\partial \ell}{\partial \mu_i}&amp;=\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{2(y_{ij}-\mu_i)}{2\sigma^2} \\
&amp;=\sum_{i=1}^k \sum_{j=1}^{n_i}  y_{ij}-\mu_i \\
&amp;= \sum_{i=1}^N \bar y_i- n_i\mu_i \\
\implies \mu_i &amp;= \bar y_i \\
\frac{\partial^2 \ell}{\partial \mu_i^2} \bigg|_{\mu_i= \bar{Y_i}}&amp;=\sum_{i=1}^k \sum_{j=1}^{n_i}  -2 &lt;0 \\
\implies \hat \mu_i &amp;= \bar y_i
\end{aligned}
\]</span></p>
<p>Finding <span class="math inline">\(\hat \sigma^2\)</span>, <span class="math display">\[
\begin{aligned}
0 =\frac{\partial \ell}{\partial \sigma^2} \bigg|_{\mu_i = \bar y_i}&amp;=-\frac{N}{2\sigma^2}+\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{(y_{ij}-\bar y_i)^2}{2(\sigma^2)^2} \\
&amp;=-N\sigma^2+\sum_{i=1}^k \sum_{j=1}^{n_i}(y_{ij}-\bar y_i)^2 \\
\implies \sigma^2&amp;=\frac{\sum_{i=1}^k \sum_{j=1}^{n_i}(y_{ij}-\bar y_i)^2}{N} \\
&amp;= SSE/N \\
\frac{\partial^2 \ell}{\partial \left(\sigma^2 \right)^2} \bigg|_{\mu_i = \bar y_i, \sigma^2 = SSE/N}&amp;=\frac{N}{2(\sigma^2)^2}-2\sum_{i=1}^k \sum_{j=1}^{n_i}  \frac{(y_{ij}-\bar y_i)^2}{2(\sigma^2)^3}  \bigg|_{\sigma^2 = SSE/N} \\
&amp;=\frac{N}{2(SSE/N)^2}-\frac{SSE}{(SSE/N)^3} \\
&amp;=\frac{N}{2(SSE/N)^2}-\frac{N}{(SSE/N)^2} &lt;0. \\
\implies \hat \sigma^2 &amp;= SSE/N
\end{aligned}
\]</span> </p>
</section>
<section id="b." class="level3">
<h3 class="anchored" data-anchor-id="b.">b.</h3>
<p>Now define <span class="math inline">\(\mathbf V_i^T = (Y_{i1} -\bar {Y_i}, \dots, Y_{i,n_i-1}- \bar{Y_i})\)</span>. Using standard matrix manipulations with the multivariate normal distribution, the density of <span class="math inline">\(\mathbf V_i\)</span> is given by <span class="math display">\[(2 \pi)^{-(n_i-1)/2}n_i^{1/2}\sigma^{-(n_i-1)} \exp \left(- \frac{1}{2 \sigma ^2} \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \right) \]</span> where <span class="math inline">\(I_{n_i-1}\)</span> is the <span class="math inline">\(n_i-1\)</span> by <span class="math inline">\(n_i-1\)</span> identity matrix and <span class="math inline">\(J_{n_i-1}\)</span> is an <span class="math inline">\(n_i-1\)</span> by <span class="math inline">\(n_i-1\)</span> matrix of 1’s. Now form the (marginal) likelihood based on <span class="math inline">\(\mathbf V_1, \dots, \mathbf V_k\)</span> and show that the MLE for <span class="math inline">\(\sigma^2\)</span> is now <span class="math inline">\(\sigma^2 = SSE/(N-k)\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal L (\sigma^2; \mathbf V_1,\dots \mathbf V_k) &amp;= \prod_{i=1}^k(2 \pi)^{-(n_i-1)/2}n_i^{1/2}(\sigma^2)^{-(n_i-1)/2} \exp \left(- \frac{1}{2 \sigma ^2} \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \right) \\
\ell (\sigma^2; \mathbf V_1,\dots \mathbf V_k) &amp;= \sum_{i=1}^k\frac{-(n_i-1)\log(2 \pi)+n_i}{2}-\frac{(n_i-1)}{2}\log(\sigma^2) - \frac{1}{2 \sigma ^2} \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
0=\frac{d \ell}{d \sigma^2}&amp;=\sum_{i=1}^k \frac{-(n_i-1)}{\sigma^2}+\frac{1}{2 (\sigma^2)^2}  \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
&amp;=\sum_{i=1}^k -2\sigma^2(n_i-1)+ \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
&amp;=-2\sigma^2(N-k)+\sum_{i=1}^k\sum_{j=1}^{n_i} 2(y_{ij} - \bar y_i)^2\\
\implies  \sigma^2 &amp;= SSE/(N-k)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2 = SSE/(N-k)}&amp;=\sum_{i=1}^k \frac{(n_i-1)}{(\sigma^2)^2}-\frac{1}{(\sigma^2)^3}  \mathbf v_i^t [I_{n_i-1} + J_{n_i -1}] \mathbf v_i \\
&amp;=\sum_{i=1}^k \frac{(n_i-1)}{(\sigma^2)^2}-\frac{1}{(\sigma^2)^3} \sum_{i=1}^k\sum_{j=1}^{n_i} 2(y_{ij} - \bar y_i)^2 \\
&amp;=\frac{N-k}{(SSE/(N-k))^2}-\frac{1}{(SSE/(N-k))^3}\sum_{i=1}^k\sum_{j=1}^{n_i} 2(y_{ij} - \bar y_i)^2\\
&amp;=\frac{N-k}{(SSE/(N-k))^2}-2\frac{(N-k)}{(SSE/(N-k))^2}\\
&amp;=-1 \\
\implies  \hat \sigma^2 &amp;= SSE/(N-k)
\end{aligned}
\]</span></p>
</section>
<section id="c." class="level3">
<h3 class="anchored" data-anchor-id="c.">c.</h3>
<p>Finally, let us take a more general approach and assume that <span class="math inline">\(Y\)</span> has an <span class="math inline">\(N\)</span> dimensional multivariate normal distribution with mean <span class="math inline">\(X \mathbf{\beta}\)</span> and covariance matrix <span class="math inline">\(\Sigma = \sigma^2 Q(\boldsymbol{\theta})\)</span>, where X is an <span class="math inline">\(N\times p\)</span> full rank matrix of known constants, <span class="math inline">\(\boldsymbol \beta\)</span> is a <span class="math inline">\(p\)</span>-vector of regression parameters, and <span class="math inline">\(Q(\boldsymbol{\theta})\)</span> is an <span class="math inline">\(N\times N\)</span> standardized covariance matrix depending on the unknown parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>. Typically <span class="math inline">\(\boldsymbol{\theta}\)</span> would consist of variance component and/or spatial correlation parameters. We can concentrate the likelihood by noting that if <span class="math inline">\(Q(\boldsymbol{\theta})\)</span> were known, then the generalized least squares estimator would be <span class="math inline">\(\boldsymbol{\hat \beta(\theta)} =(X^T Q(\boldsymbol{\theta})^{-1}X)^{-1}X^TQ(\boldsymbol{\theta})^{-1} \mathbf Y\)</span>. Substituting for <span class="math inline">\(\boldsymbol \beta\)</span> yields the profile log likelihood <span class="math display">\[-\frac{N}{2} \log(2\pi) - N \log \sigma- \frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2\sigma^2},\]</span> where <span class="math inline">\(GSSE(\boldsymbol{\theta}) =(\mathbf Y - X \boldsymbol{\hat{\beta}}(\boldsymbol{\theta}))^T Q(\boldsymbol{\theta})^{-1}(\mathbf Y -X\boldsymbol{\hat \beta}(\boldsymbol{\theta}))\)</span>. To connect with part a), let <span class="math inline">\(Q(\boldsymbol{\theta})\)</span> be the identity matrix (so that <span class="math inline">\(GSSE(\boldsymbol{\theta})\)</span> is just <span class="math inline">\(SSE\)</span>) and find the maximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\ell(\sigma^2 | Y) &amp;= -\frac{N}{2} \log(2\pi) - \frac{N}{2} \log \sigma^2- \frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2\sigma^2} \\
0=\frac{\partial \ell}{\partial \sigma^2} &amp;= - \frac{N}{2 \sigma^2}+ \frac{GSSE(\boldsymbol{\theta})}{2(\sigma^2)^2}\\
&amp;=-N\sigma^2 +GSSE(\boldsymbol \theta) \\
\implies \sigma^2 &amp;= GSSE(\boldsymbol \theta)/N \\
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/N}&amp;= \frac{N}{2 (\sigma^2)^2}-\frac{GSSE(\boldsymbol{\theta})}{(\sigma^2)^3}\bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/N}\\
&amp;= \frac{N}{2 (GSSE(\boldsymbol \theta)/N)^2}-\frac{GSSE(\boldsymbol{\theta})}{(GSSE(\boldsymbol \theta)/N)^3} \\
&amp;= -1/2 \\
\implies \hat \sigma^2 &amp;= GSSE(\boldsymbol \theta)/N \\
\end{aligned}
\]</span> </p>
</section>
<section id="d." class="level3">
<h3 class="anchored" data-anchor-id="d.">d.</h3>
<p>Continuing part c), the REML approach is to transform to <span class="math inline">\(\mathbf V = A^T \mathbf Y\)</span>, where the <span class="math inline">\(N \times p\)</span> columns of <span class="math inline">\(A\)</span> are linearly independent and <span class="math inline">\(A^T X =0\)</span> so that <span class="math inline">\(\mathbf V\)</span> is <span class="math inline">\(MN(0;A^T \Sigma A)\)</span> A special choice of <span class="math inline">\(A\)</span> leads to the REML log likelihood <span class="math display">\[-\frac{N-p}{2}\log(2\pi) -(N-p)\log \sigma - \frac{1}{2} \log |X^T Q(\boldsymbol{\theta} )^{-1}X|-\frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2 \sigma^2}.\]</span> To connect with part b), let <span class="math inline">\(Q(\boldsymbol{\theta})\)</span> be the identity matrix and find the maximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[\ell(\sigma^2 | Y) = -\frac{N-p}{2}\log(2\pi) -\frac{(N-p)}{2}\log (\sigma^2) - \frac{1}{2} \log |X^T Q(\boldsymbol{\theta} )^{-1}X|-\frac{1}{2} \log |Q(\boldsymbol{\theta})| - \frac{GSSE(\boldsymbol{\theta})}{2 \sigma^2}\]</span> <span class="math display">\[
\begin{aligned}
0=\frac{\partial \ell}{\partial \sigma^2} &amp;= - \frac{N-p}{2 \sigma^2}+ \frac{GSSE(\boldsymbol{\theta})}{2(\sigma^2)^2}\\
&amp;=-(N-p)\sigma^2 +GSSE(\boldsymbol \theta) \\
\implies \sigma^2 &amp;= GSSE(\boldsymbol \theta)/(N-p) \\
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/(N-p)}&amp;= \frac{N}{2 (\sigma^2)^2}-\frac{GSSE(\boldsymbol{\theta})}{(\sigma^2)^3}\bigg|_{\sigma^2=GSSE(\boldsymbol \theta)/(N-p)}\\
&amp;= \frac{N-p}{2 (GSSE(\boldsymbol \theta)/(N-p))^2}-\frac{GSSE(\boldsymbol{\theta})}{(GSSE(\boldsymbol \theta)/(N-p))^3} \\
&amp;= -1/2 \\
\implies \hat \sigma^2 &amp;= GSSE(\boldsymbol \theta)/(N-p) \\
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-9" class="level2">
<h2 class="anchored" data-anchor-id="section-9">2.25</h2>
<p>If <span class="math inline">\(\mathbf Y\)</span> is from an exponential family where <span class="math inline">\((\mathbf W, \mathbf V)\)</span> are jointly sufficient for <span class="math inline">\((\boldsymbol \theta_1, \boldsymbol \theta_2)\)</span>, then the conditional density of <span class="math inline">\(\mathbf W | \mathbf V\)</span> is free of the nuisance parameter <span class="math inline">\(\boldsymbol \theta_2\)</span> and can be used as a conditional likelihood for estimating <span class="math inline">\(\boldsymbol \theta_1\)</span>. In some cases it may be difficult to find the conditional density. However, from (2.19, p.&nbsp;57) we have <span class="math display">\[\frac{f_{\mathbf Y}(\mathbf y; \boldsymbol \theta_1, \boldsymbol \theta_2)}{f_{\mathbf V}(\mathbf y; \boldsymbol \theta_1, \boldsymbol \theta_2)}= f_{\mathbf{W|V}}(\mathbf{w|v}; \boldsymbol \theta_1).\]</span> Thus, if you know the density of <span class="math inline">\(\mathbf Y\)</span> and of <span class="math inline">\(\mathbf V\)</span>, then you can get a conditional likelihood equation.</p>
<section id="a.-1" class="level3">
<h3 class="anchored" data-anchor-id="a.-1">a.</h3>
<p>Now let <span class="math inline">\(\mathbf Y_1, \dots, \mathbf Y_n\)</span> be iid <span class="math inline">\(N(\mu,\sigma^2)\)</span>, <span class="math inline">\(V=\bar Y\)</span>, and <span class="math inline">\(\boldsymbol \theta = (\sigma, \mu)^T\)</span>. Form the ratio above and note that it is free of <span class="math inline">\(\mu\)</span>. (It helps to remember that <span class="math inline">\(\sum (Y_i-\mu)^2= \sum (Y_i- \bar Y)^2 + n(\bar Y - \mu)^2\)</span>.)</p>
<p>Solution:</p>
<p>By Central Limit Theorem, <span class="math inline">\(\mathbf V = \bar Y \sim N(\mu, \sigma^2/n)\)</span>. Thus, <span class="math display">\[ \begin{aligned}
\frac{f_{\mathbf Y}(\mathbf y; \boldsymbol \sigma, \boldsymbol \mu)}{f_{\mathbf V}(\mathbf y; \boldsymbol \sigma, \boldsymbol \mu)} &amp;= \frac{\prod_{i=1}^n(2\pi)^{-1/2}\sigma^{-1}\exp(-(y_i- \mu)^2/2\sigma^2)}{(2\pi)^{-1/2}\sigma^{-1}n^{1/2}\exp(-n(\bar y- \mu)^2/2\sigma^2)} \\
&amp;= \frac{(2\pi)^{-n/2}\sigma^{-n}\exp(-\sum_{i=1}^n(y_i- \mu)^2/2\sigma^2)}{(2\pi)^{-1/2}\sigma^{-1}n^{1/2}\exp(-n(\bar y- \mu)^2/2\sigma^2)} \\
&amp;= \frac{(2\pi)^{-n/2}\sigma^{-n}\exp(-\sum_{i=1}^n[(y_i- \bar y)^2 + n(\bar y - \mu)^2]/2\sigma^2)}{(2\pi)^{-1/2}\sigma^{-1}n^{1/2}\exp(-n(\bar y- \mu)^2/2\sigma^2)} \\
&amp;=(2\pi)^{-(n-1)/2}\sigma^{-(n-1)}n^{-1/2}\exp\left({-\sum_{i=1}^n (Y_i- \bar Y)^2}/{2\sigma^2}\right) &amp;\text{This is free of}~\mu.
\end{aligned}\]</span></p>
</section>
<section id="b.-1" class="level3">
<h3 class="anchored" data-anchor-id="b.-1">b.</h3>
<p>Find the conditional maximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal L (\sigma^2| \mathbf Y)&amp;=(2\pi)^{-(n-1)/2}(\sigma^2)^{-(n-1)/2}n^{-1/2}\exp\left({-\sum_{i=1}^n (Y_i- \bar Y)^2}/{2\sigma^2}\right) \\
\ell (\sigma^2| \mathbf Y)&amp;=\frac{-(n-1)}{2}\log(2\pi)-\frac{n-1}{2}\log(\sigma^2) -\frac{\log n}{2} -\frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{2\sigma^2} \\
0 = \frac{\partial \ell}{\partial \sigma^2}&amp;=-\frac{n-1}{2(\sigma^2)} +\frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{2(\sigma^2)^2}\\
\implies \sigma^2 &amp;= \frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{n-1}= SSE/(n-1) \\
\frac{\partial^2 \ell}{\partial (\sigma^2)^2} \bigg|_{\sigma^2 = SSE/(n-1)}&amp;=\frac{n-1}{2(\sigma^2)^2} -\frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{(\sigma^2)^3} \bigg|_{\sigma^2 = SSE/(n-1)} \\
&amp;=\frac{n-1}{2(SSE/(n-1))^2} -\frac{SSE}{(SSE/(n-1))^3}\\
&amp;= -1/2 &lt;0. \\
\implies \hat \sigma^2 &amp;= \frac{\sum_{i=1}^n (Y_i- \bar Y)^2}{n-1}= SSE/(n-1)
\end{aligned}\]</span></p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-10" class="level2">
<h2 class="anchored" data-anchor-id="section-10">2.26</h2>
<p>Consider the normal theory linear measurement error model <span class="math display">\[\begin{aligned} Y_i = \alpha + \beta U_i +\sigma_e e_i, &amp; X_i =U_i+ \sigma Z_i, &amp; i = 1,\dots,n \end{aligned}\]</span> where <span class="math inline">\(e_1, \dots, e_n\)</span>, <span class="math inline">\(Z_1,\dots, Z_n\)</span> are iid <span class="math inline">\(N(0,1)\)</span> random variables, <span class="math inline">\(\sigma^2\)</span> is known, and <span class="math inline">\(\alpha,\beta, \sigma_e\)</span>, and <span class="math inline">\(U_1, \dots, U_n\)</span> are unknown parameters.</p>
<section id="a.-2" class="level3">
<h3 class="anchored" data-anchor-id="a.-2">a.</h3>
<p>Let <span class="math inline">\(s_X^2\)</span> denote the sample variance of <span class="math inline">\(X_1,\dots, X_n\)</span>. Show that <span class="math inline">\(E(s_X^2)=s_U^2+\sigma^2\)</span> where <span class="math inline">\(s_U^2\)</span> is the sample variance of <span class="math inline">\(\{U_i\}_1^n\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\bar X = \sum_{i=1}^n (U_i+\sigma Z_i)/n = \bar U + \sigma \bar Z, \bar Z \sim N(0,1/n)
\]</span> <span class="math display">\[\begin{aligned}
E(s_X^2) &amp;= E \left(\frac{\sum_{i=1}^n (X_i -\bar X)^2}{n-1} \right).
= E \left(\frac{\sum_{i=1}^n X_i^2 -2X_i\bar X + \bar X^2}{n-1} \right)\\
&amp;= E \left(\frac{\sum_{i=1}^n (U_i + \sigma Z_i)^2 -2(U_i + \sigma Z_i)(\bar U + \sigma \bar Z) +(\bar U + \sigma \bar Z)^2}{n-1} \right)\\
&amp;= E \left(\frac{\sum_{i=1}^n U_i^2 + 2\sigma  U_i Z_i+ \sigma^2 Z_i^2 -2(U_i \bar U + \sigma U_i \bar Z +\sigma Z_i \bar U + \sigma^2Z_i \bar Z) +\bar U^2 + 2\sigma \bar U \bar Z + \sigma^2\bar Z^2}{n-1} \right)\\
&amp;= E \left(\frac{\sum_{i=1}^n U_i^2-2U_i \bar U+\bar U^2 + \sigma^2 Z_i^2  -2 \sigma^2Z_i \bar Z  + \sigma^2\bar Z^2}{n-1} \right)\\
&amp;= E \left(\frac{\sum_{i=1}^n(U_i- \bar U)^2}{n-1} \right) + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\\
&amp;= s_U^2 + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\\
&amp;= s_U^2 + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\\
\end{aligned}
\]</span> Note that, <span class="math display">\[ \begin{aligned}
(n-1) \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\sim \chi^2(n-1) \\
\implies E \left[(n-1) \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right)\right] &amp;= n-1 \\
\implies E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right) &amp;= 1
\end{aligned}
\]</span> Therefore, <span class="math inline">\(E(s_X^2)= s_U^2 + \sigma^2 E \left(\frac{\sum_{i=1}^n(Z_i- \bar Z)^2}{n-1} \right) =s_U^2 + \sigma^2\)</span>.</p>
<p><strong>For the remainder of this problem assume that <span class="math inline">\(s_U^2 \to \sigma_U^2\)</span> as <span class="math inline">\(n \to \infty\)</span> and that <span class="math inline">\(s_X^2\)</span> converges in probability to <span class="math inline">\(\sigma_U^2\)</span>. (Note that even though <span class="math inline">\(\{U_i\}_1^n\)</span> are parameters it still makes sense to talk about their sample variance, and denoting the limit of <span class="math inline">\(s_U^2\)</span> as <span class="math inline">\(n \to \infty\)</span> by <span class="math inline">\(\sigma_U^2\)</span> is simply a matter of convenience).</strong></p>
</section>
<section id="b.-2" class="level3">
<h3 class="anchored" data-anchor-id="b.-2">b.</h3>
<p>Show that the estimate of slope from the least squares regression of <span class="math inline">\(\{Y_i\}_1^n\)</span> on <span class="math inline">\(\{X_i\}_1^n\)</span> (call it <span class="math inline">\(\hat{\beta}_{Y|X}\)</span> is not consistent for <span class="math inline">\(\beta\)</span> as <span class="math inline">\(n \to \infty\)</span>). This shows that it is not OK to simply ignore the measurement error in the predictor variable.</p>
<p>Solution:</p>
<p>It is a well-known result that in SLR <span class="math inline">\(\hat{\beta}_{Y|X}= \frac{\widehat{Cov(X,Y)}}{s^2_X}\)</span>. Thus, <span class="math display">\[ \begin{aligned}
\lim_{n \to \infty}\hat{\beta}_{Y|X} &amp;= \lim_{n \to \infty} \frac{Cov(X,Y)}{s^2_X} \\
&amp;= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \frac{\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{n-1} \\
&amp;= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \frac{\sum_{i=1}^nX_iY_i-X_i\bar Y-\bar XY_i+\bar X\bar Y}{n-1} \\
X_iY_i &amp;= (U_i + \sigma Z_i)(\alpha + \beta U_i +\sigma_e e_i) \\
&amp;= \alpha U_i + \beta U_i^2 +\sigma_e U_ie_i +\alpha \sigma Z_i+ \beta \sigma U_i Z_i +\sigma_e\sigma e_i Z_i \\
X_i\bar Y &amp;= (U_i + \sigma Z_i)(\alpha + \beta \bar U +\sigma_e \bar e) \\
&amp;= \alpha U_i + \beta U_i \bar U +\sigma_e U_i \bar e +\alpha \sigma Z_i+ \beta \sigma \bar U Z_i +\sigma_e\sigma \bar e Z_i \\
\bar XY_i &amp;= (\bar U + \sigma \bar Z)(\alpha + \beta U_i +\sigma_e e_i) \\
&amp;= \alpha \bar U + \beta U_i \bar U +\sigma_e \bar Ue_i +\alpha \sigma \bar Z+ \beta \sigma U_i \bar Z +\sigma_e\sigma e_i \bar Z \\
\bar X \bar Y &amp;= (\bar U + \sigma \bar Z)(\alpha + \beta \bar U +\sigma_e \bar e) \\
&amp;= \alpha \bar U + \beta \bar U^2 +\sigma_e \bar U \bar e +\alpha \sigma \bar Z+ \beta \sigma \bar U \bar Z +\sigma_e\sigma \bar e \bar Z \\
\implies \lim_{n \to \infty}\hat{\beta}_{Y|X} &amp;= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \frac{\sum_{i=1}^n \beta (U_i^2-2 U_i\bar U +\bar U^2)+\sigma_e(U_i e_i -U_i \bar e -\bar U e_i + \bar U \bar e)}{n-1} \\
&amp;~~~~~+\frac{\sum_{i=1}^n \beta \sigma (U_iZ_i- \bar U Z_i - U_i \bar Z +\bar U \bar Z)+\sigma_e \sigma(e_i Z_i -\bar e Z_i - e_i\bar Z + \bar e\bar Z )}{n-1}  \\
&amp;= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \left[ \beta s_U^2+\frac{\sum_{i=1}^n \sigma_e \sigma(e_i Z_i -\bar e Z_i - e_i\bar Z + \bar e\bar Z )}{n-1} \right] ~~~~~~ \left( \overset{\text{Central Limit Theorem,}}{e_i,Z_i, \sqrt n \bar e, \sqrt n \bar Z \sim N(0,1)} \right) \\
&amp;= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \left[ \beta s_U^2+\frac{\sigma_e \sigma \left[\sum_{i=1}^n (e_i - \bar e) (Z_i- \bar Z)\right]}{n-1} \right]\\
&amp;= \frac{1}{\sigma_U^2+ \sigma^2} \lim_{n \to \infty} \left[ \beta s_U^2+\sigma_e \sigma Cov(e_i,Z_i) \right] \\
&amp;= \frac{1}{\sigma_U^2+ \sigma^2}(\beta s_U^2)~~~~~~~~~~~~~~~~~\bigg(e_i,Z_i \overset{iid}{\sim} N(0,1) \implies Cov(e_i,Z_i) =0 \bigg) \\
&amp;= \frac{\beta s_U^2}{\sigma_U^2+ \sigma^2}
\end{aligned}
\]</span> Therefore <span class="math inline">\(\lim_{n \to \infty}\hat{\beta}_{Y|X}\)</span> is not a consistent estimator of <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="c.-1" class="level3">
<h3 class="anchored" data-anchor-id="c.-1">c.&nbsp;</h3>
<p>Now construct the full likelihood for <span class="math inline">\(\alpha,\beta,\sigma_{\epsilon}^2, U_1,\dots,U_n\)</span> and show that it has no sensible maximum. Do this by showing that the full likelihood diverges to <span class="math inline">\(\infty\)</span> when <span class="math inline">\(U_i =(Y_i-\alpha)/\beta\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(\sigma_{\epsilon}^2 \to 0\)</span>. This is another well-known example of the failure of maximum likelihood to produce meaningful estimates.</p>
<p>Solution:</p>
<p><span class="math display">\[ \begin{aligned}
Y_i &amp;= \alpha + \beta U_i +\sigma_e e_i \sim N(\alpha + \beta U_i, \sigma_{e}^2) \\
\implies \mathcal L(\alpha, \beta, \mathbf U, \sigma_e | \mathbf Y) &amp;= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_{e}^2}}\exp \left(\frac{-(y_i -(\alpha +\beta U_i))^2}{2\sigma_{e}^2}\right) \\
\lim_{\sigma^2_{e} \to 0}L(\alpha, \beta, \mathbf U, \sigma_e | \mathbf Y)|_{U_i =(Y_i-\alpha)/\beta} &amp;=\lim_{\sigma^2_{e} \to 0} \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_{e}^2}}\exp \left(\frac{-(y_i -(\alpha +\beta ((Y_i-\alpha)/\beta)))^2}{2\sigma_{e}^2}\right) \\
&amp;=\lim_{\sigma^2_{e} \to 0}\prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma_{e}^2}} = \lim_{\sigma^2_{e} \to 0} (2 \pi \sigma_{e}^2)^{-n/2} \\
&amp;= \infty
\end{aligned} \]</span></p>
</section>
<section id="d.-1" class="level3">
<h3 class="anchored" data-anchor-id="d.-1">d.</h3>
<p>Consider the simple estimator (of <span class="math inline">\(\beta\)</span>) <span class="math display">\[\hat \beta_{MOM} = \frac{s_X^2}{s_X^2-\sigma^2} \hat \beta _{Y|X},\]</span> and show that it is a consistent estimator of <span class="math inline">\(\beta\)</span>. This shows that consistent estimators exist, and thus the problem with maximum likelihood is not intrinsic to the model.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\lim_{n \to \infty}\hat \beta_{MOM} &amp;= \lim_{n \to \infty}\frac{s_X^2}{s_X^2-\sigma^2} \hat \beta _{Y|X} \\
&amp;=\frac{\sigma_U^2 + \sigma^2}{\sigma_U^2} \left(\frac{\beta \sigma_U^2}{\sigma_U^2+ \sigma^2}\right) \\
&amp;=\beta
\end{aligned}
\]</span> </p>
</section>
<section id="e." class="level3">
<h3 class="anchored" data-anchor-id="e.">e.</h3>
<p>Assuming that all other parameters are known, show that <span class="math inline">\(T_i = Y_i \beta/\sigma_{e}^2+X_i/\sigma^2\)</span> is a sufficient statistic for <span class="math inline">\(U_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
X_i  &amp;\sim N(U_i, \sigma^2)\\
Y_i &amp;  \sim N(\alpha + \beta U_i, \sigma^2_e) \\
\end{aligned}
\]</span> <span class="math display">\[
\begin{aligned}
f_{X_i,Y_i|U_i}(x,y)&amp;=f_{Y_i|U_i}(y_i|u_i) f_{X_i|U_i}(x_i) \\
&amp;=\frac{1}{\sqrt{2 \pi \sigma_{e}^2}}\exp \left(\frac{-(y_i -(\alpha +\beta u_i))^2}{2 \sigma_{e}^2}\right) \frac{1}{\sqrt{2 \pi \sigma^2}}\exp \left(\frac{-(x_i- u_i)^2}{2 \sigma^2}\right)   \\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left(\frac{-(y_i^2 -2y_i\alpha - 2y_i \beta u_i +\alpha^2 +2\alpha \beta u_i + \beta^2u_i^2)}{2 \sigma_{e}^2}-\frac{x_i^2-2x_iu_i+ u_i^2}{2 \sigma^2}\right)   \\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left(\frac{-\sigma^2((y_i-\alpha)^2 - 2y_i \beta u_i +2\alpha \beta u_i + \beta^2u_i^2)-\sigma_e^2(x_i^2-2x_iu_i+ u_i^2)}{2 \sigma^2\sigma_e^2}\right)   \\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left(\frac{(2\sigma^2 y_i\beta +2\sigma_e^2x_i)u_i}{2\sigma^2\sigma^2_e}\right)\exp \left(\frac{-\sigma^2((y_i-\alpha)^2 +2\alpha \beta u_i + \beta^2u_i^2)-\sigma_e^2(x_i^2+ u_i^2)}{2 \sigma^2\sigma_e^2}\right)   \\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}\right)\right]\exp \left(\frac{-(\sigma^2(y_i-\alpha)^2 +2\sigma^2\alpha \beta u_i + \sigma^2\beta^2u_i^2+ \sigma_e^2x_i^2+\sigma_e^2u_i^2)}{2 \sigma^2\sigma_e^2}\right)   \\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}\right)\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2x_i^2)}{2 \sigma^2\sigma_e^2} -\left(\frac{\alpha \beta}{\sigma_e^2}u_i + \frac{\beta^2}{2\sigma_e^2}u_i^2+\frac{1}{2\sigma^2}u_i^2\right)\right]   \\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2x_i^2)}{2 \sigma^2\sigma_e^2}\right]\\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(t_i-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2x_i^2)}{2 \sigma^2\sigma_e^2}\right]\\
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\left(\frac{y_i\beta}{\sigma_e^2}+\frac{ x_i}{\sigma^2}\right)\)</span> is sufficient for <span class="math inline">\(U_i\)</span>.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="f." class="level3">
<h3 class="anchored" data-anchor-id="f.">f.</h3>
<p>Find the conditional distribution of <span class="math inline">\(Y_i|T_i\)</span> and use it to construct a conditional likelihood for <span class="math inline">\(\alpha, \beta\)</span>, and <span class="math inline">\(\sigma_{\epsilon}^2\)</span> in a manner similar to that for the logistic regression measurement error model.</p>
<p>Solution:</p>
<p><span class="math display">\[ \begin{aligned}
T_i &amp;= Y_i \beta/\sigma_{e}^2+X_i/\sigma^2 \implies X_i= (T_i-Y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2} \right)
\end{aligned}
\]</span> By the Jacobian method of transformations: <span class="math display">\[ \begin{aligned}
f_{Y_i,T_i} (y_i,t_i) &amp;= f_{X_i,Y_i}\left(y_i,(t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)\right)\left| \begin{matrix} 0&amp;1/\sigma^2 \\ 1 &amp;  \beta/\sigma_e^2\end{matrix} \right| \\
&amp;=\frac{1}{2 \pi \sigma_e \sigma}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{(t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)}{\sigma^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right] \\
&amp;~~~~~~\times\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2\left((t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)\right)^2)}{2 \sigma^2\sigma_e^2}\right] \left|\frac{-1}{\sigma^2}\right|\\
\implies f_{Y_i|T_i=t_i}(y_i,t_i) &amp;= \frac{f_{Y_i, T_i}(y_i,t_i)}{f_{T_i}(t_i)}
\end{aligned}
\]</span> <span class="math display">\[
\begin{aligned}
&amp;=\frac{\frac{1}{2 \pi \sigma_e \sigma^3}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{(t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)}{\sigma^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2\right]\exp \left[\frac{-(\sigma^2(y_i-\alpha)^2+ \sigma_e^2\left((t_i-y_i\beta)\left(\frac{\sigma^2}{\sigma_e^2}\right)\right)^2)}{2 \sigma^2\sigma_e^2}\right] }
{\frac{1}{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}} \exp \left[\frac{-(t_i-(\sigma_e^{-2}\beta \alpha+ \sigma_e^{-2}\beta^2 u_i+\sigma^{-2}u_i))^2}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&amp;=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\exp \left[ u_i\left(\frac{y_i\beta}{\sigma_e^2}+\frac{t_i-y_i\beta}{\sigma_e^2}-\frac{\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2-\frac{(y_i-\alpha)^2+ (t_i-y_i\beta)^2\left(\frac{\sigma^2}{\sigma_e^2}\right)}{2 \sigma_e^2}\right] }
{2 \pi \sigma_e \sigma^3 \exp \left[\frac{-(t_i-(\sigma_e^{-2}\beta \alpha+ \sigma_e^{-2}\beta^2 u_i+\sigma^{-2}u_i))^2}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&amp;=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\exp \left[ u_i\left(\frac{t_i-\alpha \beta}{\sigma_e^2}\right) -\left(\frac{\beta^2}{2\sigma_e^2}+\frac{1}{2\sigma^2}\right)u_i^2-\frac{(y_i-\alpha)^2+ (t_i-y_i\beta)^2\left(\frac{\sigma^2}{\sigma_e^2}\right)}{2 \sigma_e^2}\right] }
{2 \pi \sigma_e \sigma^3 \exp \left[\frac{-(t_i^2-2t_i\sigma_e^{-2}\beta \alpha-2t_i \sigma_e^{-2}\beta^2 u_i-2t_i\sigma^{-2}u_i+\sigma_e^{-4}\beta^2\alpha^2 +2\sigma_e^{-4}\beta^3\alpha u_i +2 \sigma_e^{-2}\sigma^{-2}\alpha\beta u_i+\sigma_e^{-4}\beta^4u_i^2 + 2\sigma_e^{-2}\sigma^{-2}\beta^2u_i^2 + \sigma^{-4}u_i^2)}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&amp;=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\exp \left[ \frac{2u_i (\beta^2 \sigma_e^{-2}+ \sigma^{-2})(t_i-\alpha\beta)\sigma_e^{-2} -(\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left(\beta^2\sigma_e^{-2}+\sigma^{-2}\right)u_i^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2} }{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] }
{2 \pi \sigma_e \sigma^3 \exp \left[\frac{-\left(t_i^2-2t_i\sigma_e^{-2}\beta \alpha-2t_i \sigma_e^{-2}\beta^2 u_i-2t_i\sigma^{-2}u_i+\sigma_e^{-4}\beta^2\alpha^2 +2\sigma_e^{-4}\beta^3\alpha u_i +2 \sigma_e^{-2}\sigma^{-2}\alpha\beta u_i+\sigma_e^{-4}\beta^4u_i^2 + 2\sigma_e^{-2}\sigma^{-2}\beta^2u_i^2 + \sigma^{-4}u_i^2\right)}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]} \\
&amp;=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\\
&amp;~~~~~~~\times\exp \left[ \frac{2(\beta^2 \sigma_e^{-2}+ \sigma^{-2})(t_i-\alpha\beta)\sigma_e^{-2} u_i-2t_i \sigma_e^{-2}\beta^2 u_i-2t_i\sigma^{-2}u_i+2\sigma_e^{-4}\beta^3\alpha u_i +2 \sigma_e^{-2}\sigma^{-2}\alpha\beta u_i}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
&amp;~~~~~~~\times \exp \left[ \frac{-(\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left(\beta^2\sigma_e^{-2}+\sigma^{-2}\right)u_i^2+\sigma_e^{-4}\beta^4u_i^2 + 2\sigma_e^{-2}\sigma^{-2}\beta^2u_i^2 + \sigma^{-4}u_i^2}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
&amp;~~~~~~\times\exp \left[\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
&amp;=\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\exp \left[\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right] \\
\end{aligned}
\]</span> Therefore, the conditional likelihood is: <span class="math display">\[\mathcal L =\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\exp \left[\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\right]\]</span></p>
<p>And the MLEs based on this are: <span class="math display">\[ \begin{aligned}
\ell (\alpha, \beta, \sigma_e^2, \sigma^2 ; Y_i, T_i) &amp;= \log \left(\frac{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}{2 \pi \sigma_e \sigma^3}\right)+\frac{t_i^2-2t_i\sigma_e^{-2}\beta \alpha+\sigma_e^{-4}\beta^2\alpha^2- (\beta^2 \sigma_e^{-2}+ \sigma^{-2})\left((y_i-\alpha)^2+ (t_i-y_i\beta)^2\sigma^2\sigma_e^{-2}\right)\sigma_e^{-2}}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})} \\
0 = \frac{\partial \ell}{\partial \alpha} &amp;= \frac{2t_i\sigma_e^{-2}\beta +2\sigma_e^{-4}\beta^2\alpha-2(\beta^2 \sigma_e^{-2}+ \sigma^{-2})(y_i-\alpha)}{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}\\
&amp;= t_i\sigma_e^{-2}\beta -\beta^2 \sigma_e^{-2}y_i- \sigma^{-2}y_i +\beta^2 \sigma_e^{-2}\alpha+\sigma^{-2}\alpha\\
\implies \alpha &amp;= \frac{t_i\sigma_e^{-2}\beta-\beta^2 \sigma_e^{-2}y_i- \sigma^{-2}y_i}{\sigma_e^{-4}\beta^2-\beta^2 \sigma_e^{-2}-\sigma^{-2}}\\
0 = \frac{\partial \ell}{\partial \beta} &amp;=  \left(\frac{1}{\sqrt{2\pi (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}}\right)\left(\frac{{2 (\beta^2 \sigma_e^{-2}+ \sigma^{-2})}[-2t_i\sigma_2^{-2}\alpha+2\sigma_e^{-4}\alpha^2\beta-2\beta\sigma_e^{-4}[(y_i-\alpha)^2-(t_i-y_i\beta)^2\sigma^2\sigma_e^2]+2y_i(t_i-y_i\beta)\sigma^2\sigma_e^{-2}(\beta^2\sigma_e^{-2}+\sigma^{-2})]}{4(\beta^2\sigma_e^{-2}+\sigma^{-2})^2}\right)\\
&amp;=  -2t_i\sigma_2^{-2}\alpha+2\sigma_e^{-4}\alpha^2\beta-\beta\sigma_e^{-4}[(y_i-\alpha)^2-(t_i-y_i\beta)^2\sigma^2\sigma_e^2]+y_i(t_i-y_i\beta)\sigma^2\sigma_e^{-2}(\beta^2\sigma_e^{-2}+\sigma^{-2})\\
\end{aligned}\]</span></p>
<p>Ran out of time to finish problem. but just taking derivatives to find MLEs of conditional likelihood from here.</p>
<!-- \newpage -->
<!-- ## Extra Problem: -->
<!-- Let $Y \sim Beta(a,b)$.  Derive the distribution of $Z = \log(Y/(1-Y))$.  Let $\mu = a/(a+b)$ and $\phi=a+b$.  Denote the distribution of $Z$ with this parametrization as $f_Z(z;\mu,\phi)$.  Now, let $X$ be a vector of covariates.  Suppose that the conditional distribution of $Z$ given $X$ is equal to $f_Z(z;\theta(X;\beta),\phi)$, where $\theta(X;\beta)=X'\beta$.  Here beta is a vector of regression coefficients.  Argue why $f_Z(z;\theta(X;\beta),\phi)$ is not a generalized linear model. -->
<!-- Solution: -->
<!-- $$\begin{aligned} -->
<!-- P(Z \leq z) &= P(\log(Y/(1-Y)) \leq z) \\ -->
<!-- &= P(Y/(1-Y) \leq e^z) \\ -->
<!-- &= P(Y \leq e^z(1-Y)) & Y \in (0,1),~\text{so}~(1-Y)>0\\ -->
<!-- &= P(Y+ e^zY \leq e^z) \\ -->
<!-- &= P\left(Y \leq \frac{e^z}{1+e^z} \right) \\ -->
<!-- &= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} \left(\frac{e^z}{1+e^z}\right)^{a-1}\left(1-\frac{e^z}{1+e^z}\right)^{b-1} & \text{pdf of beta distribution} -->
<!-- \end{aligned}$$ -->
<!-- $$\begin{aligned} -->
<!-- \mu = a/(a+b) &,~~ \phi = a+b \\ -->
<!-- \implies a = \mu\phi&,~~b= \phi-\mu \phi -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- $$\begin{aligned} -->
<!-- f_Z(z;a,b)&= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} \left(\frac{e^z}{1+e^z}\right)^{a-1}\left(1-\frac{e^z}{1+e^z}\right)^{b-1} \\ -->
<!-- \implies f_Z(z;\mu,\phi)&= \frac{\Gamma(\mu\phi)\Gamma(\phi-\mu \phi)}{\Gamma(\mu\phi+\phi-\mu \phi)} \left(\frac{e^z}{1+e^z}\right)^{\mu\phi-1}\left(\frac{1}{1+e^z}\right)^{\phi-\mu \phi-1}\\ -->
<!-- \implies f_Z(z;\theta(X;\beta),\phi)&= \frac{\Gamma(\theta\phi)\Gamma(\phi-\theta \phi)}{\Gamma(\phi)} \left(\frac{e^z}{1+e^z}\right)^{\theta\phi-1}\left(\frac{1}{1+e^z}\right)^{\phi-\theta \phi-1}  -->
<!-- \end{aligned}$$ -->
<!-- $$\begin{aligned} -->
<!-- \implies \log f_Z(z;\theta(X;\beta),\phi)&= \log \left(\frac{\Gamma(\theta\phi)\Gamma(\phi-\theta \phi)}{\Gamma(\phi)} \right)+(\theta\phi-1) \log\left(\frac{e^z}{1+e^z}\right)+(\phi-\theta \phi-1) \log\left(\frac{1}{1+e^z}\right) \\ -->
<!-- &= \log \left(\frac{\Gamma(\theta\phi)\Gamma(\phi-\theta \phi)}{\Gamma(\phi)} \right)+(\theta\phi-1) \log(e^z) -(\theta\phi-1+\phi-\theta \phi-1)\log(1+e^z) \\ -->
<!-- &= \log \left(\frac{\Gamma(\theta\phi)\Gamma(\phi-\theta \phi)}{\Gamma(\phi)} \right)+(\theta\phi-1) z -(\phi-2)\log(1+e^z) \\ -->
<!-- &= \log \left(\frac{\Gamma(\theta\phi)\Gamma(\phi-\theta \phi)}{\Gamma(\phi)} \right)+\theta\phi z-z -(\phi-2)\log(1+e^z) \\ -->
<!-- &= \log (\Gamma(\theta\phi)) + \log(\Gamma(\phi(1-\theta)))+\theta\phi z-z -(\phi-2)\log(1+e^z)-\log (\Gamma(\phi)) \\ -->
<!-- \end{aligned}$$ -->
<!-- In trying to fit this into GLM framework, we will be stuck with a term $\theta\phi z$. This term must be in the fraction $z\theta_i/a(\phi)$ as it cannot be split into a function dependent only on the data and $\phi$. This means, $a(\phi)=1/\phi$. However, the terms $\log (\Gamma(\theta\phi)) + \log(\Gamma(\phi(1-\theta)))$ cannot factor out either $\theta$ or $\phi$. Thus, there is no plausible choice $b(\theta)$ to match the function $a$. Therefore, $f_Z(z;\theta(X;\beta),\phi)$ is not a generalized linear model. -->
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-11" class="level2">
<h2 class="anchored" data-anchor-id="section-11">2.35.</h2>
<p>Derive the Fisher information matrix <span class="math inline">\(\mathbf I(\boldsymbol \theta)\)</span> for the reparameterized ZIP model:<span class="math display">\[\begin{aligned} P(Y =0) = \pi\\ P(Y=y) = \left(\frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^ye^{-y}}{y!} \end{aligned}\]</span></p>
<p>Solution:</p>
<p><span class="math display">\[\begin{aligned}
P(Y =0) &amp;= \pi\\
P(Y=y) &amp;= \left(\frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^ye^{-y}}{y!} \\
\implies \mathcal L(\pi, \lambda |y) &amp;= \pi^{I(y=0)} \left[\left(\frac{1-\pi}{1-e^{-\lambda}}\right)\frac{\lambda^ye^{-y}}{y!}\right]^{I(y \in \mathbb N^+)} \\
\implies \ell(\pi, \lambda |y) &amp;= \log (\pi) I(y=0) + \left[\log (1-\pi) - \log(1-e^{-\lambda})+y \log (\lambda) -y -\log (y!)\right]I(y \in \mathbb N^+) \\
S(\pi,\lambda) = \frac{\partial \ell}{\partial (\pi,\lambda)^T} &amp;= \begin{pmatrix}\frac{1}{\pi}I(y= 0)-\frac{1}{1-\pi} I(y \in \mathbb N^+) \\ \left[-\frac{e^{-\lambda}}{1-e^{-\lambda}} + \frac{y}{\lambda} \right]I(y \in \mathbb N^+)\end{pmatrix} \\
I(\theta) = - E \left[\frac{\partial S}{\partial (\pi,\lambda)} \right] &amp;= -E\begin{pmatrix}\frac{-1}{\pi^2}I(y= 0)+\frac{1}{(1-\pi)^2} I(y \in \mathbb N^+) &amp; 0 \\ 0&amp; \left[-\frac{-e^{-\lambda}(1-e^{-\lambda})-e^{-\lambda}(e^{-\lambda})}{1-e^{-\lambda}} - \frac{y}{\lambda^2} \right]I(y \in \mathbb N^+)\end{pmatrix} \\
&amp;= -\begin{pmatrix}\frac{-1}{\pi^2}\pi-\frac{1}{(1-\pi)^2} (1-\pi) &amp; 0 \\ 0&amp; \left[-\frac{-e^{-\lambda}(1-e^{-\lambda})-e^{-\lambda}(e^{-\lambda})}{1-e^{-\lambda}} - \frac{\left(\frac{1}{1-e^{-\lambda}}\right)\lambda}{\lambda^2} \right](1-\pi))\end{pmatrix} \\
&amp;=\begin{pmatrix}\frac{1}{\pi}+\frac{1}{1-\pi} &amp; 0 \\ 0&amp; \left[\frac{-e^{-\lambda}}{1-e^{-\lambda}} + \frac{1}{(1-e^{-\lambda})\lambda} \right](1-\pi)\end{pmatrix}
\end{aligned}\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-12" class="level2">
<h2 class="anchored" data-anchor-id="section-12">2.37</h2>
<p>Suppose that <span class="math inline">\(Y_1, \dots, Y_n\)</span> are iid from the one parameter exponential family <span class="math inline">\(f(y;\theta)= \exp[yg(\theta)-b(\theta)+c(y)]\)</span>.</p>
<section id="a.-3" class="level3">
<h3 class="anchored" data-anchor-id="a.-3">a.</h3>
<p>For <span class="math inline">\(g(\theta) = \theta\)</span>, find <span class="math inline">\(\bar{\mathbf{I}}(Y,\theta)\)</span> (sample version) and explain why it is the same as <span class="math inline">\(I(\theta)\)</span> (Fisher information).</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal L(\theta|\mathbf Y) &amp;= \prod_{i=1}^n \exp[y_ig(\theta)-b(\theta)+c(y_i)] \\
\implies \ell (\theta | \mathbf Y) &amp;= \sum_{i=1}^n y_i \theta-b(\theta)+c(y_i) \\
\implies S (\theta | \mathbf Y) &amp;= \sum_{i=1}^n y_i -b'(\theta)\\
\implies \frac{d S (\theta | \mathbf Y)}{d \theta} &amp;= -b''(\theta) = -nb''(\theta) \\
\implies \bar{\mathbf{I}}(Y,\theta) &amp;= \frac{1}{n} \sum_{i=1}^n \left[-\frac{d S (\theta | Y_i)}{d \theta}\right] \\
&amp;= b''(\theta)
\end{aligned}
\]</span></p>
<p>The fisher information is the same as the sample fisher information as the derivative of the score function is constant with respect to the sample data. The fisher information and sample information should be equal when variables are iid.</p>
</section>
<section id="b.-3" class="level3">
<h3 class="anchored" data-anchor-id="b.-3">b.</h3>
<p>Now for general differentiable <span class="math inline">\(g(\theta)\)</span>, find <span class="math inline">\(I(\theta)\)</span></p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal L(\theta|\mathbf Y) &amp;= \prod_{i=1}^n \exp[y_ig(\theta)-b(\theta)+c(y_i)] \\
\implies \ell (\theta | \mathbf Y) &amp;= \sum_{i=1}^n y_i g(\theta)-b(\theta)+c(y_i) \\
\implies S (\theta | \mathbf Y) &amp;= \sum_{i=1}^n y_ig'(\theta) -b'(\theta)\\
\implies \frac{d S (\theta | \mathbf Y)}{d \theta} &amp;= \sum_{i=1}^n y_ig''(\theta) -b''(\theta) \\
&amp;= n \bar y g''(\theta) -nb''(\theta) \\
\implies I(Y,\theta) &amp;= - E \left[\frac{d S (\theta | Y_i)}{d \theta}\right] \\
&amp;= -E\left[y_ig''(\theta) -b''(\theta)\right] \\
&amp;= - \left[b'(\theta) g''(\theta) -b''(\theta)\right] &amp; E(y_i) = b'(\theta) \\
&amp;= b''(\theta) - b'(\theta) g'' (\theta)
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-13" class="level2">
<h2 class="anchored" data-anchor-id="section-13">2.41</h2>
<p>Use simulation to verify that the information matrix for Example 2.21 (p.&nbsp;78) is correct when <span class="math inline">\(\mu =1\)</span> and <span class="math inline">\(\sigma =1\)</span>. One approach is to generate samples of size <span class="math inline">\(n=100\)</span> (or larger) from a normal(1,1) distribution and exponentiate to get lognormal data. Then form the log likelihood and use a numerical derivative routine to find the second derivative matrix for each sample. Then average over 1000 replications and compare to the given information matrix.</p>
<p>Solution:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(numDeriv)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11062023</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>n_sim <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">c</span>(<span class="at">mu =</span> <span class="dv">1</span>, <span class="at">sigma =</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">0</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>fisher_info <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="do">##Using Sima's code as a base</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define log-likelihood function of box-cox transformation</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Source: Scott Hyde master's thesis, Montana State University</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#https://math.montana.edu/grad_students/writing-projects/Pre-2000/99hyde.pdf</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(params, y) {</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> params[<span class="dv">1</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> params[<span class="dv">2</span>]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  lambda <span class="ot">=</span> params[<span class="dv">3</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(lambda <span class="sc">==</span> <span class="dv">0</span>){</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    z<span class="ot">=</span> <span class="fu">log</span>(y)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span>{</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    z <span class="ot">=</span> (y<span class="sc">^</span>(lambda)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>lambda</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  log_lik <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)<span class="sc">*</span>(z<span class="sc">-</span>mu)<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span>(lambda<span class="dv">-1</span>)<span class="sc">*</span><span class="fu">log</span>(y))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(log_lik)  <span class="co"># Return log-likelihood</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sim){</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">exp</span>(<span class="fu">rnorm</span>(n, <span class="at">mean =</span> params[<span class="st">"mu"</span>], <span class="at">sd =</span> params[<span class="st">"sigma"</span>]))</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>  fisher_info[[i]] <span class="ot">=</span> <span class="sc">-</span><span class="fu">hessian</span>(log_likelihood,params,<span class="at">y=</span> y)<span class="sc">/</span>n</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>mean_fisher_info <span class="ot">=</span> <span class="fu">Reduce</span>(<span class="st">"+"</span>, fisher_info) <span class="sc">/</span> <span class="fu">length</span>(fisher_info)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="do">## Get matrix as defined in book</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>tau_1 <span class="ot">=</span> (params[<span class="st">"sigma"</span>]<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span>params[<span class="st">"mu"</span>]<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>tau_2 <span class="ot">=</span> (<span class="dv">7</span><span class="sc">*</span>params[<span class="st">"sigma"</span>]<span class="sc">^</span><span class="dv">4</span><span class="sc">+</span><span class="dv">10</span><span class="sc">*</span>params[<span class="st">"sigma"</span>]<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>params[<span class="st">"mu"</span>]<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span>params[<span class="st">"mu"</span>]<span class="sc">^</span><span class="dv">4</span>)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>book <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="sc">-</span>tau_1,</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>                <span class="dv">0</span>,<span class="dv">2</span>,<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>params[<span class="st">"sigma"</span>]<span class="sc">*</span>params[<span class="st">"mu"</span>],</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>                <span class="sc">-</span>tau_1,<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>params[<span class="st">"sigma"</span>]<span class="sc">*</span>params[<span class="st">"mu"</span>], tau_2),<span class="at">nrow =</span><span class="dv">3</span>,<span class="at">ncol=</span> <span class="dv">3</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>mean_fisher_info</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]          [,2]      [,3]
[1,]  1.0000000000 -0.0007235042 -0.998414
[2,] -0.0007235042  1.9926544888 -2.001938
[3,] -0.9984139964 -2.0019375153  4.503790</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>book</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    1    0 -1.0
[2,]    0    2 -2.0
[3,]   -1   -2  4.5</code></pre>
</div>
</div>
<p>The mean of the fisher information matrices that I simulated is printed above, with the fisher info the book describes right below it. These two matrices are very close together, which means the simulation supports the book’s example.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-14" class="level2">
<h2 class="anchored" data-anchor-id="section-14">2.43</h2>
<p>For the generalized linear model with link function <span class="math inline">\(g\)</span>, not necessarily the canonical link, write down the likelihood function and show how to obtain the likelihood score equation, <span class="math display">\[S(\boldsymbol \beta, \phi) = \sum_{i=1}^n \mathbf D_i \frac{(Y_i-\mu_i)}{Var(Y_i)}=0, \text{ where } \mathbf D_i =\mathbf D_i(\boldsymbol \beta) = \frac{\partial \mu_i(\boldsymbol \beta)}{\partial \boldsymbol \beta^T}\]</span> (In the above expression we have suppressed the dependence of <span class="math inline">\(D_i\)</span> and <span class="math inline">\(\mu_i\)</span> on <span class="math inline">\(\boldsymbol \beta\)</span>.) The key idea used is the chain rule and the fact that the derivative of <span class="math inline">\(\theta_i= b'^{-1}(\mu_i)\)</span> with respect to <span class="math inline">\(\mu_i\)</span> is <span class="math inline">\(1/b''(\theta_i)\)</span>.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\ell (y_i;\theta_i,\phi) &amp;= \frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i,\phi) \\
&amp;= \frac{y_ib'^{-1}(\mu_i) - b(b'^{-1}(\mu_i))}{a_i(\phi)} + c(y_i,\phi) \\
S(\beta, \phi) &amp;= \frac{y_ib''^{-1}(\mu_i)\mathbf D_i - b'(b'^{-1}(\mu_i))b''^{-1}(\mu_i) \mathbf D_i}{a_i(\phi)} \\
&amp;= \frac{y_i\mathbf D_i - b'(b'^{-1}(\mu_i)) \mathbf D_i}{b''(\theta_i)a_i(\phi)} &amp; \left[b''^{-1}(\mu_i) = \frac{1}{b''(\theta_i)}\right] \\
&amp;= \frac{y_i\mathbf D_i - \mu_i \mathbf D_i}{b''(\theta_i)a_i(\phi)} \\
&amp;= \mathbf D_i\frac{Y_i - \mu_i}{Var(Y_i)} &amp; \left[Var(Y_i) = b''(\theta_i)a_i(\phi)\right]\\
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-15" class="level2">
<h2 class="anchored" data-anchor-id="section-15">2.44</h2>
<p>Continuing the last problem, show that the Fisher information matrix for the <span class="math inline">\(\boldsymbol \beta\)</span> part of the generalized linear model is given by <span class="math display">\[\bar{\mathbf I} ( \boldsymbol  \beta ) = \frac{1}{n} \sum_{i=1}^n \frac{\mathbf{D_i D_i}^T}{Var(Y_i)}\]</span> Here you can use either of two methods: a) take the expectation of the negative of the derivative of <span class="math inline">\(S(\boldsymbol \beta, \phi)\)</span>, and noting that all the ugly derivatives drop out because <span class="math inline">\(E(Y_i-\mu_i)=0\)</span>; or b) the individual summed components of <span class="math inline">\(\mathbf{\bar I}(\boldsymbol \beta)\)</span> can also be found using the cross-product definition of information in (2.39, p.&nbsp;67).</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\bar{\mathbf I}(\boldsymbol \beta)
&amp;= \frac{1}{n}\sum_{i=1}^n E\left[\mathbf s_i(\boldsymbol \beta) \mathbf s_i(\boldsymbol \beta)^T\right] \\
&amp;= \frac{1}{n}\sum_{i=1}^n E\left[\mathbf D_i\frac{Y_i - \mu_i}{Var(Y_i)}\mathbf D_i^T\frac{Y_i - \mu_i}{Var(Y_i)}\right] \\
&amp;= \frac{1}{n}\sum_{i=1}^n \mathbf D_i \mathbf D_i^T\frac{E[(Y_i - \mu_i)^2]}{[Var(Y_i)]^2} \\
&amp;= \frac{1}{n}\sum_{i=1}^n \mathbf D_i \mathbf D_i^T\frac{Var(Y_i)}{[Var(Y_i)]^2} \\
&amp;= \frac{1}{n}\sum_{i=1}^n \frac{\mathbf D_i \mathbf D_i^T}{Var(Y_i)} \\
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-16" class="level2">
<h2 class="anchored" data-anchor-id="section-16">2.45</h2>
<p>Suppose that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent and continuous random variables with densities <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>, respectively. <span class="math inline">\(Z\)</span> is a Bernoulli(<span class="math inline">\(p\)</span>) random variable and independent of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Define <span class="math inline">\(Y= ZX_1 +(1-ZX_2)\)</span>.</p>
<section id="a-1" class="level3">
<h3 class="anchored" data-anchor-id="a-1">a)</h3>
<p>Use the <span class="math inline">\(2h\)</span> method to show that the joint density of <span class="math inline">\((Y,Z)\)</span> is given by <span class="math display">\[f_{Y,Z}(y,z) = [pf_1(y)]^z[(1-p)f_2(y)]^{1-z}\]</span></p>
<p>Solution:</p>
<p><span class="math display">\[\begin{aligned}
f_{Y,Z} (y,z) &amp;= lim_{h \to 0^+} (2h)^{-2} P\left(Y \in(y-h, y+h], Z \in (z-h,z+h]\right) \\
&amp;= lim_{h \to 0^+} (2h)^{-1} P\left(Y \in(y-h, y+h], Z = z\right) ~~~~~~~~~\text{(for small }h) \\
&amp;= lim_{h \to 0^+} (2h)^{-1} \big[P(z\,x_1+ (1-z)\,x_2 \leq y+h], z= 0) -P(z\,x_1+ (1-z)\,x_2 &lt; y-h], z= 0) \\ &amp;~~~~~~~~~~~~~~~~~~+
P(z\,x_1+ (1-z)\,x_2 \leq y+h], z= 1) -P(z\,x_1+ (1-z)\,x_2 &lt; y-h], z= 1)\big] \\
&amp;= lim_{h \to 0^+} (2h)^{-1} \big[P(z=0)[P(x_2 \leq y+h]) -P(\,x_2 &lt; y-h])] \\&amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+P(z=1)[P(x_1\leq y+h]) -P(x_1 &lt; y-h])\big] \\
&amp;= lim_{h \to 0^+} (2h)^{-1} \big\{p[P((x_2 \leq y+h]) -P(\,x_2 &lt; y-h])]^{1-z} \\
&amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\times\left[(1-p)[P(z\,x_1 \leq y+h]) -P(x_1 &lt; y-h]) \right]^{z} \big\} \\
&amp;= [pf_1(y)]^z[(1-p)f_2(y)]^{1-z}
\end{aligned}
\]</span></p>
</section>
<section id="b-2" class="level3">
<h3 class="anchored" data-anchor-id="b-2">b)</h3>
<p>Use the <span class="math inline">\(2h\)</span> method to show that <span class="math display">\[P(Z=1|Y=y) = \frac{pf_1(y)}{pf_1(y) + (1-p)f_2(y)}\]</span></p>
<p>Solution:</p>
<p>I have already used the <span class="math inline">\(2h\)</span> method in the problem above to find the pdf <span class="math inline">\(f_{Y,Z}(y,z)\)</span>, now we can use simple probability rules for the rest of the work:</p>
<p><span class="math display">\[\begin{aligned}
P(Z=1|Y=y) &amp;= \frac{P(Z=1,Y=y)}{P(Y=y)}\\
&amp;= \frac{P(Z=1,Y=y)}{\sum_{Z}P(Y=y,Z=z)}\\
&amp;= \frac{P(Z=1,Y=y)}{P(Y=y,Z=0) + P(Y=y,z=1)}\\
&amp;= \frac{f_{Y,Z} (y,z=1)}{f_{Y,Z} (y,z=0) + f_{Y,Z} (y,z=1)} \\
&amp;= \frac{pf_1(y)}{[(1-p)f_2(y)] + [pf_1(y)]}
\end{aligned}
\]</span></p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-17" class="level2">
<h2 class="anchored" data-anchor-id="section-17">2.47</h2>
<p>A mixture of three component densities has the form <span class="math display">\[f(y;\boldsymbol \theta, \mathbf p) = p_1f_1(y;\boldsymbol \theta) + p_2f_2(y;\boldsymbol \theta) + p_3f_3(y;\boldsymbol \theta),\]</span> where <span class="math inline">\(p_1= p_2 = p_3 = 1\)</span>. We observe an iid sample <span class="math inline">\(Y_1,\dots, Y_n\)</span> from <span class="math inline">\(f(y;\boldsymbol \theta, \mathbf p )\)</span>.</p>
<section id="a.-4" class="level3">
<h3 class="anchored" data-anchor-id="a.-4">a.</h3>
<p>Show how to define multinomial(<span class="math inline">\(1;p_1,p_2,p_3\)</span>) vectors <span class="math inline">\((Z_{i1},Z_{i2},Z_{i3})\)</span> to get a representation for the <span class="math inline">\(Y_i\)</span> from <span class="math inline">\(f(y;\boldsymbol \theta, \mathbf p )\)</span> based on independent random variables <span class="math inline">\((X_{i1},X_{i2},X_{i3})\)</span> from the individual components.</p>
<p>Solution:</p>
<p>Let <span class="math inline">\(Z_{i1} \sim Bernoulli(p_1),Z_{i2} \sim Bernoulli(p_2),Z_{i3} \sim Bernoulli(p_3)\)</span> such that <span class="math inline">\((Z_{i1},Z_{i2},Z_{i3}) \sim multinomial(1;p_1,p_2,p_3)\)</span> Also, let the pdfs of <span class="math inline">\(X_{i1},X_{i2},X_{i3}\)</span> be <span class="math inline">\(f_1(y;\boldsymbol \theta), f_1(y;\boldsymbol \theta), f_1(y;\boldsymbol \theta)\)</span> respectively.</p>
<p>Define <span class="math inline">\(Y_i = Z_{i1}X_{i1}+ Z_{i2}X_{i2} + Z_{i3}X_{i3}\)</span>.</p>
<p>Thus, <span class="math inline">\(f_{Y_i}(y) = p_1f_1(y;\boldsymbol \theta) + p_2f_2(y;\boldsymbol \theta) + p_3f_3(y;\boldsymbol \theta)\)</span></p>
</section>
<section id="b.-4" class="level3">
<h3 class="anchored" data-anchor-id="b.-4">b.</h3>
<p>Give the complete data log likelihood and the function <span class="math inline">\(Q\)</span> to be maximized at the M step.</p>
<p>Solution:</p>
<p>The joint distribution <span class="math inline">\(f(Y_i,Z_i)\)</span> can be found as follows:</p>
<p><span class="math display">\[
\begin{aligned}
f(Y_i,Z_i) &amp;= f(Y_i|Z_i)f(Z_i) \\
&amp;= f(Z_{i1}X_{i1}+ Z_{i2}X_{i2} + Z_{i3}X_{i3}|Z_i)f(Z_i) \\
&amp;= f_{X_{i1}}(y_i|Z_{i1})f_{X_{i2}}(y_i|Z_{i2})f_{X_{i3}}(y_i|Z_{i3})f(Z_i)\\
&amp;= [f_{X_{i1}}(y_i;\boldsymbol \theta)]^{z_{i1}}[f_{X_{i2}}(y_i;\boldsymbol \theta)]^{Z_{i2}}[f_{X_{i3}}(y_i;\boldsymbol \theta)]^{Z_{i3}}p_1^{z_{i1}}p_2^{z_{i2}}p_3^{z_i3}
\end{aligned}
\]</span> Thus, the complete log likelihood is <span class="math display">\[\begin{aligned}\ell_C(\boldsymbol \theta,p_1,p_2,p_3|\mathbf Y,\mathbf Z) &amp;= \sum_{i=1}^n z_{i1}\log(f_{X_{i1}}(\boldsymbol \theta;y_i))+z_{i2}\log (f_{X_{i2}}(\boldsymbol \theta;y_i))+z_{i3}\log(f_{X_{i3}}(\boldsymbol \theta;y_i))\\
&amp;~~~~~~~~~~~~~~~~~~~~~+z_{i1}\log p_1+z_{i2}\log p_2+z_{i3} \log p_3 \end{aligned}\]</span></p>
<p>and, <span class="math display">\[\begin{aligned}Q(\boldsymbol \theta,p_1,p_2,p_3,\boldsymbol \theta^v,p_1^v,p_2^v,p_3^v,\mathbf Y) &amp;= E_{(\boldsymbol \theta^v,p_1^v,p_2^v,p_3^v)} [\ell_C(\boldsymbol \theta,p_1,p_2,p_3|\mathbf Y,\mathbf Z)]\\
&amp;=\sum_{i=1}^n \bigg\{w_{i1}^v\log(f_{X_{i1}}(y_i;\boldsymbol \theta))+w_{i2}^v\log (f_{X_{i2}}(y_i;\boldsymbol \theta))+w^v_{i3}\log(f_{X_{i3}}(y_i;\boldsymbol \theta))\\
&amp;~~~~~~~~~~~~~~~~~~~~~+w^v_{i1}\log p_1+w^v_{i2}\log p_2+w^v_{i3} \log p_3\bigg\} \end{aligned}\]</span></p>
<p>Where <span class="math inline">\(\begin{aligned} w_{ij}^v= E_{(\boldsymbol \theta^v,p_1^v,p_2^v,p_3^v)}[Z_{ij}|Y_i]&amp;= \frac{p_j^{v}f_j(Y_i,\boldsymbol \theta^v)}{\sum_{j=1}^n p_j^vf_j(Y_i,\boldsymbol \theta^v)}\end{aligned}\)</span></p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="section-18" class="level2">
<h2 class="anchored" data-anchor-id="section-18">2.48</h2>
<p>Suppose that the data <span class="math inline">\(Y_1, \dots, Y_n\)</span> are assumed to come from a mixture of two binomial distributions. Thus <span class="math display">\[f(y;p,\theta_1,\theta_2) = p {n \choose y} \theta_1^y(1-\theta_1)^{n-y} +(1-p) {n \choose y} \theta_2^y(1-\theta_2)^{n-y}.\]</span> Find <span class="math inline">\(Q(p,\theta_1,\theta_2,p^v,\theta_1^v,\theta_2^v)\)</span> and the updating formulas.</p>
<p>Solution:</p>
<p>Define <span class="math inline">\(Z_i \sim Bernoulli(p)\)</span>, <span class="math inline">\(Y_i = ZX_{1i}+ (1-Z)X_{2i}\)</span>, where <span class="math inline">\(X_{1i} \sim Binom(n,\theta_1)\)</span>, <span class="math inline">\(X_{2i} \sim Binom(n,\theta_2)\)</span> and <span class="math inline">\(X_{1i},X_{2i}\)</span> are independent of <span class="math inline">\(Z\)</span>. Thus, <span class="math inline">\(Y_i\)</span> will have a mixture density equal to <span class="math inline">\(f(y;p,\theta_1,\theta_2)\)</span>.</p>
<p>Therefore, <span class="math display">\[\mathcal L_C(p,\theta_1,\theta_2;y_i,Z) = \left[p f_{X_1}(y_i;n,\theta_1)\right]^{z_i} \left[(1-p) f_{X_2}(y_i;n,\theta_2)\right]^{1-{z_i}}\]</span> And, following steps outlined on page 85 of Boos, <span class="math display">\[\begin{aligned}
Q(p,\theta_1,\theta_2,p^v,\theta_1^v,\theta_2^v) &amp;= \sum_{i=1}^n  w_i^v \log f_{X_1}(y_i;n,\theta_1)+(1-w^v_i) \log f_{X_2}(y_i;n,\theta_2) +w^v_i \log p + (1-w^v_i)\log(1-p)
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(w^v_i = \frac{p^vf_{X_1}(y_i;n,\theta_1)}{p^vf_{X_1}(y_i;n,\theta_1)+(1-p^v)f_{X_2}(y_i;n,\theta_2)}\)</span>.</p>
<p>Substituting in the distributions,</p>
<p><span class="math display">\[
\begin{aligned}
Q(p,\theta_1,\theta_2,p^v,\theta_1^v,\theta_2^v) &amp;= \sum_{i=1}^n \bigg\{ w_i^v \log \left[{n \choose y_i} \theta_1^{y_i}(1-\theta_1)^{n-y_i}\right] +(1-w_i^v) \log \left[{n \choose y_i} \theta_2^{y_i}(1-\theta_2)^{n-y_i}\right]\\
&amp;~~~~~~~~~ +w_i^v \log p + (1-w_i^v)\log(1-p)\bigg\} \\
&amp;= \sum_{i=1}^n \bigg\{ w_i^v \left[ \log {n \choose y_i} + y_i \log \theta_1 +\log y_i +(n-y_i)\log(1-\theta_1)\right]\\
&amp;~~~~~~~~~  +(1-w_i^v) \left[ \log {n \choose y_i} + y_i \log \theta_2 +\log y_i +(n-y_i)\log(1-\theta_2)\right]\\
&amp;~~~~~~~~~ +w_i^v \log p + (1-w_i^v)\log(1-p)\bigg\}
\end{aligned}
\]</span></p>
<p>Now to maximize <span class="math inline">\(Q\)</span>, <span class="math display">\[ \begin{aligned}
0= \frac{\partial Q}{\partial \theta_1} &amp;=\sum_{i=1}^n \frac{w_i^vy_i}{\theta_1}-\frac{w_i^v(n-y_i)}{1-\theta_1} \\
\implies 0&amp;= \sum_{i=1}^n w_i^v(y_i-y_i\theta_1-(n-y_i)\theta_1)\\
\implies 0&amp;= \sum_{i=1}^n w_i^v(y_i-n\theta_1)\\
\implies \theta_1^{v+1} &amp;= \frac{\sum_{i=1}^n w_i^vy_i}{\sum_{i=1}^n w_i^vn}
\end{aligned}
\]</span> Similarly, <span class="math display">\[ \begin{aligned}
0= \frac{\partial Q}{\partial \theta_1} &amp;=\sum_{i=1}^n \frac{(1-w_i^v)y_i}{\theta_2}-\frac{(1-w_i^v)(n-y_i)}{1-\theta_2} \\
\implies 0&amp;= \sum_{i=1}^n (1-w_i^v)(y_i-y_i\theta_2-(n-y_i)\theta_2)\\
\implies \theta_2^{v+1} &amp;= \frac{\sum_{i=1}^n (1-w_i^v)y_i}{\sum_{i=1}^n (1-w_i^v)n}
\end{aligned}
\]</span></p>
<p>and, <span class="math display">\[\begin{aligned}
0=\frac{\partial Q}{\partial p} &amp;=\frac{w_i^v}{p}-\frac{1-w_i^v}{1-p} \\
\implies 0&amp;= w_i^v-pw_i^v-p+pw_i^v \\
\implies p^{v+1} &amp;= w_i^v
\end{aligned}\]</span></p>
<div style="page-break-after: always;"></div>
</section>
<section id="section-19" class="level2">
<h2 class="anchored" data-anchor-id="section-19">2.49</h2>
<p>Recall that the ZIP model is just a mixture of densities <span class="math inline">\(f(y;\lambda, p) =pf_1(y)+(1-p)f_2(y;\lambda)\)</span> where <span class="math display">\[\begin{aligned}f_1(y)=I(y=0)~&amp;~~~~~~~~~f_2(y;\lambda)= \frac{\lambda^ye^{-\lambda}}{y!}~&amp;~~y=0,1,2,\dots\end{aligned}\]</span> Lambert (1992) used it to model product defects as a function of covariates. In the “perfect” state, no defects occur (<span class="math inline">\(P(Y_i= 0)= 0\)</span>), whereas in the “imperfect” state, the number of defects <span class="math inline">\(Y_i\)</span> follows a Poisson<span class="math inline">\((\lambda)\)</span> distribution. The author used the EM Algorithm as follows (except we won’t do the more complicated modeling with covariates.) Let <span class="math inline">\(Z_i = 1\)</span> if the product is in the perfect state and <span class="math inline">\(Z_i = 0\)</span> for the imperfect state. Recall that the contribution to the complete data likelihood for a pair <span class="math inline">\((Y_i,Z_i)\)</span> is <span class="math inline">\([pf_1(Y_i)]^{Z_i}[(1-p)f_2(Y_i;\lambda)]^{1-Z_i}\)</span> (and here note that the first part reduces to <span class="math inline">\(p^{Z_i}\)</span> because <span class="math inline">\(f_1\)</span> is a point mass at 0).</p>
<section id="a.-e-step." class="level3">
<h3 class="anchored" data-anchor-id="a.-e-step.">a. E step.</h3>
<p>Write down the complete data log likelihood and find <span class="math inline">\(Q(\lambda,p,\lambda^v,p^v)\)</span> in terms of <span class="math inline">\(w_i^v = E(Z_i|Y_i,\lambda^v,p^v)\)</span>. (You do not need to give an expression for <span class="math inline">\(w_i^v\)</span>.)</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal L_C (\lambda, p, \lambda^v, p^v; Y_i,Z_i) &amp;= \prod_{i=1}^n[pf_1(Y_i)]^{Z_i}[(1-p)f_2(Y_i;\lambda)]^{1-Z_i} \\
\implies  \ell_C(\lambda, p, \lambda^v, p^v; Y_i,Z_i) &amp;= \sum_{i=1}^n z_i \log(f_1(y_i))+(1-z_i)\log(f_2(y)) +z_i \log p +(1-z_i)\log(1-p) \\
\implies  Q(\lambda, p, \lambda^v, p^v; Y_i) &amp;=\sum_{i=1}^nE_{(\lambda^v,p^v)}
\left[z_i \log(f_1(y_i))+(1-z_i)\log(f_2(y,\lambda)) +z_i \log p +(1-z_i)\log(1-p)\right] \\
&amp;=\sum_{i=1}^nw_i^v \log(f_1(y_i))+(1-w^v_i)\log(f_2(y,\lambda)) +w^v_i \log p +(1-w^v_i)\log(1-p) \\
&amp;=\sum_{i=1}^nw_i^v I(y_i=0)+(1-w^v_i)\log\left(\frac{\lambda^{y_i}e^{-\lambda}}{y_i!}\right)I(y_i \in \mathbb N^+) \\&amp;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+w^v_i \log p +(1-w^v_i)\log(1-p) \\
\end{aligned}
\]</span></p>
</section>
<section id="b.-m-step." class="level3">
<h3 class="anchored" data-anchor-id="b.-m-step.">b. M step.</h3>
<p>Find expressions for <span class="math inline">\(\lambda^{v+1}\)</span> and <span class="math inline">\(p^{v+1}\)</span> by maximizing <span class="math inline">\(Q\)</span> from the E step.</p>
<p>Solution:</p>
<p><span class="math display">\[
\begin{aligned}
0 =\frac{\partial Q}{\partial p} &amp;= \sum_{i=1}^n\frac{w_i^v}{p}-\frac{1-w_i^v}{1-p}\\
\implies 0 &amp;= \sum_{i=1}^n w_i^v-pw_i^v-p+pw_i^v \\
\implies p^{v+1} &amp;= \frac{\sum_{i=1}^n w_i^v}{n}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
0 =\frac{\partial Q}{\partial \lambda} &amp;= (1-w_i^v)\left[\frac{y_i}{\lambda}-1\right]I(y_i \in \mathbb N^+)\\
\implies 0 &amp;= (1-w_i^v)(y_i-\lambda)I(y_i \in \mathbb N^+) \\
\implies \lambda^{v+1} &amp;=\frac{(1-w_i^v)y_i}{1-w_i^v}
\end{aligned}
\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/ch1.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Chapter 1: Roles of Modeling in Statistical Inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/ch3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Chapter 3: Likelihood-Based Tests and Confidence Regions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>